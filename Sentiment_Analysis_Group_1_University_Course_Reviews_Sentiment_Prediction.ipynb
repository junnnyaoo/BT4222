{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Group 1\n",
        "## Table of Contents\n",
        "1. Data Preprocessing and Exploration  \n",
        "  1.1 Data Cleaning  \n",
        "  1.2 Data Preprocessing  \n",
        "  1.3 Data Validation  \n",
        "  1.4 Class Balancing  \n",
        "  1.5 Data Splitting  \n",
        "2. Sentiment Prediction  \n",
        "  2.1 SVM  \n",
        "  2.2 Logistic Regression  \n",
        "  2.3 LSTM  \n",
        "  2.4 MLP"
      ],
      "metadata": {
        "id": "QdAvptSkvOH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install\n",
        "! pip install langdetect -q\n",
        "! pip install transformers -q\n",
        "! pip install sentence-transformers -q\n",
        "! pip install umap-learn -q\n",
        "! pip install hdbscan -q"
      ],
      "metadata": {
        "id": "E3gEvs2PWiSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de040197-906b-4d2d-e271-c5067c438d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "## General\n",
        "import re\n",
        "import gdown\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.sparse import csr_matrix\n",
        "import joblib\n",
        "\n",
        "## Data Processing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import langdetect\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "## Embedding\n",
        "import umap\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "## Prediction\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "\n",
        "## Neural Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "## Clustering\n",
        "from textblob import TextBlob\n",
        "from sklearn import decomposition\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "## SVM and Logistic Regression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "ontl7bapzxVK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "893b4a0b-098e-4fa6-d91c-824f63ecd86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_from_gdrive(file_id, output_name):\n",
        "  url = f'https://drive.google.com/uc?id={file_id}'\n",
        "  output = output_name #with file type\n",
        "  gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "8PrtEzNzmkZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a id=\"data_preprocessing\">1. Data Preprocessing and Exploration</a>\n"
      ],
      "metadata": {
        "id": "SAGfMauavOc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset\n",
        "file_id = '1CHw0LZcNqNKN0sc9YwY8ug3zdtyAM67u' #https://drive.google.com/file/d/1CHw0LZcNqNKN0sc9YwY8ug3zdtyAM67u/view?usp=sharing\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output = \"coursera_reviews.csv\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "file_id2 = '125groP8PUYeQy8dPv2dWgFQ4z1ccXvQR'\n",
        "url2 = f'https://drive.google.com/uc?id={file_id2}' #https://drive.google.com/file/d/125groP8PUYeQy8dPv2dWgFQ4z1ccXvQR/view?usp=sharing\n",
        "output2 = \"huggingface_reviews.csv\"\n",
        "gdown.download(url2, output2, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "boKqhFcypVTo",
        "outputId": "63ccded4-3134-4792-9aae-89917df61971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CHw0LZcNqNKN0sc9YwY8ug3zdtyAM67u\n",
            "To: /content/coursera_reviews.csv\n",
            "100%|██████████| 212M/212M [00:02<00:00, 92.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=125groP8PUYeQy8dPv2dWgFQ4z1ccXvQR\n",
            "To: /content/huggingface_reviews.csv\n",
            "100%|██████████| 17.0M/17.0M [00:00<00:00, 63.0MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'huggingface_reviews.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read csv\n",
        "df_coursera = pd.read_csv(\"coursera_reviews.csv\", sep=\"|\")\n",
        "df_coursera[\"label\"] = df_coursera[[\"star_ratings\"]] > 3\n",
        "df_coursera = df_coursera[[\"review\", \"label\"]]\n",
        "\n",
        "df_huggingface = pd.read_csv(\"huggingface_reviews.csv\", sep = \",\")\n",
        "df_huggingface = df_huggingface[[\"review\", \"label\"]]\n",
        "\n",
        "df = pd.concat([df_coursera, df_huggingface], ignore_index = True, sort = False)\n",
        "print(len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "A5sQLgE-pkYf",
        "outputId": "d2398057-e765-42e5-8bd7-afe35b30a66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-1772326a6966>:2: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_coursera = pd.read_csv(\"coursera_reviews.csv\", sep=\"|\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "638271\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  label\n",
              "0  The labs did not require any code changes to c...      0\n",
              "1  Good overview of key topics, but the course is...      0\n",
              "2  The lectures define many important concepts in...      0\n",
              "3  It would have been better to have an opportuni...      1\n",
              "4  AWS and DeepLearning.AI structured the course ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ac22886-4833-4373-a799-4ba23edb1065\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The labs did not require any code changes to c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good overview of key topics, but the course is...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The lectures define many important concepts in...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It would have been better to have an opportuni...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AWS and DeepLearning.AI structured the course ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ac22886-4833-4373-a799-4ba23edb1065')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3ac22886-4833-4373-a799-4ba23edb1065 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3ac22886-4833-4373-a799-4ba23edb1065');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e1f8c650-7394-4732-9b3a-7bd3956a7dd1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e1f8c650-7394-4732-9b3a-7bd3956a7dd1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e1f8c650-7394-4732-9b3a-7bd3956a7dd1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a id=\"data_cleaning\">1.1 Data Cleaning</a>"
      ],
      "metadata": {
        "id": "rxdpYzzUxyCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df.copy()\n",
        "\n",
        "# drop na\n",
        "df_cleaned = df_cleaned.dropna(subset=[\"review\", \"label\"])\n",
        "df_cleaned = df_cleaned.reset_index(drop=True)\n",
        "\n",
        "print(len(df_cleaned))\n",
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "JXsa__r7DmFU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "9e515b3b-8121-45e9-ee78-c0611f2b52d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "638271\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  label\n",
              "0  The labs did not require any code changes to c...      0\n",
              "1  Good overview of key topics, but the course is...      0\n",
              "2  The lectures define many important concepts in...      0\n",
              "3  It would have been better to have an opportuni...      1\n",
              "4  AWS and DeepLearning.AI structured the course ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4235f1be-bf6a-4fa3-922d-a7988738c6ef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The labs did not require any code changes to c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good overview of key topics, but the course is...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The lectures define many important concepts in...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It would have been better to have an opportuni...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AWS and DeepLearning.AI structured the course ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4235f1be-bf6a-4fa3-922d-a7988738c6ef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4235f1be-bf6a-4fa3-922d-a7988738c6ef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4235f1be-bf6a-4fa3-922d-a7988738c6ef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-05628a61-154d-46dc-bd59-e9107ab10b26\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-05628a61-154d-46dc-bd59-e9107ab10b26')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-05628a61-154d-46dc-bd59-e9107ab10b26 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a id=\"data_preprocessing\">1.2 Data Preprocessing</a>"
      ],
      "metadata": {
        "id": "Kd9h1AP4g2U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stopwords\n",
        "df_cleaned[\"review\"] = df_cleaned[\"review\"].str.lower()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def clean_text(text):\n",
        "    # converts text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    #replace all non-word characters (characters that are not a letter, digit, or underscore) in text with a space\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\d', '', text)\n",
        "    text = re.sub(r'_', '', text)\n",
        "\n",
        "    # split the text into individual words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # not to remove words like \"no\" and \"not\" because it affects the sentiment of the reviews\n",
        "    words = [word for word in words if ((word not in stop_words) or (word in [\"no\", \"not\"]))]\n",
        "\n",
        "    # stemming\n",
        "    porter = PorterStemmer()\n",
        "    words = [porter.stem(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "df_cleaned[\"review\"] = df_cleaned[\"review\"].apply(clean_text)\n",
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "chTPT8SBhOX9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "cceefb8b-f4e7-4411-9de3-47733121d23c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  label\n",
              "0  lab not requir code chang complet similar free...      0\n",
              "1  good overview key topic cours practic would ho...      0\n",
              "2  lectur defin mani import concept easi understa...      0\n",
              "3  would better opportun write code assign instea...      1\n",
              "4  aw deeplearn ai structur cours three comprehen...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c93336d9-a70d-4232-8c53-52705891bce2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lab not requir code chang complet similar free...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>good overview key topic cours practic would ho...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lectur defin mani import concept easi understa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>would better opportun write code assign instea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aw deeplearn ai structur cours three comprehen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c93336d9-a70d-4232-8c53-52705891bce2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c93336d9-a70d-4232-8c53-52705891bce2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c93336d9-a70d-4232-8c53-52705891bce2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-182fca6b-4772-4245-ab58-52a18f146926\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-182fca6b-4772-4245-ab58-52a18f146926')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-182fca6b-4772-4245-ab58-52a18f146926 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a id=\"data_validation\">1.3 Data Validation</a>"
      ],
      "metadata": {
        "id": "Q_wqYBkUx_pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot count of positive vs negative reviews (significant imbalance)\n",
        "\n",
        "print(\"\\nClass distribution before balancing:\")\n",
        "print(df_cleaned['label'].value_counts())\n",
        "\n",
        "fig=plt.figure(figsize=(5,5))\n",
        "colors=[\"skyblue\",'pink']\n",
        "pos=df_cleaned[df_cleaned['label']==1]\n",
        "neg=df_cleaned[df_cleaned['label']==0]\n",
        "ck=[pos['label'].count(),neg['label'].count()]\n",
        "\n",
        "legpie=plt.pie(ck,labels=[\"Positive\",\"Negative\"],\n",
        "                 autopct ='%1.1f%%',\n",
        "                 shadow = True,\n",
        "                 colors = colors,\n",
        "                 startangle = 45,\n",
        "                 explode=(0, 0.1))"
      ],
      "metadata": {
        "id": "9f85-QtzyAHC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "outputId": "df841f65-3da7-49e1-8744-f737e66891a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class distribution before balancing:\n",
            "1    588972\n",
            "0     49299\n",
            "Name: label, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a id=\"class_balancing\">1.4 Class Balancing</a>"
      ],
      "metadata": {
        "id": "-i8RDIsEyBGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "X = df_cleaned['review']\n",
        "y = df_cleaned['label']\n",
        "\n",
        "over_sampler = RandomOverSampler(sampling_strategy=0.5, random_state=42)\n",
        "under_sampler = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
        "\n",
        "X_resampled, y_resampled = over_sampler.fit_resample(X.values.reshape(-1, 1), y)\n",
        "X_resampled, y_resampled = under_sampler.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "balanced_df = pd.DataFrame({'review': X_resampled.flatten(), 'label': y_resampled})\n",
        "\n",
        "print(\"\\nClass distribution after balancing:\")\n",
        "print(balanced_df['label'].value_counts())\n",
        "\n",
        "fig=plt.figure(figsize=(5,5))\n",
        "colors=[\"skyblue\",'pink']\n",
        "pos=balanced_df[balanced_df['label']==1]\n",
        "neg=balanced_df[balanced_df['label']==0]\n",
        "ck=[pos['label'].count(),neg['label'].count()]\n",
        "\n",
        "legpie=plt.pie(ck,labels=[\"Positive\",\"Negative\"],\n",
        "                 autopct ='%1.1f%%',\n",
        "                 shadow = True,\n",
        "                 colors = colors,\n",
        "                 startangle = 45,\n",
        "                 explode=(0, 0.1))"
      ],
      "metadata": {
        "id": "69KGynXmyBfS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "outputId": "0ffad9ad-b185-4bb8-f118-852d2eef3e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class distribution after balancing:\n",
            "0    294486\n",
            "1    294486\n",
            "Name: label, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "AvqQsKajX2qF",
        "outputId": "4f289f75-0700-43bd-e88e-040c30034af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  label\n",
              "0  lab not requir code chang complet similar free...      0\n",
              "1  good overview key topic cours practic would ho...      0\n",
              "2  lectur defin mani import concept easi understa...      0\n",
              "3  pretti superfici coverag lab simplifi execut s...      0\n",
              "4  cours materi pretti superfici lectur never rea...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18e0ba93-8628-4f9e-a472-4af36c41e521\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lab not requir code chang complet similar free...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>good overview key topic cours practic would ho...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lectur defin mani import concept easi understa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pretti superfici coverag lab simplifi execut s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cours materi pretti superfici lectur never rea...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18e0ba93-8628-4f9e-a472-4af36c41e521')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-18e0ba93-8628-4f9e-a472-4af36c41e521 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-18e0ba93-8628-4f9e-a472-4af36c41e521');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2604efd3-b5d9-4201-9510-658e4695a258\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2604efd3-b5d9-4201-9510-658e4695a258')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2604efd3-b5d9-4201-9510-658e4695a258 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a id=\"data_splitting\">1.5 Data Splitting</a>"
      ],
      "metadata": {
        "id": "V5JfPzObZn_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " df_X = balanced_df[\"review\"]\n",
        " df_y = balanced_df[\"label\"]\n",
        "# split test\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "for train_val_index, test_index in sss.split(df_X, df_y):\n",
        "  df_X_train_val, df_X_test = df_X.iloc[train_val_index], df_X.iloc[test_index]\n",
        "  df_y_train_val, df_y_test = df_y.iloc[train_val_index], df_y.iloc[test_index]\n",
        "\n",
        "# split train/validation\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "for train_index, valid_index in sss.split(df_X_train_val, df_y_train_val):\n",
        "    df_X_train, df_X_val = df_X_train_val.iloc[train_index], df_X_train_val.iloc[valid_index]\n",
        "    df_y_train, df_y_val = df_y_train_val.iloc[train_index], df_y_train_val.iloc[valid_index]\n",
        "\n",
        "print(\"Size of training set: \", len(df_X_train))\n",
        "print(\"Size of validation set: \", len(df_X_val))\n",
        "print(\"Size of test set: \", len(df_X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsEtsNw0zNMf",
        "outputId": "3701154b-90eb-4a85-eb58-7d499527e344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training set:  288596\n",
            "Size of validation set:  123684\n",
            "Size of test set:  176692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a id=\"sentiment_prediction\">2. Sentiment Prediction</a>"
      ],
      "metadata": {
        "id": "B44c-rSBzfMz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq0qGyNyIMBg"
      },
      "source": [
        "#### <a id=\"svm_with_word_embedding\">Support Vector Machine (SVM) with Different Word Embedding Methods</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <a id=\"svm_word_embedding\">Word Embedding</a>"
      ],
      "metadata": {
        "id": "GVA0EbIVlUCs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVzdIA8sIMBg"
      },
      "source": [
        "##### <a id=\"svm_with_count_vec\">CountVectorizer (Bag-of-Words)</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emSNalZHIMBg",
        "outputId": "53a6fffb-bf36-449a-d2ea-f61d841399fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<56424x34420 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 827123 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_vec = CountVectorizer()\n",
        "X_train_review_us_count_vec = count_vec.fit_transform(df_X_train)\n",
        "X_train_review_us_count_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od1HczlHIMBg",
        "outputId": "50dec615-99d8-4748-bb3e-55cfc4e88dea",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<24183x34420 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 354343 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_val_review_us_count_vec = count_vec.transform(df_X_val)\n",
        "X_val_review_us_count_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb-uLXb8IMBg"
      },
      "source": [
        "##### <a id=\"svm_with_ngrams\">N-grams</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exunlpbVIMBg",
        "outputId": "236c1082-14fe-4f50-d7a6-18ca00c4fb2f",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<56424x1181356 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 2516008 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ngrams = CountVectorizer(ngram_range=(1, 3))\n",
        "X_train_review_us_ngrams = ngrams.fit_transform(df_X_train)\n",
        "X_train_review_us_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaDUNcXvIMBh",
        "outputId": "6df332e5-d0e7-483b-88f3-47fe2039fd7a",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<24183x1181356 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 653856 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_val_review_us_ngrams = ngrams.transform(df_X_val)\n",
        "X_val_review_us_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiVPrpwy98h5",
        "outputId": "db9e7891-0bd9-41a3-e286-016a415926aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ThN5jOdJrVp_G2fK6t424HxooFWCsKFC\n",
            "To: /content/PRED-ngrams_transform.joblib\n",
            "100%|██████████| 30.5M/30.5M [00:00<00:00, 41.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# to load presaved model\n",
        "download_from_gdrive(\"1ThN5jOdJrVp_G2fK6t424HxooFWCsKFC\", \"PRED-ngrams_transform.joblib\")\n",
        "ngrams = joblib.load('PRED-ngrams_transform.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRLmXWoOIMBh"
      },
      "source": [
        "##### <a id=\"svm_with_count_vec\">TFIDFVectorizer</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu70GReYIMBh",
        "outputId": "2af46c8a-02bd-4fd0-a21d-185bbe1fab54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<56424x34420 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 827123 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_vec = TfidfVectorizer()\n",
        "X_train_review_us_tfidf_vec = tfidf_vec.fit_transform(df_X_train)\n",
        "X_train_review_us_tfidf_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKbX7PDPIMBh",
        "outputId": "36ae1681-603b-43f4-f41c-df57a8e5ad96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<24183x34420 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 354343 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_val_review_us_tfidf_vec = tfidf_vec.transform(df_X_val)\n",
        "X_val_review_us_tfidf_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDLwLBCDIMBh"
      },
      "source": [
        "##### <a id=\"svm_with_w2v\">Word2Vec</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quOCD7CVADo5"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "  if isinstance(text, (str, bytes)):\n",
        "    return word_tokenize(text)\n",
        "  else:\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPmSpO3HD87k"
      },
      "outputs": [],
      "source": [
        "X_train_review_us_tokenized = df_X_train.apply(tokenize_text)\n",
        "X_val_review_us_tokenized = df_X_val.apply(tokenize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txEOJrGfFXO2"
      },
      "outputs": [],
      "source": [
        "w2v_model_us = Word2Vec(X_train_review_us_tokenized, vector_size=100, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ounbcp3js6mv",
        "outputId": "50341c04-eb32-4f2a-a59b-a116aefa7637"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kf_P4P7tj9u8welDb64Agyc8W0FzrBJ5\n",
            "To: /content/PRED-w2v_us.joblib\n",
            "100%|██████████| 29.4M/29.4M [00:00<00:00, 173MB/s]\n"
          ]
        }
      ],
      "source": [
        "# to load presaved model\n",
        "download_from_gdrive(\"1kf_P4P7tj9u8welDb64Agyc8W0FzrBJ5\", \"PRED-w2v_us.joblib\")\n",
        "w2v_model_us = joblib.load('PRED-w2v_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZczS_TL4aEbc"
      },
      "outputs": [],
      "source": [
        "wv = w2v_model.wv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_tbuqND9jKu"
      },
      "outputs": [],
      "source": [
        "# vectorizing documents by averaging word embeddings\n",
        "def document_vectorizer(doc, model):\n",
        "  if isinstance(doc, (csr_matrix)):\n",
        "    return np.zeros(model.vector_size)\n",
        "\n",
        "  if isinstance(doc, (float)):\n",
        "    return np.zeros(model.vector_size)\n",
        "\n",
        "  # filter out words that are not in the model's vocabulary\n",
        "  words = [word for word in doc if word in model.wv.key_to_index]\n",
        "\n",
        "  # return the mean of the word vectors\n",
        "  if words:\n",
        "    return np.mean(model.wv[words], axis=0)\n",
        "  else:\n",
        "    return np.zeros(model.vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYTBnC-mFa-e"
      },
      "outputs": [],
      "source": [
        "# apply the vectorizer to each document in the corpus\n",
        "X_train_review_us_w2v = [document_vectorizer(doc, w2v_model_us) for doc in df_X_train]\n",
        "X_val_review_us_w2v = [document_vectorizer(doc, w2v_model_us) for doc in df_X_val]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdVslJFlW1xg"
      },
      "source": [
        "##### <a id=\"svm_with_d2v\">Doc2Vec</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFe5wr97W9w6"
      },
      "outputs": [],
      "source": [
        "tagged_data_train_us = [TaggedDocument(words=tokenize_text(text), tags=[str(i)]) for i, text in enumerate(df_X_train)]\n",
        "tagged_data_val_us = [TaggedDocument(words=tokenize_text(text), tags=[str(i)]) for i, text in enumerate(df_X_val)]\n",
        "\n",
        "# train doc2vec model on training data\n",
        "doc2vec = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=20)\n",
        "doc2vec.build_vocab(tagged_data_train_us)\n",
        "doc2vec.train(tagged_data_train_us, total_examples=doc2vec.corpus_count, epochs=doc2vec.epochs)\n",
        "\n",
        "# encode reviews using trained doc2vec model\n",
        "X_train_review_us_d2v = [doc2vec.infer_vector(doc.words) for doc in tagged_data_train_us]\n",
        "X_val_review_us_d2v = [doc2vec.infer_vector(doc.words) for doc in tagged_data_val_us]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VESnTIsnIMBi"
      },
      "source": [
        "##### <a id=\"svm_with_bpe\">Byte Pair Encoding</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuN6HKC-IMBi"
      },
      "outputs": [],
      "source": [
        "bpe_tokenizer = ByteLevelBPETokenizer()\n",
        "# bpe_tokenizer.train(X_train_reviews, special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"])\n",
        "# bpe_trainer = bpe_tokenizer.get_trainer(special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"])\n",
        "bpe_tokenizer.train_from_iterator(df_X_train)\n",
        "\n",
        "bpe_encoded_X_train_us = bpe_tokenizer.encode_batch(df_X_train)\n",
        "bpe_encoded_X_val_us = bpe_tokenizer.encode_batch(df_X_val)\n",
        "\n",
        "bpe_embeddings_X_train_us = [encoded.ids for encoded in bpe_encoded_X_train_us]\n",
        "bpe_embeddings_X_val_us = [encoded.ids for encoded in bpe_encoded_X_val_us]\n",
        "\n",
        "max_length = max(max(len(ids) for ids in bpe_embeddings_X_train_us), max(len(ids) for ids in bpe_embeddings_X_val_us))\n",
        "bpe_embeddings_X_train_us = [ids + [0] * (max_length - len(ids)) for ids in bpe_embeddings_X_train_us]\n",
        "bpe_embeddings_X_val_us = [ids + [0] * (max_length - len(ids)) for ids in bpe_embeddings_X_val_us]\n",
        "\n",
        "X_train_review_us_bpe = np.array(bpe_embeddings_X_train_us)\n",
        "X_val_review_us_bpe = np.array(bpe_embeddings_X_val_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPR6t2OlIMBi"
      },
      "source": [
        "##### <a id=\"svm_with_use\">Universal Sentence Encoder</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGJAISCGIMBi"
      },
      "outputs": [],
      "source": [
        "use_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "use_embed = hub.load(use_url)\n",
        "\n",
        "X_train_review_us_use = np.array(use_embed(df_X_train))\n",
        "X_val_review_us_use = np.array(use_embed(df_X_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcKaWFhHIMBi"
      },
      "source": [
        "##### <a id=\"svm_with_bert\">BERT (using pre-trained model)</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Undersampled"
      ],
      "metadata": {
        "id": "HOnc6GezNgf1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "010a4cf6787246e6be39630bf9578626",
            "6c0e63a676294798bd088da564519202",
            "bfc3b6c1881c4685b7fe55edfd814d06",
            "e46c7bd6182d48038ffb0386a53fa7f6",
            "8ce68d0bdcad445e9f5627bf98269cde",
            "281555ce846342e8aa659b09341cdaa4",
            "6ae3a9e87cb242c9a79c342c10a1a68b",
            "bdbd22e0701845a0aad28618192c6b4c",
            "6568fa324ce84cfe9224b45e4920091b",
            "b78f9fafa912492dbc6be17d7fe1e4f9",
            "c39b690018a0433f95322db8733428fb",
            "6a961b1ec8ae41f787cae5c1ad559b09",
            "c1d26b9151b94cacbb4f4e0efa8a158e",
            "017cb96a74694bb581c5d2c0ac17a085",
            "fa9f49f26e2a43c6988b485b1e2194af",
            "bb3eb4948be54735931ac0b30e99487e",
            "a49254cc79af4dcda97be43767ea1ff3",
            "c405af27cba44b02a1554c1249318db1",
            "b49291e00f484a5d8b8bb08744065b2b",
            "9eb0f17d3b6047e6aa77e9a038b67d1b",
            "74bf377da4894cca83714ea81b41ff9d",
            "e6a21bcb06454c9cbe6be591359c831b",
            "7b839fd6d5a44a33b1e24bd0e31cd92d",
            "47d7f92991184dea9b7c60d27c910ada",
            "4597ec8b5ad94a189c76d7c13be0fcae",
            "b23b2e9f3ccb46ba92632ac2735ef1c4",
            "bd878535b1af41849e0a75251329d30c",
            "ac7c7e584e924e7e9bc0fc812dc1613a",
            "1f3eda8533584bb0a71d1653818cff2d",
            "7e3da9ce65e34feba8bb9ebc148b7f2c",
            "f4b97b1813274bfb83c7ee255d9c2b06",
            "89b328f85e3d4f06a9227c671e5567c8",
            "82f8e569234d4e52b04f83d4393c9cbd",
            "00416e40850a41848ebd63b89e266c75",
            "0544d66066ef46e1b9c37b6a42f7eda2",
            "3817b916f7d5454eb9f9dbb0ce670a70",
            "25c21967900542c0a94525c8b4ff2b1d",
            "c29fc1bbee434d4199a3afff32c07a10",
            "77e498ae5c96448ba13143e1f1a1b86c",
            "a34dcd63a22f4b97ae82e7b7cb818473",
            "ffe95a1497b740fca111cf5781980091",
            "d919f806f3a74e2b83ac1cc763176851",
            "cb0d647002ed470ebd4c139d961898b6",
            "916b91bbc7314085bf0cd336b8bf6ffe",
            "6de711c299b642c5a2f6b529b7771470",
            "d208ffb1553d423293d5d07538724aa4",
            "3d1fc63cae85400bba24b34bbb04aeac",
            "1459d961bafa48309bc479aba9740c21",
            "fbe1a657d16a43759d27e8c11e798b42",
            "52268d3b7e8440388194427537746027",
            "8ea9d90fb6da488982b57fbac4d5cd71",
            "07f31aa3432d4384ae9b0d879abcf9ed",
            "3f04855f643640f68a618b48bbbcb1d3",
            "340283820c064b698cfcb809a5da7d35",
            "fa3f01d4725c4b0abe57766c78efe620"
          ]
        },
        "id": "wKI47ZoAIMBi",
        "outputId": "7f64d88f-a8ee-417c-a03f-1f6c9a67b860"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "010a4cf6787246e6be39630bf9578626"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a961b1ec8ae41f787cae5c1ad559b09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b839fd6d5a44a33b1e24bd0e31cd92d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00416e40850a41848ebd63b89e266c75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6de711c299b642c5a2f6b529b7771470"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "tokenized_text_train = [tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) for text in df_X_train]\n",
        "\n",
        "bert_embeddings_train = [model(**inputs).last_hidden_state.mean(dim=1).detach().numpy() for inputs in tokenized_text_train]\n",
        "\n",
        "X_train_reviews_bert = np.vstack(bert_embeddings_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiBPJafOIMBj"
      },
      "outputs": [],
      "source": [
        "tokenized_text_val = [tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) for text in df_X_val]\n",
        "\n",
        "bert_embeddings_val = [model(**inputs).last_hidden_state.mean(dim=1).detach().numpy() for inputs in tokenized_text_val]\n",
        "\n",
        "X_val_reviews_bert = np.vstack(bert_embeddings_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02943a6e-0bb7-4e38-e4a5-3f3dd427bcf4",
        "id": "kPmTC73fXywp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wWUySRzo0dy3pkthih5ecX0V7V6qtwbi\n",
            "To: /content/word_embedding__bert__X_train_reviews_bert.csv\n",
            "100%|██████████| 173M/173M [00:07<00:00, 22.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "file_id = '1wWUySRzo0dy3pkthih5ecX0V7V6qtwbi'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output = \"word_embedding__bert__X_train_reviews_bert.csv\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "X_train_reviews_bert = np.load('word_embedding__bert__X_train_reviews_bert.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to load pre-processed df\n",
        "# Download the dataset\n",
        "file_id = '15E93Y9lsFXsj5cOccy6pHNJaJMMvRkwI'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output = \"word_embedding__bert__X_val_reviews_bert.csv\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "X_val_reviews_bert = np.load('word_embedding__bert__X_val_reviews_bert.npy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrC6lJPzYdbr",
        "outputId": "50d7d219-dd42-471c-80db-6d7605d3ce3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15E93Y9lsFXsj5cOccy6pHNJaJMMvRkwI\n",
            "To: /content/word_embedding__bert__X_val_reviews_bert.csv\n",
            "100%|██████████| 74.3M/74.3M [00:04<00:00, 17.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gkeVlXmIMBo"
      },
      "source": [
        "#### <a id=\"fitting_models\">Fitting Models</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JjRHc91IMBo"
      },
      "outputs": [],
      "source": [
        "def get_scores(actual_y, pred_y, pred_proba_y):\n",
        "    print(\"Accuracy Score:\", accuracy_score(actual_y, pred_y))\n",
        "    print(\"Precision Score:\", precision_score(actual_y, pred_y))\n",
        "    print(\"Recall Score:\", recall_score(actual_y, pred_y))\n",
        "    print(\"F1 Score:\", f1_score(actual_y, pred_y))\n",
        "    print(\"ROC AUC Score:\", roc_auc_score(actual_y, pred_proba_y))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(actual_y, pred_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRJpwj-9vkvt"
      },
      "source": [
        "A simple but well-performing model according to our research, Support Vector Machine (SVM), has been used to compare the performance of the different embedding methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcPKy81KIMBo"
      },
      "source": [
        "##### CountVectorizer (BoW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "Zei-zADPDaYI",
        "outputId": "5b85f394-ae0d-427f-9b22-ca032e39f95c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(probability=True)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svc_clf__count_vec_us = SVC(probability=True)\n",
        "svc_clf__count_vec_us.fit(X_train_review_us_count_vec, df_y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_qN2U9xwaxq",
        "outputId": "43eee20f-4188-43a7-8555-a0055d5f4628"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Cqp9p_VDQbBAt3IbPkYVwf64Wewzv8GN\n",
            "To: /content/PRED-svc_clf__count_vec_us.joblib\n",
            "100%|██████████| 7.10M/7.10M [00:00<00:00, 24.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# to load presaved model\n",
        "download_from_gdrive(\"1Cqp9p_VDQbBAt3IbPkYVwf64Wewzv8GN\", \"PRED-svc_clf__count_vec_us.joblib\")\n",
        "svc_clf__count_vec_us = joblib.load('PRED-svc_clf__count_vec_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mx079xLIMBp"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__count_vec__train_us = svc_clf__count_vec_us.predict(X_train_review_us_count_vec)\n",
        "y_pred_proba__svc__count_vec__train_us = svc_clf__count_vec_us.predict_proba(X_train_review_us_count_vec)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR_ioPr7IMBp"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__count_vec__val_us = svc_clf__count_vec_us.predict(X_val_review_us_count_vec)\n",
        "y_pred_proba__svc__count_vec__val_us = svc_clf__count_vec_us.predict_proba(X_val_review_us_count_vec)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO5-WEJYIMBp",
        "outputId": "0a21a4cb-9161-46cf-c33e-dc20c91da8de",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.9288068906848149\n",
            "Precision Score: 0.9229513113606825\n",
            "Recall Score: 0.9420068027210884\n",
            "F1 Score: 0.9323817058595789\n",
            "ROC AUC Score: 0.9664278338629708\n",
            "Confusion Matrix:\n",
            " [[24712  2312]\n",
            " [ 1705 27695]]\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "get_scores(df_y_train, y_pred__svc__count_vec__train_us, y_pred_proba__svc__count_vec__train_us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drxjr1pjIMBp",
        "outputId": "50d52620-95af-4315-a282-ce203d89117f",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.8691642889633213\n",
            "Precision Score: 0.8775608194622279\n",
            "Recall Score: 0.8703174603174603\n",
            "F1 Score: 0.8739241313356709\n",
            "ROC AUC Score: 0.9321907824685602\n",
            "Confusion Matrix:\n",
            " [[10053  1530]\n",
            " [ 1634 10966]]\n"
          ]
        }
      ],
      "source": [
        "# val\n",
        "get_scores(df_y_val, y_pred__svc__count_vec__val_us, y_pred_proba__svc__count_vec__val_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHctqvKMIMBp"
      },
      "source": [
        "##### N-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "VvtrRw6kIMBp",
        "outputId": "3bd2d25b-2adb-4861-e82c-c60235888472"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(probability=True)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svc_clf__ngrams_us = SVC(probability=True)\n",
        "svc_clf__ngrams_us.fit(X_train_review_us_ngrams, df_y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvPvPlC5EXD7",
        "outputId": "a3f49794-129e-4127-8b29-349e779d3d1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['PRED-svc_clf__ngrams_us.joblib']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to save model\n",
        "# joblib.dump(svc_clf__ngrams_us, 'PRED-svc_clf__ngrams_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJQXUORYEXD8",
        "outputId": "3875e338-a65c-4e13-e29f-04f9ea906b27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lC7rJmD1fIyqdCnODXkVI4fLa_SLhp-t\n",
            "To: /content/PRED-svc_clf__ngrams_us.joblib\n",
            "100%|██████████| 23.4M/23.4M [00:00<00:00, 37.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# to load presaved model\n",
        "download_from_gdrive(\"1lC7rJmD1fIyqdCnODXkVI4fLa_SLhp-t\", \"PRED-svc_clf__ngrams_us.joblib\")\n",
        "svc_clf__ngrams_us = joblib.load('PRED-svc_clf__ngrams_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6shz_DWIMBp"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__ngrams__train_us = svc_clf__ngrams_us.predict(X_train_review_us_ngrams)\n",
        "y_pred_proba__svc__ngrams__train_us = svc_clf__ngrams_us.predict_proba(X_train_review_us_ngrams)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUOzQYgBIMBq"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__ngrams__val_us = svc_clf__ngrams_us.predict(X_val_review_us_ngrams)\n",
        "y_pred_proba__svc__ngrams__val_us = svc_clf__ngrams_us.predict_proba(X_val_review_us_ngrams)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Poqu0mIMBq",
        "outputId": "8db641c1-bc50-4ea8-ba3a-ea09bc5d3615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.9458740961293067\n",
            "Precision Score: 0.9374667906482466\n",
            "Recall Score: 0.9601700680272108\n",
            "F1 Score: 0.9486826186315366\n",
            "ROC AUC Score: 0.9753593889080203\n",
            "Confusion Matrix:\n",
            " [[25141  1883]\n",
            " [ 1171 28229]]\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "get_scores(df_y_train, y_pred__svc__ngrams__train_us, y_pred_proba__svc__ngrams__train_us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV91SgxOIMBq",
        "outputId": "e9dc260b-272e-407b-fa1d-52f4fc2e4a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.8728032088657321\n",
            "Precision Score: 0.8609216310444141\n",
            "Recall Score: 0.9015079365079365\n",
            "F1 Score: 0.8807474606497636\n",
            "ROC AUC Score: 0.9350059337096374\n",
            "Confusion Matrix:\n",
            " [[ 9748  1835]\n",
            " [ 1241 11359]]\n"
          ]
        }
      ],
      "source": [
        "# val\n",
        "get_scores(df_y_val, y_pred__svc__ngrams__val_us, y_pred_proba__svc__ngrams__val_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYjDptyIIMBr"
      },
      "source": [
        "##### TFIDFVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "3EHbJLnKIMBr",
        "outputId": "408e6b8c-677c-4267-e7a5-f87ed5ab2c8e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(probability=True)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svc_clf__tfidf_us = SVC(probability=True)\n",
        "svc_clf__tfidf_us.fit(X_train_review_us_tfidf_vec, df_y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba6YHZ1f3AXy",
        "outputId": "bd92d24c-7c39-40e7-b90c-c446c877f96f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['PRED-svc_clf__tfidf_us.joblib']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to save model\n",
        "joblib.dump(svc_clf__tfidf_us, 'PRED-svc_clf__tfidf_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyWTxEwt3AXz",
        "outputId": "00e41588-841e-42d9-8c62-7ca4d9f10650"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1p_MCQSc5MstJ6XIzcbCvSd6I3tmcp7Ko\n",
            "To: /content/PRED-svc_clf__tfidf_us.joblib\n",
            "100%|██████████| 5.68M/5.68M [00:00<00:00, 124MB/s]\n"
          ]
        }
      ],
      "source": [
        "# to load presaved model\n",
        "download_from_gdrive(\"1p_MCQSc5MstJ6XIzcbCvSd6I3tmcp7Ko\", \"PRED-svc_clf__tfidf_us.joblib\")\n",
        "svc_clf__tfidf_us = joblib.load('PRED-svc_clf__tfidf_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf8iTau9IMBr"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__tfidf_vec__train_us = svc_clf__tfidf_us.predict(X_train_review_us_tfidf_vec)\n",
        "y_pred_proba__svc__tfidf_vec__train_us = svc_clf__tfidf_us.predict_proba(X_train_review_us_tfidf_vec)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbuxbndwIMBr"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__tfidf_vec__val_us = svc_clf__tfidf_us.predict(X_val_review_us_tfidf_vec)\n",
        "y_pred_proba__svc__tfidf_vec__val_us = svc_clf__tfidf_us.predict_proba(X_val_review_us_tfidf_vec)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWr65SL5IMBr",
        "outputId": "64954d5b-9f95-4ffb-8161-681613373ac8",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.95705728058982\n",
            "Precision Score: 0.9566645224633511\n",
            "Recall Score: 0.9611224489795919\n",
            "F1 Score: 0.9588883044606953\n",
            "ROC AUC Score: 0.984009062365325\n",
            "Confusion Matrix:\n",
            " [[25744  1280]\n",
            " [ 1143 28257]]\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "get_scores(df_y_train, y_pred__svc__tfidf_vec__train_us, y_pred_proba__svc__tfidf_vec__train_us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX0l0f_JIMBr",
        "outputId": "714e23c5-ea51-4799-9b3f-6b98f2d0017d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.8835132117603275\n",
            "Precision Score: 0.887937187723055\n",
            "Recall Score: 0.8885714285714286\n",
            "F1 Score: 0.8882541949303819\n",
            "ROC AUC Score: 0.9405217758921464\n",
            "Confusion Matrix:\n",
            " [[10170  1413]\n",
            " [ 1404 11196]]\n"
          ]
        }
      ],
      "source": [
        "# val\n",
        "get_scores(df_y_val, y_pred__svc__tfidf_vec__val_us, y_pred_proba__svc__tfidf_vec__val_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8E4dTMGIMBs"
      },
      "source": [
        "##### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "YAuDsbR8yOvE",
        "outputId": "12a7eba2-2557-4334-c24f-8be273134ddd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(probability=True)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svc_clf__w2v_us = SVC(probability=True)\n",
        "svc_clf__w2v_us.fit(X_train_review_us_w2v, df_y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljg7vUyCbXCN",
        "outputId": "575667a4-3a73-4009-97ce-617a058d514e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['PRED-svc_clf__w2v_us.joblib']"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to save model\n",
        "# joblib.dump(svc_clf, 'PRED-svc_clf__w2v_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idl5U7U-bqwE",
        "outputId": "ab03f210-1dca-413b-b06a-fb386864e41d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1y6x_ZsmmNz_zFapwmF_5hBr2EL24_z5B\n",
            "To: /content/PRED-svc_clf__w2v_us.joblib\n",
            "100%|██████████| 37.7M/37.7M [00:01<00:00, 36.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# to load presaved model\n",
        "download_from_gdrive(\"1y6x_ZsmmNz_zFapwmF_5hBr2EL24_z5B\", \"PRED-svc_clf__w2v_us.joblib\")\n",
        "svc_clf__w2v_us = joblib.load('PRED-svc_clf__w2v_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70aFbcpGguyv"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__w2v__train_us = svc_clf__w2v_us.predict(X_train_review_us_w2v)\n",
        "y_pred_proba__svc__w2v__train_us = svc_clf__w2v_us.predict_proba(X_train_review_us_w2v)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qmoh4rcyOvU"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__w2v__val_us = svc_clf__w2v_us.predict(X_val_review_us_w2v)\n",
        "y_pred_proba__svc__w2v__val_us = svc_clf__w2v_us.predict_proba(X_val_review_us_w2v)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luLpYyB6yOvU",
        "outputId": "de548677-3d7b-48e8-bb2b-d0b4cf48656f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.6443422100327346\n",
            "Precision Score: 0.7144833948339483\n",
            "Recall Score: 0.5268707482993197\n",
            "F1 Score: 0.6064996084573218\n",
            "ROC AUC Score: 0.7021576749822186\n",
            "Confusion Matrix:\n",
            " [[20925  6190]\n",
            " [13910 15490]]\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "get_scores(df_y_train, y_pred__svc__w2v__train_us, y_pred_proba__svc__w2v__train_us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quROBGvCyOvU",
        "outputId": "a1310b13-36eb-406a-e761-b71dcc6ce7e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.6389083852854961\n",
            "Precision Score: 0.7080094991364422\n",
            "Recall Score: 0.5205555555555555\n",
            "F1 Score: 0.599981705085986\n",
            "ROC AUC Score: 0.6985135626117469\n",
            "Confusion Matrix:\n",
            " [[8916 2705]\n",
            " [6041 6559]]\n"
          ]
        }
      ],
      "source": [
        "# val\n",
        "get_scores(df_y_val, y_pred__svc__w2v__val_us, y_pred_proba__svc__w2v__val_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVfAbPH_X7rB"
      },
      "source": [
        "##### Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "KmU-8KFpYCkh",
        "outputId": "52132496-489d-4e08-9432-417aadd4127f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(probability=True)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svc_clf__d2v_us = SVC(probability=True)\n",
        "svc_clf__d2v_us.fit(X_train_review_us_d2v, df_y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NFGYf7MhbKr",
        "outputId": "65c83567-c244-4188-d1e9-917422dd1af3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['PRED-svc_clf__d2v_us.joblib']"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to save model\n",
        "# joblib.dump(svc_clf__d2v_us, 'PRED-svc_clf__d2v_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjCS1NqAhbKs",
        "outputId": "3c5d6ede-12ac-4dfe-e9c7-fdb9c5dced69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FkBFnNVQcLUIi3e2o6-k_5QvWxxZhAfz\n",
            "To: /content/PRED-svc_clf__d2v_us.joblib\n",
            "100%|██████████| 25.1M/25.1M [00:00<00:00, 30.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# to load presaved model\n",
        "download_from_gdrive(\"1FkBFnNVQcLUIi3e2o6-k_5QvWxxZhAfz\", \"PRED-svc_clf__d2v_us.joblib\")\n",
        "svc_clf__d2v_us = joblib.load('PRED-svc_clf__d2v_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXAgimJpYCki"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__d2v__train_us = svc_clf__d2v_us.predict(X_train_review_us_d2v)\n",
        "y_pred_proba__svc__d2v__train_us = svc_clf__d2v_us.predict_proba(X_train_review_us_d2v)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5KnL5_VYCki"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__d2v__val_us = svc_clf__d2v_us.predict(X_val_review_us_d2v)\n",
        "y_pred_proba__svc__d2v__val_us = svc_clf__d2v_us.predict_proba(X_val_review_us_d2v)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tMZfVwDYCki",
        "outputId": "449b744a-e022-4fcd-bdee-594e36b37ede",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.8592620161633348\n",
            "Precision Score: 0.8357900666604074\n",
            "Recall Score: 0.9083673469387755\n",
            "F1 Score: 0.8705686763483449\n",
            "ROC AUC Score: 0.9235953302280058\n",
            "Confusion Matrix:\n",
            " [[21777  5247]\n",
            " [ 2694 26706]]\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "get_scores(df_y_train, y_pred__svc__d2v__train_us, y_pred_proba__svc__d2v__train_us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkCyVV3sYCki",
        "outputId": "8dccd13f-b476-4dee-bfba-43f1cfb84674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.804159947070256\n",
            "Precision Score: 0.8069476971116315\n",
            "Recall Score: 0.8203968253968253\n",
            "F1 Score: 0.813616686343959\n",
            "ROC AUC Score: 0.8775600702452553\n",
            "Confusion Matrix:\n",
            " [[ 9110  2473]\n",
            " [ 2263 10337]]\n"
          ]
        }
      ],
      "source": [
        "# val\n",
        "get_scores(df_y_val, y_pred__svc__d2v__val_us, y_pred_proba__svc__d2v__val_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t54sW1URIMBt"
      },
      "source": [
        "##### Byte Pair Encoding (BPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl1p7BJmIMBt"
      },
      "outputs": [],
      "source": [
        "svc_clf__bpe_us = SVC(probability=True)\n",
        "svc_clf__bpe_us.fit(X_train_review_us_bpe, df_y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28BRIrZ151Me",
        "outputId": "0f7e62ae-fab3-47a3-8b9d-f988f8c33306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vDTej9aFDeu0pCmR9V1aYHuvX0bNdNM7\n",
            "To: /content/PRED-svc_clf__bpe_us.joblib\n",
            "100%|██████████| 233M/233M [00:02<00:00, 103MB/s] \n"
          ]
        }
      ],
      "source": [
        "# to load presaved model\n",
        "download_from_gdrive(\"1vDTej9aFDeu0pCmR9V1aYHuvX0bNdNM7\", \"PRED-svc_clf__bpe_us.joblib\")\n",
        "svc_clf__bpe_us = joblib.load('PRED-svc_clf__bpe_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syx7LIDDIMBu"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__bpe__train_us = svc_clf__bpe_us.predict(X_train_review_us_bpe)\n",
        "y_pred_proba__svc__bpe__train_us = svc_clf__bpe_us.predict_proba(X_train_review_us_bpe)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou6gg0S7IMBu"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__bpe__val_us = svc_clf__bpe_us.predict(X_val_review_us_bpe)\n",
        "y_pred_proba__svc__bpe__val_us = svc_clf__bpe_us.predict_proba(X_val_review_us_bpe)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX06xzcBIMBu",
        "outputId": "c534dece-362b-465b-ea72-4386912df96a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.6821210832269956\n",
            "Precision Score: 0.6636218314683717\n",
            "Recall Score: 0.7907482993197279\n",
            "F1 Score: 0.7216290042215048\n",
            "ROC AUC Score: 0.7449204498999127\n",
            "Confusion Matrix:\n",
            " [[15240 11784]\n",
            " [ 6152 23248]]\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "get_scores(df_y_train, y_pred__svc__bpe__train_us, y_pred_proba__svc__bpe__train_us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_MS6XyOIMBu",
        "outputId": "d709f782-abc5-4f20-ecdd-bd95e569c0af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.6354877393210107\n",
            "Precision Score: 0.6312322307745649\n",
            "Recall Score: 0.7224603174603175\n",
            "F1 Score: 0.6737722512120202\n",
            "ROC AUC Score: 0.690787391620725\n",
            "Confusion Matrix:\n",
            " [[6265 5318]\n",
            " [3497 9103]]\n"
          ]
        }
      ],
      "source": [
        "# val\n",
        "get_scores(df_y_val, y_pred__svc__bpe__val_us, y_pred_proba__svc__bpe__val_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDiFmQ16VAKf"
      },
      "source": [
        "##### Universal Sentence Encoder (USE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HOiajgmVQCk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "7319f86e-c9c3-4024-8909-017e30ea3021"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(probability=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "svc_clf__use_us = SVC(probability=True)\n",
        "svc_clf__use_us.fit(X_train_review_us_use, df_y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyThHp53UHeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01a799c-ccc6-467f-8377-0d75feb0b03c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PRED-svc_clf__use_us.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# to save model\n",
        "joblib.dump(svc_clf__use_us, 'PRED-svc_clf__use_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c666de-5c29-45c0-b51d-78430f2ab49f",
        "id": "T2oSMRxzUHeE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1liWZNRHz0HFFetvHno-5tFIFR3RF-jeW\n",
            "To: /content/PRED-svc_clf__use_us.joblib\n",
            "100%|██████████| 91.1M/91.1M [00:03<00:00, 23.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# to load presaved model\n",
        "download_from_gdrive(\"1liWZNRHz0HFFetvHno-5tFIFR3RF-jeW\", \"PRED-svc_clf__use_us.joblib\")\n",
        "svc_clf__use_us = joblib.load('PRED-svc_clf__use_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZG0-DsWVQJ6"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__use__train_us = svc_clf__use_us.predict(X_train_review_us_use)\n",
        "y_pred_proba__svc__use__train_us = svc_clf__use_us.predict_proba(X_train_review_us_use)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHGR2NejVQPD"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__use__val_us = svc_clf__use_us.predict(X_val_review_us_use)\n",
        "y_pred_proba__svc__use__val_us = svc_clf__use_us.predict_proba(X_val_review_us_use)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orT1r2_3VQSy",
        "outputId": "c53dac37-95c0-4062-8608-6078c54eea90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.9183503473699135\n",
            "Precision Score: 0.9193248317153199\n",
            "Recall Score: 0.9244217687074829\n",
            "F1 Score: 0.9218662551091362\n",
            "ROC AUC Score: 0.9680098680235861\n",
            "Confusion Matrix:\n",
            " [[24639  2385]\n",
            " [ 2222 27178]]\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "get_scores(df_y_train, y_pred__svc__use__train_us, y_pred_proba__svc__use__train_us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX6CLVCNVQXQ",
        "outputId": "54e362fd-080a-4277-bc66-895d611ac984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.8708596948269446\n",
            "Precision Score: 0.8755051905856248\n",
            "Recall Score: 0.8768253968253968\n",
            "F1 Score: 0.876164796383679\n",
            "ROC AUC Score: 0.9327849790812753\n",
            "Confusion Matrix:\n",
            " [[10012  1571]\n",
            " [ 1552 11048]]\n"
          ]
        }
      ],
      "source": [
        "# val\n",
        "get_scores(df_y_val, y_pred__svc__use__val_us, y_pred_proba__svc__use__val_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG7wQC8oIMBs"
      },
      "source": [
        "###### BERT (using pre-trained model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "A2gIg4v7IMBs",
        "outputId": "29c33d4a-9a41-4416-fdb5-0de10a009555"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(probability=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "svc_clf__bert_us = SVC(probability=True)\n",
        "svc_clf__bert_us.fit(X_train_reviews_bert, df_y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb584a24-0420-4d10-9cb0-0f03724b41b2",
        "id": "mv98BaBePP3I"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PRED-svc_clf__bert_us.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# to save model\n",
        "joblib.dump(svc_clf__bert_us, 'PRED-svc_clf__bert_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf97ea8-6345-456e-f7f8-11c894056da8",
        "id": "CvYfH0YPPP3I"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19lrIR7U5xfudavVz7JNHy5TKzf_WrizV\n",
            "To: /content/PRED-svc_clf__bert_us.joblib\n",
            "100%|██████████| 130M/130M [00:06<00:00, 20.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# to load presaved model\n",
        "download_from_gdrive(\"19lrIR7U5xfudavVz7JNHy5TKzf_WrizV\", \"PRED-svc_clf__bert_us.joblib\")\n",
        "svc_clf__bert_us = joblib.load('PRED-svc_clf__bert_us.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVgSbAvMIMBt"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__bert__train_us = svc_clf__bert_us.predict(X_train_reviews_bert)\n",
        "y_pred_proba__svc__bert__train_us = svc_clf__bert_us.predict_proba(X_train_reviews_bert)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C_x--yPIMBt"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__bert__val_us = svc_clf__bert_us.predict(X_val_reviews_bert)\n",
        "y_pred_proba__svc__bert__val_us = svc_clf__bert_us.predict_proba(X_val_reviews_bert)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yepkvd11IMBt",
        "outputId": "ac21aeab-c4cf-40fe-9fae-f0739ea68205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.8844995037572664\n",
            "Precision Score: 0.8865763421968442\n",
            "Recall Score: 0.8925170068027211\n",
            "F1 Score: 0.8895367561062427\n",
            "ROC AUC Score: 0.9440883877973925\n",
            "Confusion Matrix:\n",
            " [[23667  3357]\n",
            " [ 3160 26240]]\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "get_scores(df_y_train, y_pred__svc__bert__train_us, y_pred_proba__svc__bert__train_us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePG5NyXtIMBt",
        "outputId": "44478132-ca46-4b1e-b47c-c9f8a1c85c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.8631683413968491\n",
            "Precision Score: 0.867262234168709\n",
            "Recall Score: 0.8706349206349207\n",
            "F1 Score: 0.868945304764545\n",
            "ROC AUC Score: 0.9297261688928357\n",
            "Confusion Matrix:\n",
            " [[ 9904  1679]\n",
            " [ 1630 10970]]\n"
          ]
        }
      ],
      "source": [
        "# val\n",
        "get_scores(df_y_val, y_pred__svc__bert__val_us, y_pred_proba__svc__bert__val_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEoDLpHCjOu1"
      },
      "source": [
        "##### <a id=\"svm_with_count_vec\">TFIDFVectorizer</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "a4813860-d30f-404a-db45-86d574900f7f",
        "id": "BNzkNZcVjOu1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d89d4b19aad2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtfidf_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_train_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         )\n\u001b[0;32m-> 2133\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1386\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
          ]
        }
      ],
      "source": [
        "tfidf_vec = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vec.fit_transform(df_X_train)\n",
        "y_train_tfidf = tfidf_vec.fit_transform(df_y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a id=\"SVM\"> Support Vector Machine (SVM)</a>"
      ],
      "metadata": {
        "id": "4Oo8hsTydvxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "\n",
        "# loss = hinge means soft margin linear SVM\n",
        "clf= SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
        "\n",
        "clf.fit(X_train_tfidf, y_train_tfidf)\n",
        "y_pred = clf.predict(df_X_test)\n",
        "\n",
        "# evaluation\n",
        "accuracy = accuracy_score(df_y_test, y_pred)\n",
        "report = classification_report(df_y_test, y_pred)\n",
        "precision_score = precision_score(df_y_test, y_pred)\n",
        "recall_score = recall_score(df_y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of SVM using SGD: {accuracy}\")\n",
        "print(f\"Precision of SVM using SGD: {precision_score}\")\n",
        "print(f\"Recall of SVM using SGD: {recall_score}\")\n",
        "print(f\"Classification Report of SVM using SGD:\\n{report}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "nXQ7RKTZdyGO",
        "outputId": "74c3ca39-4d18-405a-8b15-3eb22508d402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-bc965cc52f74>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hinge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'optimal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_no_change\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_tfidf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a id=\"SVM\"> Support Vector Machine (SVM) with Hyperparameter Tuning </a>"
      ],
      "metadata": {
        "id": "Tp07vmHBdy4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "\n",
        "param_grid = {\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
        "    'penalty': ['l2'],\n",
        "    'max_iter': [500, 1000],\n",
        "    'learning_rate': ['optimal', 'constant', 'adaptive', 'invscaling'],\n",
        "    'eta0': [0.01, 0.1, 0.5],\n",
        "    'loss': ['hinge']\n",
        "}\n",
        "\n",
        "sgd_classifier = SGDClassifier()\n",
        "grid_search_sgd = GridSearchCV(sgd_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# train SGDClassifier on the resampled data with hyperparameter tuning\n",
        "grid_search_sgd.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "y_pred = grid_search_sgd.predict(df_X_test)\n",
        "\n",
        "# evaluation\n",
        "accuracy = accuracy_score(df_y_test, y_pred)\n",
        "report = classification_report(df_y_test, y_pred)\n",
        "precision_score = precision_score(df_y_test, y_pred)\n",
        "recall_score = recall_score(df_y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of SVM using SGD (Hyperparameter Tuning): {accuracy}\")\n",
        "print(f\"Precision of SVM using SGD (Hyperparameter Tuning): {precision_score}\")\n",
        "print(f\"Recall of SVM using SGD (Hyperparameter Tuning): {recall_score}\")\n",
        "print(f\"Classification Report of SVM using SGD (Hyperparameter Tuning):\\n{report}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SguMcC6CUguk",
        "outputId": "6eb130df-bc52-4418-ec9e-3d1f2ef3cb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM using SGD: 0.8769143752175427\n",
            "Precision of SVM using SGD: 0.8921478484737035\n",
            "Recall of SVM using SGD: 0.8542125187076327\n",
            "Classification Report of SVM using SGD:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88     11625\n",
            "           1       0.89      0.85      0.87     11359\n",
            "\n",
            "    accuracy                           0.88     22984\n",
            "   macro avg       0.88      0.88      0.88     22984\n",
            "weighted avg       0.88      0.88      0.88     22984\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a id=\"LR\"> Logistic Regression (LR)</a>"
      ],
      "metadata": {
        "id": "DwYBLw54d6mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "\n",
        "# use log loss means logistic regression\n",
        "clf= SGDClassifier(loss='log_loss', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
        "\n",
        "clf.fit(X_train_tfidf, y_train_tfidf)\n",
        "y_pred = clf.predict(df_X_test)\n",
        "\n",
        "# evaluation\n",
        "accuracy = accuracy_score(df_y_test, y_pred)\n",
        "report = classification_report(df_y_test, y_pred)\n",
        "precision_score = precision_score(df_y_test, y_pred)\n",
        "recall_score = recall_score(df_y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of Logistic Regression: {accuracy}\")\n",
        "print(f\"Precision of Logistic Regression: {precision_score}\")\n",
        "print(f\"Recall of Logistic Regression: {recall_score}\")\n",
        "print(f\"Classification Report of Logistic Regression:\\n{report}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw0MCtCNd7Uc",
        "outputId": "de9b4c97-c2f9-471f-d10c-ae172c2e2da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression: 0.8692568743473721\n",
            "Precision of Logistic Regression: 0.8799345097325814\n",
            "Recall of Logistic Regression: 0.8516594770666432\n",
            "Classification Report of Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.87     11625\n",
            "           1       0.88      0.85      0.87     11359\n",
            "\n",
            "    accuracy                           0.87     22984\n",
            "   macro avg       0.87      0.87      0.87     22984\n",
            "weighted avg       0.87      0.87      0.87     22984\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a id=\"LR\"> Logistic Regression (LR) with Hyperparameter Tuning</a>"
      ],
      "metadata": {
        "id": "SkqHkBoEd82q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "\n",
        "param_grid = {\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'penalty': ['l2'],\n",
        "    'max_iter': [500, 1000],\n",
        "    'learning_rate': ['optimal', 'constant'],\n",
        "    'eta0': [0.01, 0.1, 0.5]\n",
        "}\n",
        "\n",
        "clf = SGDClassifier(loss='log', fit_intercept=True, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
        "\n",
        "grid_search_lr = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_lr.fit(X_train_tfidf, y_train_tfidf)\n",
        "\n",
        "best_params = grid_search_lr.best_params_\n",
        "best_clf_lr = grid_search_lr.best_estimator_\n",
        "\n",
        "# use best estimator to make predictions\n",
        "y_pred = best_clf_lr.predict(df_X_test)\n",
        "\n",
        "# evaluate\n",
        "accuracy = accuracy_score(df_y_test, y_pred)\n",
        "report = classification_report(df_y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of Logistic Regression (Hyperparameter Tuning): {accuracy}\")\n",
        "print(f\"Precision of Logistic Regression (Hyperparameter Tuning): {precision_score}\")\n",
        "print(f\"Recall of Logistic Regression (Hyperparameter Tuning): {recall_score}\")\n",
        "print(f\"Classification Report of Logistic Regression (Hyperparameter Tuning):\\n{report}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJwSBYl1d-o8",
        "outputId": "54f8922e-ec26-416c-a33a-14ccf4977ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression (Hyperparameter Tuning): 0.870692655760529\n",
            "Precision of Logistic Regression (Hyperparameter Tuning): <function precision_score at 0x7c51f9376560>\n",
            "Recall of Logistic Regression (Hyperparameter Tuning): <function recall_score at 0x7c51f93765f0>\n",
            "Classification Report of Logistic Regression (Hyperparameter Tuning):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.87     11625\n",
            "           1       0.88      0.85      0.87     11359\n",
            "\n",
            "    accuracy                           0.87     22984\n",
            "   macro avg       0.87      0.87      0.87     22984\n",
            "weighted avg       0.87      0.87      0.87     22984\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a id=\"mlp\"> Multilayer Perceptron (MLP)</a>"
      ],
      "metadata": {
        "id": "9fxztnkryzdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word2Vec"
      ],
      "metadata": {
        "id": "2TNOHTjDzRte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize sentences and train Word2Vec model\n",
        "x_train_words = df_X_train.apply(word_tokenize).tolist()\n",
        "x_val_words = df_X_val.apply(word_tokenize).tolist()\n",
        "x_test_words = df_X_test.apply(word_tokenize).tolist()\n",
        "\n",
        "model = Word2Vec(sentences = x_train_words, vector_size = 100, window = 5, min_count = 1, workers = 4)\n",
        "model.save(\"word2vec_mlp.bin\")\n",
        "wv = model.wv\n",
        "\n",
        "# word embedding function\n",
        "def get_sentence_vectors(sentences):\n",
        "  vectors = []\n",
        "  for sentence in sentences:\n",
        "    sentence_vectors = [wv[word] for word in sentence if word in wv]\n",
        "    if len(sentence_vectors) == 0:\n",
        "      vectors.append([0]*100)\n",
        "    else:\n",
        "      vectors.append(np.mean(sentence_vectors, axis = 0))\n",
        "\n",
        "  return vectors\n",
        "\n",
        "# word embedding for each review in dataset\n",
        "x_train_vectors = get_sentence_vectors(x_train_words)\n",
        "x_val_vectors = get_sentence_vectors(x_val_words)\n",
        "x_test_vectors = get_sentence_vectors(x_test_words)"
      ],
      "metadata": {
        "id": "wchywMHvgjzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP Model"
      ],
      "metadata": {
        "id": "6NwFlhzpzuKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize MLP model\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, vector_length, hidden_size2, hidden_size3, hidden_size4, output_dim, dropout):\n",
        "    super(MLP, self).__init__()\n",
        "\n",
        "    # Defining & Initializing all the layers in our network\n",
        "    self.relu = nn.ReLU() # ReLU activation function\n",
        "    self.dropout = nn.Dropout(dropout) # Dropout function\n",
        "    self.fc1 = nn.Linear(vector_length, hidden_size2) # First dense layer\n",
        "    self.fc2 = nn.Linear(hidden_size2, hidden_size3) # Second dense layer\n",
        "    self.fc3 = nn.Linear(hidden_size3, hidden_size4) # Third dense layer\n",
        "    self.fc4 = nn.Linear(hidden_size4, output_dim) # Output layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x, 1) # Flatten\n",
        "    x = self.relu(self.fc1(x)) # Apply ReLU activation function to output of first layer\n",
        "    x = self.dropout(x) # Drop some nodes\n",
        "    x = self.relu(self.fc2(x)) # Apply ReLU activation function to output of second layer\n",
        "    x = self.dropout(x) # Drop some nodes\n",
        "    x = self.relu(self.fc3(x)) # Apply ReLU activation function to output of third layer\n",
        "    x = self.dropout(x) # Drop some nodes\n",
        "    x = self.fc4(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "xNxP369kgtuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameters"
      ],
      "metadata": {
        "id": "oxgZHyu70EXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  epochs = 10\n",
        "  lr = 0.01\n",
        "  use_cuda = True\n",
        "  gamma = 0.7\n",
        "  log_interval = 10\n",
        "  seed = 0\n",
        "\n",
        "args = Args()"
      ],
      "metadata": {
        "id": "y3CkOOlOz8Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train & Test Functions"
      ],
      "metadata": {
        "id": "121N4XSM0Lp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train function\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "\n",
        "  for batch_no, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    target = target.long()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "\n",
        "    loss = F.cross_entropy(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_no % args.log_interval ==0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_no*len(data), len(train_loader.dataset), 100. * batch_no/len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  predictions = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      pred = output.argmax(dim = 1, keepdim = True)\n",
        "      predictions.append(pred)\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
        "  return (correct, predictions)"
      ],
      "metadata": {
        "id": "qlPoTdqxqc1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Loading"
      ],
      "metadata": {
        "id": "E99qy4gsqeuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data to tensor\n",
        "x_train_tensor = torch.tensor(x_train_vectors)\n",
        "y_train_tensor = torch.tensor(df_y_train.values)\n",
        "x_val_tensor = torch.tensor(x_val_vectors)\n",
        "y_val_tensor = torch.tensor(df_y_val.values)\n",
        "x_test_tensor = torch.tensor(x_test_vectors)\n",
        "y_test_tensor = torch.tensor(df_y_test.values)\n",
        "\n",
        "# Saving the datasets\n",
        "# torch.save(x_train_tensor, \"x_train_tensor.pt\")\n",
        "# torch.save(y_train_tensor, \"y_train_tensor.pt\")\n",
        "# torch.save(x_val_tensor, \"x_val_tensor.pt\")\n",
        "# torch.save(y_val_tensor, \"y_val_tensor.pt\")\n",
        "# torch.save(x_test_tensor, \"x_test_tensor.pt\")\n",
        "# torch.save(y_test_tensor, \"y_test_tensor.pt\")\n",
        "\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "# dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size = 128, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 128, shuffle = False)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 128, shuffle = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnmO-piyhn9b",
        "outputId": "68276e39-e848-4970-9777-8872d2386354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-f946527786c7>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  x_train_tensor = torch.tensor(x_train_vectors)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Training"
      ],
      "metadata": {
        "id": "6wWdUFYP0Pfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = MLP(100, 128, 128, 128, 2, 0.3).to(device)\n",
        "\n",
        "optimizer = optim.Adadelta(model.parameters(), lr = args.lr)\n",
        "scheduler = StepLR(optimizer, step_size = 1, gamma = args.gamma)\n",
        "\n",
        "# Model training\n",
        "acc = 0\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "  train(args, model, device, train_loader, optimizer, epoch)\n",
        "  acc_ = test(model, device, val_loader)[0]\n",
        "  if acc_>acc or acc_ == acc:\n",
        "    acc = acc_\n",
        "    torch.save(model.state_dict(), \"MLP.pt\")\n",
        "\n",
        "  scheduler.step()\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "DMgD-tKLhkiN",
        "outputId": "7cdf77d0-82b6-4b36-b3b4-fdea33b3e93d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/288596 (0%)]\tLoss: 0.693782\n",
            "Train Epoch: 1 [1280/288596 (0%)]\tLoss: 0.693584\n",
            "Train Epoch: 1 [2560/288596 (1%)]\tLoss: 0.690542\n",
            "Train Epoch: 1 [3840/288596 (1%)]\tLoss: 0.691423\n",
            "Train Epoch: 1 [5120/288596 (2%)]\tLoss: 0.693509\n",
            "Train Epoch: 1 [6400/288596 (2%)]\tLoss: 0.694611\n",
            "Train Epoch: 1 [7680/288596 (3%)]\tLoss: 0.690860\n",
            "Train Epoch: 1 [8960/288596 (3%)]\tLoss: 0.692189\n",
            "Train Epoch: 1 [10240/288596 (4%)]\tLoss: 0.692836\n",
            "Train Epoch: 1 [11520/288596 (4%)]\tLoss: 0.685857\n",
            "Train Epoch: 1 [12800/288596 (4%)]\tLoss: 0.690412\n",
            "Train Epoch: 1 [14080/288596 (5%)]\tLoss: 0.689040\n",
            "Train Epoch: 1 [15360/288596 (5%)]\tLoss: 0.684946\n",
            "Train Epoch: 1 [16640/288596 (6%)]\tLoss: 0.687471\n",
            "Train Epoch: 1 [17920/288596 (6%)]\tLoss: 0.691391\n",
            "Train Epoch: 1 [19200/288596 (7%)]\tLoss: 0.688882\n",
            "Train Epoch: 1 [20480/288596 (7%)]\tLoss: 0.686205\n",
            "Train Epoch: 1 [21760/288596 (8%)]\tLoss: 0.688126\n",
            "Train Epoch: 1 [23040/288596 (8%)]\tLoss: 0.689317\n",
            "Train Epoch: 1 [24320/288596 (8%)]\tLoss: 0.685117\n",
            "Train Epoch: 1 [25600/288596 (9%)]\tLoss: 0.689751\n",
            "Train Epoch: 1 [26880/288596 (9%)]\tLoss: 0.681300\n",
            "Train Epoch: 1 [28160/288596 (10%)]\tLoss: 0.683549\n",
            "Train Epoch: 1 [29440/288596 (10%)]\tLoss: 0.683951\n",
            "Train Epoch: 1 [30720/288596 (11%)]\tLoss: 0.688274\n",
            "Train Epoch: 1 [32000/288596 (11%)]\tLoss: 0.683865\n",
            "Train Epoch: 1 [33280/288596 (12%)]\tLoss: 0.682616\n",
            "Train Epoch: 1 [34560/288596 (12%)]\tLoss: 0.680638\n",
            "Train Epoch: 1 [35840/288596 (12%)]\tLoss: 0.684594\n",
            "Train Epoch: 1 [37120/288596 (13%)]\tLoss: 0.685395\n",
            "Train Epoch: 1 [38400/288596 (13%)]\tLoss: 0.679257\n",
            "Train Epoch: 1 [39680/288596 (14%)]\tLoss: 0.679497\n",
            "Train Epoch: 1 [40960/288596 (14%)]\tLoss: 0.676115\n",
            "Train Epoch: 1 [42240/288596 (15%)]\tLoss: 0.675270\n",
            "Train Epoch: 1 [43520/288596 (15%)]\tLoss: 0.674257\n",
            "Train Epoch: 1 [44800/288596 (16%)]\tLoss: 0.678227\n",
            "Train Epoch: 1 [46080/288596 (16%)]\tLoss: 0.680336\n",
            "Train Epoch: 1 [47360/288596 (16%)]\tLoss: 0.682914\n",
            "Train Epoch: 1 [48640/288596 (17%)]\tLoss: 0.681267\n",
            "Train Epoch: 1 [49920/288596 (17%)]\tLoss: 0.672068\n",
            "Train Epoch: 1 [51200/288596 (18%)]\tLoss: 0.677613\n",
            "Train Epoch: 1 [52480/288596 (18%)]\tLoss: 0.673542\n",
            "Train Epoch: 1 [53760/288596 (19%)]\tLoss: 0.681283\n",
            "Train Epoch: 1 [55040/288596 (19%)]\tLoss: 0.674293\n",
            "Train Epoch: 1 [56320/288596 (20%)]\tLoss: 0.669857\n",
            "Train Epoch: 1 [57600/288596 (20%)]\tLoss: 0.670170\n",
            "Train Epoch: 1 [58880/288596 (20%)]\tLoss: 0.675182\n",
            "Train Epoch: 1 [60160/288596 (21%)]\tLoss: 0.670771\n",
            "Train Epoch: 1 [61440/288596 (21%)]\tLoss: 0.663659\n",
            "Train Epoch: 1 [62720/288596 (22%)]\tLoss: 0.672094\n",
            "Train Epoch: 1 [64000/288596 (22%)]\tLoss: 0.667089\n",
            "Train Epoch: 1 [65280/288596 (23%)]\tLoss: 0.665314\n",
            "Train Epoch: 1 [66560/288596 (23%)]\tLoss: 0.660766\n",
            "Train Epoch: 1 [67840/288596 (24%)]\tLoss: 0.659078\n",
            "Train Epoch: 1 [69120/288596 (24%)]\tLoss: 0.655093\n",
            "Train Epoch: 1 [70400/288596 (24%)]\tLoss: 0.663585\n",
            "Train Epoch: 1 [71680/288596 (25%)]\tLoss: 0.660621\n",
            "Train Epoch: 1 [72960/288596 (25%)]\tLoss: 0.664852\n",
            "Train Epoch: 1 [74240/288596 (26%)]\tLoss: 0.654797\n",
            "Train Epoch: 1 [75520/288596 (26%)]\tLoss: 0.655882\n",
            "Train Epoch: 1 [76800/288596 (27%)]\tLoss: 0.655352\n",
            "Train Epoch: 1 [78080/288596 (27%)]\tLoss: 0.644238\n",
            "Train Epoch: 1 [79360/288596 (27%)]\tLoss: 0.656045\n",
            "Train Epoch: 1 [80640/288596 (28%)]\tLoss: 0.641368\n",
            "Train Epoch: 1 [81920/288596 (28%)]\tLoss: 0.649939\n",
            "Train Epoch: 1 [83200/288596 (29%)]\tLoss: 0.639317\n",
            "Train Epoch: 1 [84480/288596 (29%)]\tLoss: 0.652025\n",
            "Train Epoch: 1 [85760/288596 (30%)]\tLoss: 0.628484\n",
            "Train Epoch: 1 [87040/288596 (30%)]\tLoss: 0.649024\n",
            "Train Epoch: 1 [88320/288596 (31%)]\tLoss: 0.629618\n",
            "Train Epoch: 1 [89600/288596 (31%)]\tLoss: 0.639187\n",
            "Train Epoch: 1 [90880/288596 (31%)]\tLoss: 0.618042\n",
            "Train Epoch: 1 [92160/288596 (32%)]\tLoss: 0.626638\n",
            "Train Epoch: 1 [93440/288596 (32%)]\tLoss: 0.621920\n",
            "Train Epoch: 1 [94720/288596 (33%)]\tLoss: 0.623324\n",
            "Train Epoch: 1 [96000/288596 (33%)]\tLoss: 0.624632\n",
            "Train Epoch: 1 [97280/288596 (34%)]\tLoss: 0.605619\n",
            "Train Epoch: 1 [98560/288596 (34%)]\tLoss: 0.627125\n",
            "Train Epoch: 1 [99840/288596 (35%)]\tLoss: 0.620738\n",
            "Train Epoch: 1 [101120/288596 (35%)]\tLoss: 0.601396\n",
            "Train Epoch: 1 [102400/288596 (35%)]\tLoss: 0.611752\n",
            "Train Epoch: 1 [103680/288596 (36%)]\tLoss: 0.605364\n",
            "Train Epoch: 1 [104960/288596 (36%)]\tLoss: 0.617365\n",
            "Train Epoch: 1 [106240/288596 (37%)]\tLoss: 0.578538\n",
            "Train Epoch: 1 [107520/288596 (37%)]\tLoss: 0.597604\n",
            "Train Epoch: 1 [108800/288596 (38%)]\tLoss: 0.597919\n",
            "Train Epoch: 1 [110080/288596 (38%)]\tLoss: 0.592641\n",
            "Train Epoch: 1 [111360/288596 (39%)]\tLoss: 0.577995\n",
            "Train Epoch: 1 [112640/288596 (39%)]\tLoss: 0.581475\n",
            "Train Epoch: 1 [113920/288596 (39%)]\tLoss: 0.578491\n",
            "Train Epoch: 1 [115200/288596 (40%)]\tLoss: 0.548450\n",
            "Train Epoch: 1 [116480/288596 (40%)]\tLoss: 0.559534\n",
            "Train Epoch: 1 [117760/288596 (41%)]\tLoss: 0.549837\n",
            "Train Epoch: 1 [119040/288596 (41%)]\tLoss: 0.560466\n",
            "Train Epoch: 1 [120320/288596 (42%)]\tLoss: 0.557449\n",
            "Train Epoch: 1 [121600/288596 (42%)]\tLoss: 0.574198\n",
            "Train Epoch: 1 [122880/288596 (43%)]\tLoss: 0.538309\n",
            "Train Epoch: 1 [124160/288596 (43%)]\tLoss: 0.525822\n",
            "Train Epoch: 1 [125440/288596 (43%)]\tLoss: 0.537620\n",
            "Train Epoch: 1 [126720/288596 (44%)]\tLoss: 0.542107\n",
            "Train Epoch: 1 [128000/288596 (44%)]\tLoss: 0.547676\n",
            "Train Epoch: 1 [129280/288596 (45%)]\tLoss: 0.520314\n",
            "Train Epoch: 1 [130560/288596 (45%)]\tLoss: 0.501384\n",
            "Train Epoch: 1 [131840/288596 (46%)]\tLoss: 0.520367\n",
            "Train Epoch: 1 [133120/288596 (46%)]\tLoss: 0.601779\n",
            "Train Epoch: 1 [134400/288596 (47%)]\tLoss: 0.503322\n",
            "Train Epoch: 1 [135680/288596 (47%)]\tLoss: 0.515012\n",
            "Train Epoch: 1 [136960/288596 (47%)]\tLoss: 0.489082\n",
            "Train Epoch: 1 [138240/288596 (48%)]\tLoss: 0.516448\n",
            "Train Epoch: 1 [139520/288596 (48%)]\tLoss: 0.491165\n",
            "Train Epoch: 1 [140800/288596 (49%)]\tLoss: 0.550160\n",
            "Train Epoch: 1 [142080/288596 (49%)]\tLoss: 0.497360\n",
            "Train Epoch: 1 [143360/288596 (50%)]\tLoss: 0.500600\n",
            "Train Epoch: 1 [144640/288596 (50%)]\tLoss: 0.472350\n",
            "Train Epoch: 1 [145920/288596 (51%)]\tLoss: 0.522201\n",
            "Train Epoch: 1 [147200/288596 (51%)]\tLoss: 0.460817\n",
            "Train Epoch: 1 [148480/288596 (51%)]\tLoss: 0.491279\n",
            "Train Epoch: 1 [149760/288596 (52%)]\tLoss: 0.476693\n",
            "Train Epoch: 1 [151040/288596 (52%)]\tLoss: 0.555168\n",
            "Train Epoch: 1 [152320/288596 (53%)]\tLoss: 0.460846\n",
            "Train Epoch: 1 [153600/288596 (53%)]\tLoss: 0.452541\n",
            "Train Epoch: 1 [154880/288596 (54%)]\tLoss: 0.457530\n",
            "Train Epoch: 1 [156160/288596 (54%)]\tLoss: 0.444209\n",
            "Train Epoch: 1 [157440/288596 (55%)]\tLoss: 0.430641\n",
            "Train Epoch: 1 [158720/288596 (55%)]\tLoss: 0.440677\n",
            "Train Epoch: 1 [160000/288596 (55%)]\tLoss: 0.480040\n",
            "Train Epoch: 1 [161280/288596 (56%)]\tLoss: 0.423391\n",
            "Train Epoch: 1 [162560/288596 (56%)]\tLoss: 0.487396\n",
            "Train Epoch: 1 [163840/288596 (57%)]\tLoss: 0.535769\n",
            "Train Epoch: 1 [165120/288596 (57%)]\tLoss: 0.470008\n",
            "Train Epoch: 1 [166400/288596 (58%)]\tLoss: 0.432374\n",
            "Train Epoch: 1 [167680/288596 (58%)]\tLoss: 0.437063\n",
            "Train Epoch: 1 [168960/288596 (59%)]\tLoss: 0.442880\n",
            "Train Epoch: 1 [170240/288596 (59%)]\tLoss: 0.406478\n",
            "Train Epoch: 1 [171520/288596 (59%)]\tLoss: 0.415247\n",
            "Train Epoch: 1 [172800/288596 (60%)]\tLoss: 0.452320\n",
            "Train Epoch: 1 [174080/288596 (60%)]\tLoss: 0.427077\n",
            "Train Epoch: 1 [175360/288596 (61%)]\tLoss: 0.435224\n",
            "Train Epoch: 1 [176640/288596 (61%)]\tLoss: 0.440438\n",
            "Train Epoch: 1 [177920/288596 (62%)]\tLoss: 0.443958\n",
            "Train Epoch: 1 [179200/288596 (62%)]\tLoss: 0.486228\n",
            "Train Epoch: 1 [180480/288596 (63%)]\tLoss: 0.392663\n",
            "Train Epoch: 1 [181760/288596 (63%)]\tLoss: 0.400165\n",
            "Train Epoch: 1 [183040/288596 (63%)]\tLoss: 0.425575\n",
            "Train Epoch: 1 [184320/288596 (64%)]\tLoss: 0.398524\n",
            "Train Epoch: 1 [185600/288596 (64%)]\tLoss: 0.425046\n",
            "Train Epoch: 1 [186880/288596 (65%)]\tLoss: 0.417684\n",
            "Train Epoch: 1 [188160/288596 (65%)]\tLoss: 0.423924\n",
            "Train Epoch: 1 [189440/288596 (66%)]\tLoss: 0.413428\n",
            "Train Epoch: 1 [190720/288596 (66%)]\tLoss: 0.384698\n",
            "Train Epoch: 1 [192000/288596 (67%)]\tLoss: 0.356039\n",
            "Train Epoch: 1 [193280/288596 (67%)]\tLoss: 0.508002\n",
            "Train Epoch: 1 [194560/288596 (67%)]\tLoss: 0.429747\n",
            "Train Epoch: 1 [195840/288596 (68%)]\tLoss: 0.407590\n",
            "Train Epoch: 1 [197120/288596 (68%)]\tLoss: 0.472114\n",
            "Train Epoch: 1 [198400/288596 (69%)]\tLoss: 0.483848\n",
            "Train Epoch: 1 [199680/288596 (69%)]\tLoss: 0.463406\n",
            "Train Epoch: 1 [200960/288596 (70%)]\tLoss: 0.405996\n",
            "Train Epoch: 1 [202240/288596 (70%)]\tLoss: 0.432936\n",
            "Train Epoch: 1 [203520/288596 (71%)]\tLoss: 0.436702\n",
            "Train Epoch: 1 [204800/288596 (71%)]\tLoss: 0.343445\n",
            "Train Epoch: 1 [206080/288596 (71%)]\tLoss: 0.392085\n",
            "Train Epoch: 1 [207360/288596 (72%)]\tLoss: 0.368765\n",
            "Train Epoch: 1 [208640/288596 (72%)]\tLoss: 0.478320\n",
            "Train Epoch: 1 [209920/288596 (73%)]\tLoss: 0.412851\n",
            "Train Epoch: 1 [211200/288596 (73%)]\tLoss: 0.561142\n",
            "Train Epoch: 1 [212480/288596 (74%)]\tLoss: 0.416030\n",
            "Train Epoch: 1 [213760/288596 (74%)]\tLoss: 0.432811\n",
            "Train Epoch: 1 [215040/288596 (75%)]\tLoss: 0.436510\n",
            "Train Epoch: 1 [216320/288596 (75%)]\tLoss: 0.408554\n",
            "Train Epoch: 1 [217600/288596 (75%)]\tLoss: 0.360731\n",
            "Train Epoch: 1 [218880/288596 (76%)]\tLoss: 0.432704\n",
            "Train Epoch: 1 [220160/288596 (76%)]\tLoss: 0.403742\n",
            "Train Epoch: 1 [221440/288596 (77%)]\tLoss: 0.358358\n",
            "Train Epoch: 1 [222720/288596 (77%)]\tLoss: 0.426677\n",
            "Train Epoch: 1 [224000/288596 (78%)]\tLoss: 0.408756\n",
            "Train Epoch: 1 [225280/288596 (78%)]\tLoss: 0.472161\n",
            "Train Epoch: 1 [226560/288596 (78%)]\tLoss: 0.366169\n",
            "Train Epoch: 1 [227840/288596 (79%)]\tLoss: 0.433728\n",
            "Train Epoch: 1 [229120/288596 (79%)]\tLoss: 0.350145\n",
            "Train Epoch: 1 [230400/288596 (80%)]\tLoss: 0.409876\n",
            "Train Epoch: 1 [231680/288596 (80%)]\tLoss: 0.387573\n",
            "Train Epoch: 1 [232960/288596 (81%)]\tLoss: 0.420386\n",
            "Train Epoch: 1 [234240/288596 (81%)]\tLoss: 0.419618\n",
            "Train Epoch: 1 [235520/288596 (82%)]\tLoss: 0.402086\n",
            "Train Epoch: 1 [236800/288596 (82%)]\tLoss: 0.389203\n",
            "Train Epoch: 1 [238080/288596 (82%)]\tLoss: 0.368365\n",
            "Train Epoch: 1 [239360/288596 (83%)]\tLoss: 0.363494\n",
            "Train Epoch: 1 [240640/288596 (83%)]\tLoss: 0.402113\n",
            "Train Epoch: 1 [241920/288596 (84%)]\tLoss: 0.380658\n",
            "Train Epoch: 1 [243200/288596 (84%)]\tLoss: 0.445771\n",
            "Train Epoch: 1 [244480/288596 (85%)]\tLoss: 0.384002\n",
            "Train Epoch: 1 [245760/288596 (85%)]\tLoss: 0.442865\n",
            "Train Epoch: 1 [247040/288596 (86%)]\tLoss: 0.401700\n",
            "Train Epoch: 1 [248320/288596 (86%)]\tLoss: 0.414003\n",
            "Train Epoch: 1 [249600/288596 (86%)]\tLoss: 0.378864\n",
            "Train Epoch: 1 [250880/288596 (87%)]\tLoss: 0.364550\n",
            "Train Epoch: 1 [252160/288596 (87%)]\tLoss: 0.390183\n",
            "Train Epoch: 1 [253440/288596 (88%)]\tLoss: 0.429121\n",
            "Train Epoch: 1 [254720/288596 (88%)]\tLoss: 0.391669\n",
            "Train Epoch: 1 [256000/288596 (89%)]\tLoss: 0.360324\n",
            "Train Epoch: 1 [257280/288596 (89%)]\tLoss: 0.416458\n",
            "Train Epoch: 1 [258560/288596 (90%)]\tLoss: 0.394215\n",
            "Train Epoch: 1 [259840/288596 (90%)]\tLoss: 0.352383\n",
            "Train Epoch: 1 [261120/288596 (90%)]\tLoss: 0.441705\n",
            "Train Epoch: 1 [262400/288596 (91%)]\tLoss: 0.343577\n",
            "Train Epoch: 1 [263680/288596 (91%)]\tLoss: 0.405756\n",
            "Train Epoch: 1 [264960/288596 (92%)]\tLoss: 0.356406\n",
            "Train Epoch: 1 [266240/288596 (92%)]\tLoss: 0.380983\n",
            "Train Epoch: 1 [267520/288596 (93%)]\tLoss: 0.369689\n",
            "Train Epoch: 1 [268800/288596 (93%)]\tLoss: 0.413284\n",
            "Train Epoch: 1 [270080/288596 (94%)]\tLoss: 0.469224\n",
            "Train Epoch: 1 [271360/288596 (94%)]\tLoss: 0.307246\n",
            "Train Epoch: 1 [272640/288596 (94%)]\tLoss: 0.456059\n",
            "Train Epoch: 1 [273920/288596 (95%)]\tLoss: 0.396350\n",
            "Train Epoch: 1 [275200/288596 (95%)]\tLoss: 0.351274\n",
            "Train Epoch: 1 [276480/288596 (96%)]\tLoss: 0.329531\n",
            "Train Epoch: 1 [277760/288596 (96%)]\tLoss: 0.426488\n",
            "Train Epoch: 1 [279040/288596 (97%)]\tLoss: 0.373386\n",
            "Train Epoch: 1 [280320/288596 (97%)]\tLoss: 0.262161\n",
            "Train Epoch: 1 [281600/288596 (98%)]\tLoss: 0.445798\n",
            "Train Epoch: 1 [282880/288596 (98%)]\tLoss: 0.370097\n",
            "Train Epoch: 1 [284160/288596 (98%)]\tLoss: 0.334414\n",
            "Train Epoch: 1 [285440/288596 (99%)]\tLoss: 0.380700\n",
            "Train Epoch: 1 [286720/288596 (99%)]\tLoss: 0.548436\n",
            "Train Epoch: 1 [288000/288596 (100%)]\tLoss: 0.519869\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 105438/123684 (85%)\n",
            "\n",
            "Train Epoch: 2 [0/288596 (0%)]\tLoss: 0.276658\n",
            "Train Epoch: 2 [1280/288596 (0%)]\tLoss: 0.422209\n",
            "Train Epoch: 2 [2560/288596 (1%)]\tLoss: 0.378954\n",
            "Train Epoch: 2 [3840/288596 (1%)]\tLoss: 0.416022\n",
            "Train Epoch: 2 [5120/288596 (2%)]\tLoss: 0.338590\n",
            "Train Epoch: 2 [6400/288596 (2%)]\tLoss: 0.401569\n",
            "Train Epoch: 2 [7680/288596 (3%)]\tLoss: 0.353595\n",
            "Train Epoch: 2 [8960/288596 (3%)]\tLoss: 0.340830\n",
            "Train Epoch: 2 [10240/288596 (4%)]\tLoss: 0.390329\n",
            "Train Epoch: 2 [11520/288596 (4%)]\tLoss: 0.318020\n",
            "Train Epoch: 2 [12800/288596 (4%)]\tLoss: 0.384001\n",
            "Train Epoch: 2 [14080/288596 (5%)]\tLoss: 0.397816\n",
            "Train Epoch: 2 [15360/288596 (5%)]\tLoss: 0.477325\n",
            "Train Epoch: 2 [16640/288596 (6%)]\tLoss: 0.433727\n",
            "Train Epoch: 2 [17920/288596 (6%)]\tLoss: 0.329586\n",
            "Train Epoch: 2 [19200/288596 (7%)]\tLoss: 0.342047\n",
            "Train Epoch: 2 [20480/288596 (7%)]\tLoss: 0.476543\n",
            "Train Epoch: 2 [21760/288596 (8%)]\tLoss: 0.402568\n",
            "Train Epoch: 2 [23040/288596 (8%)]\tLoss: 0.344185\n",
            "Train Epoch: 2 [24320/288596 (8%)]\tLoss: 0.395599\n",
            "Train Epoch: 2 [25600/288596 (9%)]\tLoss: 0.423134\n",
            "Train Epoch: 2 [26880/288596 (9%)]\tLoss: 0.355589\n",
            "Train Epoch: 2 [28160/288596 (10%)]\tLoss: 0.377202\n",
            "Train Epoch: 2 [29440/288596 (10%)]\tLoss: 0.454859\n",
            "Train Epoch: 2 [30720/288596 (11%)]\tLoss: 0.393344\n",
            "Train Epoch: 2 [32000/288596 (11%)]\tLoss: 0.460677\n",
            "Train Epoch: 2 [33280/288596 (12%)]\tLoss: 0.378892\n",
            "Train Epoch: 2 [34560/288596 (12%)]\tLoss: 0.334017\n",
            "Train Epoch: 2 [35840/288596 (12%)]\tLoss: 0.364352\n",
            "Train Epoch: 2 [37120/288596 (13%)]\tLoss: 0.464521\n",
            "Train Epoch: 2 [38400/288596 (13%)]\tLoss: 0.470414\n",
            "Train Epoch: 2 [39680/288596 (14%)]\tLoss: 0.395936\n",
            "Train Epoch: 2 [40960/288596 (14%)]\tLoss: 0.398433\n",
            "Train Epoch: 2 [42240/288596 (15%)]\tLoss: 0.413194\n",
            "Train Epoch: 2 [43520/288596 (15%)]\tLoss: 0.395906\n",
            "Train Epoch: 2 [44800/288596 (16%)]\tLoss: 0.366530\n",
            "Train Epoch: 2 [46080/288596 (16%)]\tLoss: 0.429454\n",
            "Train Epoch: 2 [47360/288596 (16%)]\tLoss: 0.507368\n",
            "Train Epoch: 2 [48640/288596 (17%)]\tLoss: 0.386599\n",
            "Train Epoch: 2 [49920/288596 (17%)]\tLoss: 0.300900\n",
            "Train Epoch: 2 [51200/288596 (18%)]\tLoss: 0.374983\n",
            "Train Epoch: 2 [52480/288596 (18%)]\tLoss: 0.342256\n",
            "Train Epoch: 2 [53760/288596 (19%)]\tLoss: 0.366857\n",
            "Train Epoch: 2 [55040/288596 (19%)]\tLoss: 0.380298\n",
            "Train Epoch: 2 [56320/288596 (20%)]\tLoss: 0.419188\n",
            "Train Epoch: 2 [57600/288596 (20%)]\tLoss: 0.470856\n",
            "Train Epoch: 2 [58880/288596 (20%)]\tLoss: 0.438992\n",
            "Train Epoch: 2 [60160/288596 (21%)]\tLoss: 0.426712\n",
            "Train Epoch: 2 [61440/288596 (21%)]\tLoss: 0.384740\n",
            "Train Epoch: 2 [62720/288596 (22%)]\tLoss: 0.337522\n",
            "Train Epoch: 2 [64000/288596 (22%)]\tLoss: 0.409742\n",
            "Train Epoch: 2 [65280/288596 (23%)]\tLoss: 0.382886\n",
            "Train Epoch: 2 [66560/288596 (23%)]\tLoss: 0.354976\n",
            "Train Epoch: 2 [67840/288596 (24%)]\tLoss: 0.387065\n",
            "Train Epoch: 2 [69120/288596 (24%)]\tLoss: 0.311891\n",
            "Train Epoch: 2 [70400/288596 (24%)]\tLoss: 0.440926\n",
            "Train Epoch: 2 [71680/288596 (25%)]\tLoss: 0.432182\n",
            "Train Epoch: 2 [72960/288596 (25%)]\tLoss: 0.331973\n",
            "Train Epoch: 2 [74240/288596 (26%)]\tLoss: 0.418911\n",
            "Train Epoch: 2 [75520/288596 (26%)]\tLoss: 0.398372\n",
            "Train Epoch: 2 [76800/288596 (27%)]\tLoss: 0.307226\n",
            "Train Epoch: 2 [78080/288596 (27%)]\tLoss: 0.340233\n",
            "Train Epoch: 2 [79360/288596 (27%)]\tLoss: 0.477200\n",
            "Train Epoch: 2 [80640/288596 (28%)]\tLoss: 0.374421\n",
            "Train Epoch: 2 [81920/288596 (28%)]\tLoss: 0.394612\n",
            "Train Epoch: 2 [83200/288596 (29%)]\tLoss: 0.357860\n",
            "Train Epoch: 2 [84480/288596 (29%)]\tLoss: 0.444771\n",
            "Train Epoch: 2 [85760/288596 (30%)]\tLoss: 0.297652\n",
            "Train Epoch: 2 [87040/288596 (30%)]\tLoss: 0.419580\n",
            "Train Epoch: 2 [88320/288596 (31%)]\tLoss: 0.296055\n",
            "Train Epoch: 2 [89600/288596 (31%)]\tLoss: 0.390068\n",
            "Train Epoch: 2 [90880/288596 (31%)]\tLoss: 0.347175\n",
            "Train Epoch: 2 [92160/288596 (32%)]\tLoss: 0.322069\n",
            "Train Epoch: 2 [93440/288596 (32%)]\tLoss: 0.363734\n",
            "Train Epoch: 2 [94720/288596 (33%)]\tLoss: 0.452662\n",
            "Train Epoch: 2 [96000/288596 (33%)]\tLoss: 0.456601\n",
            "Train Epoch: 2 [97280/288596 (34%)]\tLoss: 0.380146\n",
            "Train Epoch: 2 [98560/288596 (34%)]\tLoss: 0.402730\n",
            "Train Epoch: 2 [99840/288596 (35%)]\tLoss: 0.404780\n",
            "Train Epoch: 2 [101120/288596 (35%)]\tLoss: 0.331733\n",
            "Train Epoch: 2 [102400/288596 (35%)]\tLoss: 0.322694\n",
            "Train Epoch: 2 [103680/288596 (36%)]\tLoss: 0.456464\n",
            "Train Epoch: 2 [104960/288596 (36%)]\tLoss: 0.431915\n",
            "Train Epoch: 2 [106240/288596 (37%)]\tLoss: 0.411904\n",
            "Train Epoch: 2 [107520/288596 (37%)]\tLoss: 0.391184\n",
            "Train Epoch: 2 [108800/288596 (38%)]\tLoss: 0.373930\n",
            "Train Epoch: 2 [110080/288596 (38%)]\tLoss: 0.436127\n",
            "Train Epoch: 2 [111360/288596 (39%)]\tLoss: 0.344619\n",
            "Train Epoch: 2 [112640/288596 (39%)]\tLoss: 0.288392\n",
            "Train Epoch: 2 [113920/288596 (39%)]\tLoss: 0.338789\n",
            "Train Epoch: 2 [115200/288596 (40%)]\tLoss: 0.396627\n",
            "Train Epoch: 2 [116480/288596 (40%)]\tLoss: 0.405977\n",
            "Train Epoch: 2 [117760/288596 (41%)]\tLoss: 0.393206\n",
            "Train Epoch: 2 [119040/288596 (41%)]\tLoss: 0.408374\n",
            "Train Epoch: 2 [120320/288596 (42%)]\tLoss: 0.461285\n",
            "Train Epoch: 2 [121600/288596 (42%)]\tLoss: 0.351158\n",
            "Train Epoch: 2 [122880/288596 (43%)]\tLoss: 0.401210\n",
            "Train Epoch: 2 [124160/288596 (43%)]\tLoss: 0.333490\n",
            "Train Epoch: 2 [125440/288596 (43%)]\tLoss: 0.367696\n",
            "Train Epoch: 2 [126720/288596 (44%)]\tLoss: 0.419325\n",
            "Train Epoch: 2 [128000/288596 (44%)]\tLoss: 0.338391\n",
            "Train Epoch: 2 [129280/288596 (45%)]\tLoss: 0.288775\n",
            "Train Epoch: 2 [130560/288596 (45%)]\tLoss: 0.503903\n",
            "Train Epoch: 2 [131840/288596 (46%)]\tLoss: 0.310017\n",
            "Train Epoch: 2 [133120/288596 (46%)]\tLoss: 0.315995\n",
            "Train Epoch: 2 [134400/288596 (47%)]\tLoss: 0.362646\n",
            "Train Epoch: 2 [135680/288596 (47%)]\tLoss: 0.364417\n",
            "Train Epoch: 2 [136960/288596 (47%)]\tLoss: 0.327117\n",
            "Train Epoch: 2 [138240/288596 (48%)]\tLoss: 0.369349\n",
            "Train Epoch: 2 [139520/288596 (48%)]\tLoss: 0.391123\n",
            "Train Epoch: 2 [140800/288596 (49%)]\tLoss: 0.432920\n",
            "Train Epoch: 2 [142080/288596 (49%)]\tLoss: 0.332829\n",
            "Train Epoch: 2 [143360/288596 (50%)]\tLoss: 0.422210\n",
            "Train Epoch: 2 [144640/288596 (50%)]\tLoss: 0.362180\n",
            "Train Epoch: 2 [145920/288596 (51%)]\tLoss: 0.374029\n",
            "Train Epoch: 2 [147200/288596 (51%)]\tLoss: 0.346202\n",
            "Train Epoch: 2 [148480/288596 (51%)]\tLoss: 0.528364\n",
            "Train Epoch: 2 [149760/288596 (52%)]\tLoss: 0.326892\n",
            "Train Epoch: 2 [151040/288596 (52%)]\tLoss: 0.356026\n",
            "Train Epoch: 2 [152320/288596 (53%)]\tLoss: 0.311841\n",
            "Train Epoch: 2 [153600/288596 (53%)]\tLoss: 0.431779\n",
            "Train Epoch: 2 [154880/288596 (54%)]\tLoss: 0.403419\n",
            "Train Epoch: 2 [156160/288596 (54%)]\tLoss: 0.419449\n",
            "Train Epoch: 2 [157440/288596 (55%)]\tLoss: 0.384319\n",
            "Train Epoch: 2 [158720/288596 (55%)]\tLoss: 0.411602\n",
            "Train Epoch: 2 [160000/288596 (55%)]\tLoss: 0.283247\n",
            "Train Epoch: 2 [161280/288596 (56%)]\tLoss: 0.318510\n",
            "Train Epoch: 2 [162560/288596 (56%)]\tLoss: 0.370525\n",
            "Train Epoch: 2 [163840/288596 (57%)]\tLoss: 0.463853\n",
            "Train Epoch: 2 [165120/288596 (57%)]\tLoss: 0.345780\n",
            "Train Epoch: 2 [166400/288596 (58%)]\tLoss: 0.400969\n",
            "Train Epoch: 2 [167680/288596 (58%)]\tLoss: 0.375140\n",
            "Train Epoch: 2 [168960/288596 (59%)]\tLoss: 0.355003\n",
            "Train Epoch: 2 [170240/288596 (59%)]\tLoss: 0.341466\n",
            "Train Epoch: 2 [171520/288596 (59%)]\tLoss: 0.287020\n",
            "Train Epoch: 2 [172800/288596 (60%)]\tLoss: 0.372794\n",
            "Train Epoch: 2 [174080/288596 (60%)]\tLoss: 0.394350\n",
            "Train Epoch: 2 [175360/288596 (61%)]\tLoss: 0.418498\n",
            "Train Epoch: 2 [176640/288596 (61%)]\tLoss: 0.338417\n",
            "Train Epoch: 2 [177920/288596 (62%)]\tLoss: 0.359784\n",
            "Train Epoch: 2 [179200/288596 (62%)]\tLoss: 0.481151\n",
            "Train Epoch: 2 [180480/288596 (63%)]\tLoss: 0.347709\n",
            "Train Epoch: 2 [181760/288596 (63%)]\tLoss: 0.421026\n",
            "Train Epoch: 2 [183040/288596 (63%)]\tLoss: 0.262391\n",
            "Train Epoch: 2 [184320/288596 (64%)]\tLoss: 0.432950\n",
            "Train Epoch: 2 [185600/288596 (64%)]\tLoss: 0.436174\n",
            "Train Epoch: 2 [186880/288596 (65%)]\tLoss: 0.336039\n",
            "Train Epoch: 2 [188160/288596 (65%)]\tLoss: 0.375806\n",
            "Train Epoch: 2 [189440/288596 (66%)]\tLoss: 0.310342\n",
            "Train Epoch: 2 [190720/288596 (66%)]\tLoss: 0.363497\n",
            "Train Epoch: 2 [192000/288596 (67%)]\tLoss: 0.416155\n",
            "Train Epoch: 2 [193280/288596 (67%)]\tLoss: 0.401820\n",
            "Train Epoch: 2 [194560/288596 (67%)]\tLoss: 0.303802\n",
            "Train Epoch: 2 [195840/288596 (68%)]\tLoss: 0.389742\n",
            "Train Epoch: 2 [197120/288596 (68%)]\tLoss: 0.462317\n",
            "Train Epoch: 2 [198400/288596 (69%)]\tLoss: 0.388920\n",
            "Train Epoch: 2 [199680/288596 (69%)]\tLoss: 0.408283\n",
            "Train Epoch: 2 [200960/288596 (70%)]\tLoss: 0.321434\n",
            "Train Epoch: 2 [202240/288596 (70%)]\tLoss: 0.381555\n",
            "Train Epoch: 2 [203520/288596 (71%)]\tLoss: 0.318626\n",
            "Train Epoch: 2 [204800/288596 (71%)]\tLoss: 0.388157\n",
            "Train Epoch: 2 [206080/288596 (71%)]\tLoss: 0.298960\n",
            "Train Epoch: 2 [207360/288596 (72%)]\tLoss: 0.372322\n",
            "Train Epoch: 2 [208640/288596 (72%)]\tLoss: 0.483050\n",
            "Train Epoch: 2 [209920/288596 (73%)]\tLoss: 0.407745\n",
            "Train Epoch: 2 [211200/288596 (73%)]\tLoss: 0.378702\n",
            "Train Epoch: 2 [212480/288596 (74%)]\tLoss: 0.338846\n",
            "Train Epoch: 2 [213760/288596 (74%)]\tLoss: 0.345382\n",
            "Train Epoch: 2 [215040/288596 (75%)]\tLoss: 0.342778\n",
            "Train Epoch: 2 [216320/288596 (75%)]\tLoss: 0.362650\n",
            "Train Epoch: 2 [217600/288596 (75%)]\tLoss: 0.427081\n",
            "Train Epoch: 2 [218880/288596 (76%)]\tLoss: 0.337913\n",
            "Train Epoch: 2 [220160/288596 (76%)]\tLoss: 0.443832\n",
            "Train Epoch: 2 [221440/288596 (77%)]\tLoss: 0.352546\n",
            "Train Epoch: 2 [222720/288596 (77%)]\tLoss: 0.392912\n",
            "Train Epoch: 2 [224000/288596 (78%)]\tLoss: 0.355047\n",
            "Train Epoch: 2 [225280/288596 (78%)]\tLoss: 0.434386\n",
            "Train Epoch: 2 [226560/288596 (78%)]\tLoss: 0.347793\n",
            "Train Epoch: 2 [227840/288596 (79%)]\tLoss: 0.375233\n",
            "Train Epoch: 2 [229120/288596 (79%)]\tLoss: 0.351383\n",
            "Train Epoch: 2 [230400/288596 (80%)]\tLoss: 0.385245\n",
            "Train Epoch: 2 [231680/288596 (80%)]\tLoss: 0.377828\n",
            "Train Epoch: 2 [232960/288596 (81%)]\tLoss: 0.374851\n",
            "Train Epoch: 2 [234240/288596 (81%)]\tLoss: 0.409253\n",
            "Train Epoch: 2 [235520/288596 (82%)]\tLoss: 0.256694\n",
            "Train Epoch: 2 [236800/288596 (82%)]\tLoss: 0.293224\n",
            "Train Epoch: 2 [238080/288596 (82%)]\tLoss: 0.324875\n",
            "Train Epoch: 2 [239360/288596 (83%)]\tLoss: 0.460395\n",
            "Train Epoch: 2 [240640/288596 (83%)]\tLoss: 0.435313\n",
            "Train Epoch: 2 [241920/288596 (84%)]\tLoss: 0.449125\n",
            "Train Epoch: 2 [243200/288596 (84%)]\tLoss: 0.355104\n",
            "Train Epoch: 2 [244480/288596 (85%)]\tLoss: 0.441625\n",
            "Train Epoch: 2 [245760/288596 (85%)]\tLoss: 0.382901\n",
            "Train Epoch: 2 [247040/288596 (86%)]\tLoss: 0.373640\n",
            "Train Epoch: 2 [248320/288596 (86%)]\tLoss: 0.446576\n",
            "Train Epoch: 2 [249600/288596 (86%)]\tLoss: 0.343427\n",
            "Train Epoch: 2 [250880/288596 (87%)]\tLoss: 0.259971\n",
            "Train Epoch: 2 [252160/288596 (87%)]\tLoss: 0.446065\n",
            "Train Epoch: 2 [253440/288596 (88%)]\tLoss: 0.357397\n",
            "Train Epoch: 2 [254720/288596 (88%)]\tLoss: 0.329805\n",
            "Train Epoch: 2 [256000/288596 (89%)]\tLoss: 0.285566\n",
            "Train Epoch: 2 [257280/288596 (89%)]\tLoss: 0.409643\n",
            "Train Epoch: 2 [258560/288596 (90%)]\tLoss: 0.482499\n",
            "Train Epoch: 2 [259840/288596 (90%)]\tLoss: 0.452066\n",
            "Train Epoch: 2 [261120/288596 (90%)]\tLoss: 0.320459\n",
            "Train Epoch: 2 [262400/288596 (91%)]\tLoss: 0.285489\n",
            "Train Epoch: 2 [263680/288596 (91%)]\tLoss: 0.404214\n",
            "Train Epoch: 2 [264960/288596 (92%)]\tLoss: 0.269953\n",
            "Train Epoch: 2 [266240/288596 (92%)]\tLoss: 0.275199\n",
            "Train Epoch: 2 [267520/288596 (93%)]\tLoss: 0.398596\n",
            "Train Epoch: 2 [268800/288596 (93%)]\tLoss: 0.435787\n",
            "Train Epoch: 2 [270080/288596 (94%)]\tLoss: 0.498591\n",
            "Train Epoch: 2 [271360/288596 (94%)]\tLoss: 0.381774\n",
            "Train Epoch: 2 [272640/288596 (94%)]\tLoss: 0.288814\n",
            "Train Epoch: 2 [273920/288596 (95%)]\tLoss: 0.402876\n",
            "Train Epoch: 2 [275200/288596 (95%)]\tLoss: 0.306100\n",
            "Train Epoch: 2 [276480/288596 (96%)]\tLoss: 0.379746\n",
            "Train Epoch: 2 [277760/288596 (96%)]\tLoss: 0.347242\n",
            "Train Epoch: 2 [279040/288596 (97%)]\tLoss: 0.283068\n",
            "Train Epoch: 2 [280320/288596 (97%)]\tLoss: 0.296191\n",
            "Train Epoch: 2 [281600/288596 (98%)]\tLoss: 0.373888\n",
            "Train Epoch: 2 [282880/288596 (98%)]\tLoss: 0.316118\n",
            "Train Epoch: 2 [284160/288596 (98%)]\tLoss: 0.452180\n",
            "Train Epoch: 2 [285440/288596 (99%)]\tLoss: 0.467237\n",
            "Train Epoch: 2 [286720/288596 (99%)]\tLoss: 0.362926\n",
            "Train Epoch: 2 [288000/288596 (100%)]\tLoss: 0.315680\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 106629/123684 (86%)\n",
            "\n",
            "Train Epoch: 3 [0/288596 (0%)]\tLoss: 0.338037\n",
            "Train Epoch: 3 [1280/288596 (0%)]\tLoss: 0.484660\n",
            "Train Epoch: 3 [2560/288596 (1%)]\tLoss: 0.336145\n",
            "Train Epoch: 3 [3840/288596 (1%)]\tLoss: 0.351393\n",
            "Train Epoch: 3 [5120/288596 (2%)]\tLoss: 0.386210\n",
            "Train Epoch: 3 [6400/288596 (2%)]\tLoss: 0.388140\n",
            "Train Epoch: 3 [7680/288596 (3%)]\tLoss: 0.366635\n",
            "Train Epoch: 3 [8960/288596 (3%)]\tLoss: 0.422756\n",
            "Train Epoch: 3 [10240/288596 (4%)]\tLoss: 0.348026\n",
            "Train Epoch: 3 [11520/288596 (4%)]\tLoss: 0.331445\n",
            "Train Epoch: 3 [12800/288596 (4%)]\tLoss: 0.383242\n",
            "Train Epoch: 3 [14080/288596 (5%)]\tLoss: 0.269930\n",
            "Train Epoch: 3 [15360/288596 (5%)]\tLoss: 0.358778\n",
            "Train Epoch: 3 [16640/288596 (6%)]\tLoss: 0.346911\n",
            "Train Epoch: 3 [17920/288596 (6%)]\tLoss: 0.342019\n",
            "Train Epoch: 3 [19200/288596 (7%)]\tLoss: 0.385541\n",
            "Train Epoch: 3 [20480/288596 (7%)]\tLoss: 0.330723\n",
            "Train Epoch: 3 [21760/288596 (8%)]\tLoss: 0.351793\n",
            "Train Epoch: 3 [23040/288596 (8%)]\tLoss: 0.401283\n",
            "Train Epoch: 3 [24320/288596 (8%)]\tLoss: 0.288674\n",
            "Train Epoch: 3 [25600/288596 (9%)]\tLoss: 0.332338\n",
            "Train Epoch: 3 [26880/288596 (9%)]\tLoss: 0.383624\n",
            "Train Epoch: 3 [28160/288596 (10%)]\tLoss: 0.391342\n",
            "Train Epoch: 3 [29440/288596 (10%)]\tLoss: 0.285834\n",
            "Train Epoch: 3 [30720/288596 (11%)]\tLoss: 0.359385\n",
            "Train Epoch: 3 [32000/288596 (11%)]\tLoss: 0.429014\n",
            "Train Epoch: 3 [33280/288596 (12%)]\tLoss: 0.384605\n",
            "Train Epoch: 3 [34560/288596 (12%)]\tLoss: 0.326689\n",
            "Train Epoch: 3 [35840/288596 (12%)]\tLoss: 0.413669\n",
            "Train Epoch: 3 [37120/288596 (13%)]\tLoss: 0.334067\n",
            "Train Epoch: 3 [38400/288596 (13%)]\tLoss: 0.345262\n",
            "Train Epoch: 3 [39680/288596 (14%)]\tLoss: 0.273258\n",
            "Train Epoch: 3 [40960/288596 (14%)]\tLoss: 0.355281\n",
            "Train Epoch: 3 [42240/288596 (15%)]\tLoss: 0.441983\n",
            "Train Epoch: 3 [43520/288596 (15%)]\tLoss: 0.268266\n",
            "Train Epoch: 3 [44800/288596 (16%)]\tLoss: 0.328211\n",
            "Train Epoch: 3 [46080/288596 (16%)]\tLoss: 0.308630\n",
            "Train Epoch: 3 [47360/288596 (16%)]\tLoss: 0.393613\n",
            "Train Epoch: 3 [48640/288596 (17%)]\tLoss: 0.432395\n",
            "Train Epoch: 3 [49920/288596 (17%)]\tLoss: 0.417755\n",
            "Train Epoch: 3 [51200/288596 (18%)]\tLoss: 0.392445\n",
            "Train Epoch: 3 [52480/288596 (18%)]\tLoss: 0.312721\n",
            "Train Epoch: 3 [53760/288596 (19%)]\tLoss: 0.317124\n",
            "Train Epoch: 3 [55040/288596 (19%)]\tLoss: 0.376638\n",
            "Train Epoch: 3 [56320/288596 (20%)]\tLoss: 0.376732\n",
            "Train Epoch: 3 [57600/288596 (20%)]\tLoss: 0.285191\n",
            "Train Epoch: 3 [58880/288596 (20%)]\tLoss: 0.281463\n",
            "Train Epoch: 3 [60160/288596 (21%)]\tLoss: 0.291678\n",
            "Train Epoch: 3 [61440/288596 (21%)]\tLoss: 0.437674\n",
            "Train Epoch: 3 [62720/288596 (22%)]\tLoss: 0.405028\n",
            "Train Epoch: 3 [64000/288596 (22%)]\tLoss: 0.381261\n",
            "Train Epoch: 3 [65280/288596 (23%)]\tLoss: 0.335177\n",
            "Train Epoch: 3 [66560/288596 (23%)]\tLoss: 0.459579\n",
            "Train Epoch: 3 [67840/288596 (24%)]\tLoss: 0.328245\n",
            "Train Epoch: 3 [69120/288596 (24%)]\tLoss: 0.437794\n",
            "Train Epoch: 3 [70400/288596 (24%)]\tLoss: 0.543822\n",
            "Train Epoch: 3 [71680/288596 (25%)]\tLoss: 0.433954\n",
            "Train Epoch: 3 [72960/288596 (25%)]\tLoss: 0.301826\n",
            "Train Epoch: 3 [74240/288596 (26%)]\tLoss: 0.362149\n",
            "Train Epoch: 3 [75520/288596 (26%)]\tLoss: 0.377326\n",
            "Train Epoch: 3 [76800/288596 (27%)]\tLoss: 0.373256\n",
            "Train Epoch: 3 [78080/288596 (27%)]\tLoss: 0.371344\n",
            "Train Epoch: 3 [79360/288596 (27%)]\tLoss: 0.267170\n",
            "Train Epoch: 3 [80640/288596 (28%)]\tLoss: 0.331264\n",
            "Train Epoch: 3 [81920/288596 (28%)]\tLoss: 0.366609\n",
            "Train Epoch: 3 [83200/288596 (29%)]\tLoss: 0.386132\n",
            "Train Epoch: 3 [84480/288596 (29%)]\tLoss: 0.317735\n",
            "Train Epoch: 3 [85760/288596 (30%)]\tLoss: 0.401059\n",
            "Train Epoch: 3 [87040/288596 (30%)]\tLoss: 0.346044\n",
            "Train Epoch: 3 [88320/288596 (31%)]\tLoss: 0.247601\n",
            "Train Epoch: 3 [89600/288596 (31%)]\tLoss: 0.486788\n",
            "Train Epoch: 3 [90880/288596 (31%)]\tLoss: 0.314330\n",
            "Train Epoch: 3 [92160/288596 (32%)]\tLoss: 0.329985\n",
            "Train Epoch: 3 [93440/288596 (32%)]\tLoss: 0.360053\n",
            "Train Epoch: 3 [94720/288596 (33%)]\tLoss: 0.473204\n",
            "Train Epoch: 3 [96000/288596 (33%)]\tLoss: 0.237500\n",
            "Train Epoch: 3 [97280/288596 (34%)]\tLoss: 0.321170\n",
            "Train Epoch: 3 [98560/288596 (34%)]\tLoss: 0.303843\n",
            "Train Epoch: 3 [99840/288596 (35%)]\tLoss: 0.323083\n",
            "Train Epoch: 3 [101120/288596 (35%)]\tLoss: 0.340613\n",
            "Train Epoch: 3 [102400/288596 (35%)]\tLoss: 0.550467\n",
            "Train Epoch: 3 [103680/288596 (36%)]\tLoss: 0.410931\n",
            "Train Epoch: 3 [104960/288596 (36%)]\tLoss: 0.497418\n",
            "Train Epoch: 3 [106240/288596 (37%)]\tLoss: 0.327341\n",
            "Train Epoch: 3 [107520/288596 (37%)]\tLoss: 0.363537\n",
            "Train Epoch: 3 [108800/288596 (38%)]\tLoss: 0.293675\n",
            "Train Epoch: 3 [110080/288596 (38%)]\tLoss: 0.330584\n",
            "Train Epoch: 3 [111360/288596 (39%)]\tLoss: 0.341964\n",
            "Train Epoch: 3 [112640/288596 (39%)]\tLoss: 0.411963\n",
            "Train Epoch: 3 [113920/288596 (39%)]\tLoss: 0.218885\n",
            "Train Epoch: 3 [115200/288596 (40%)]\tLoss: 0.347728\n",
            "Train Epoch: 3 [116480/288596 (40%)]\tLoss: 0.326112\n",
            "Train Epoch: 3 [117760/288596 (41%)]\tLoss: 0.379908\n",
            "Train Epoch: 3 [119040/288596 (41%)]\tLoss: 0.381999\n",
            "Train Epoch: 3 [120320/288596 (42%)]\tLoss: 0.271406\n",
            "Train Epoch: 3 [121600/288596 (42%)]\tLoss: 0.416220\n",
            "Train Epoch: 3 [122880/288596 (43%)]\tLoss: 0.344128\n",
            "Train Epoch: 3 [124160/288596 (43%)]\tLoss: 0.346660\n",
            "Train Epoch: 3 [125440/288596 (43%)]\tLoss: 0.440738\n",
            "Train Epoch: 3 [126720/288596 (44%)]\tLoss: 0.477250\n",
            "Train Epoch: 3 [128000/288596 (44%)]\tLoss: 0.410671\n",
            "Train Epoch: 3 [129280/288596 (45%)]\tLoss: 0.336742\n",
            "Train Epoch: 3 [130560/288596 (45%)]\tLoss: 0.416335\n",
            "Train Epoch: 3 [131840/288596 (46%)]\tLoss: 0.359050\n",
            "Train Epoch: 3 [133120/288596 (46%)]\tLoss: 0.403798\n",
            "Train Epoch: 3 [134400/288596 (47%)]\tLoss: 0.327994\n",
            "Train Epoch: 3 [135680/288596 (47%)]\tLoss: 0.458096\n",
            "Train Epoch: 3 [136960/288596 (47%)]\tLoss: 0.382372\n",
            "Train Epoch: 3 [138240/288596 (48%)]\tLoss: 0.334333\n",
            "Train Epoch: 3 [139520/288596 (48%)]\tLoss: 0.294366\n",
            "Train Epoch: 3 [140800/288596 (49%)]\tLoss: 0.397006\n",
            "Train Epoch: 3 [142080/288596 (49%)]\tLoss: 0.336214\n",
            "Train Epoch: 3 [143360/288596 (50%)]\tLoss: 0.393407\n",
            "Train Epoch: 3 [144640/288596 (50%)]\tLoss: 0.399002\n",
            "Train Epoch: 3 [145920/288596 (51%)]\tLoss: 0.378974\n",
            "Train Epoch: 3 [147200/288596 (51%)]\tLoss: 0.418167\n",
            "Train Epoch: 3 [148480/288596 (51%)]\tLoss: 0.344955\n",
            "Train Epoch: 3 [149760/288596 (52%)]\tLoss: 0.391279\n",
            "Train Epoch: 3 [151040/288596 (52%)]\tLoss: 0.347241\n",
            "Train Epoch: 3 [152320/288596 (53%)]\tLoss: 0.446199\n",
            "Train Epoch: 3 [153600/288596 (53%)]\tLoss: 0.333752\n",
            "Train Epoch: 3 [154880/288596 (54%)]\tLoss: 0.321239\n",
            "Train Epoch: 3 [156160/288596 (54%)]\tLoss: 0.310387\n",
            "Train Epoch: 3 [157440/288596 (55%)]\tLoss: 0.333227\n",
            "Train Epoch: 3 [158720/288596 (55%)]\tLoss: 0.325320\n",
            "Train Epoch: 3 [160000/288596 (55%)]\tLoss: 0.256331\n",
            "Train Epoch: 3 [161280/288596 (56%)]\tLoss: 0.399395\n",
            "Train Epoch: 3 [162560/288596 (56%)]\tLoss: 0.390019\n",
            "Train Epoch: 3 [163840/288596 (57%)]\tLoss: 0.354545\n",
            "Train Epoch: 3 [165120/288596 (57%)]\tLoss: 0.308050\n",
            "Train Epoch: 3 [166400/288596 (58%)]\tLoss: 0.314944\n",
            "Train Epoch: 3 [167680/288596 (58%)]\tLoss: 0.398766\n",
            "Train Epoch: 3 [168960/288596 (59%)]\tLoss: 0.483753\n",
            "Train Epoch: 3 [170240/288596 (59%)]\tLoss: 0.313532\n",
            "Train Epoch: 3 [171520/288596 (59%)]\tLoss: 0.411662\n",
            "Train Epoch: 3 [172800/288596 (60%)]\tLoss: 0.342386\n",
            "Train Epoch: 3 [174080/288596 (60%)]\tLoss: 0.374521\n",
            "Train Epoch: 3 [175360/288596 (61%)]\tLoss: 0.298113\n",
            "Train Epoch: 3 [176640/288596 (61%)]\tLoss: 0.447380\n",
            "Train Epoch: 3 [177920/288596 (62%)]\tLoss: 0.288057\n",
            "Train Epoch: 3 [179200/288596 (62%)]\tLoss: 0.203158\n",
            "Train Epoch: 3 [180480/288596 (63%)]\tLoss: 0.274659\n",
            "Train Epoch: 3 [181760/288596 (63%)]\tLoss: 0.288125\n",
            "Train Epoch: 3 [183040/288596 (63%)]\tLoss: 0.492661\n",
            "Train Epoch: 3 [184320/288596 (64%)]\tLoss: 0.360194\n",
            "Train Epoch: 3 [185600/288596 (64%)]\tLoss: 0.343894\n",
            "Train Epoch: 3 [186880/288596 (65%)]\tLoss: 0.459764\n",
            "Train Epoch: 3 [188160/288596 (65%)]\tLoss: 0.299370\n",
            "Train Epoch: 3 [189440/288596 (66%)]\tLoss: 0.410123\n",
            "Train Epoch: 3 [190720/288596 (66%)]\tLoss: 0.339086\n",
            "Train Epoch: 3 [192000/288596 (67%)]\tLoss: 0.471461\n",
            "Train Epoch: 3 [193280/288596 (67%)]\tLoss: 0.405433\n",
            "Train Epoch: 3 [194560/288596 (67%)]\tLoss: 0.345741\n",
            "Train Epoch: 3 [195840/288596 (68%)]\tLoss: 0.395368\n",
            "Train Epoch: 3 [197120/288596 (68%)]\tLoss: 0.393778\n",
            "Train Epoch: 3 [198400/288596 (69%)]\tLoss: 0.337312\n",
            "Train Epoch: 3 [199680/288596 (69%)]\tLoss: 0.296846\n",
            "Train Epoch: 3 [200960/288596 (70%)]\tLoss: 0.422662\n",
            "Train Epoch: 3 [202240/288596 (70%)]\tLoss: 0.314527\n",
            "Train Epoch: 3 [203520/288596 (71%)]\tLoss: 0.399766\n",
            "Train Epoch: 3 [204800/288596 (71%)]\tLoss: 0.352299\n",
            "Train Epoch: 3 [206080/288596 (71%)]\tLoss: 0.479751\n",
            "Train Epoch: 3 [207360/288596 (72%)]\tLoss: 0.315306\n",
            "Train Epoch: 3 [208640/288596 (72%)]\tLoss: 0.388588\n",
            "Train Epoch: 3 [209920/288596 (73%)]\tLoss: 0.477931\n",
            "Train Epoch: 3 [211200/288596 (73%)]\tLoss: 0.416174\n",
            "Train Epoch: 3 [212480/288596 (74%)]\tLoss: 0.396796\n",
            "Train Epoch: 3 [213760/288596 (74%)]\tLoss: 0.416696\n",
            "Train Epoch: 3 [215040/288596 (75%)]\tLoss: 0.334996\n",
            "Train Epoch: 3 [216320/288596 (75%)]\tLoss: 0.433389\n",
            "Train Epoch: 3 [217600/288596 (75%)]\tLoss: 0.356239\n",
            "Train Epoch: 3 [218880/288596 (76%)]\tLoss: 0.290915\n",
            "Train Epoch: 3 [220160/288596 (76%)]\tLoss: 0.353292\n",
            "Train Epoch: 3 [221440/288596 (77%)]\tLoss: 0.227030\n",
            "Train Epoch: 3 [222720/288596 (77%)]\tLoss: 0.380784\n",
            "Train Epoch: 3 [224000/288596 (78%)]\tLoss: 0.392282\n",
            "Train Epoch: 3 [225280/288596 (78%)]\tLoss: 0.283677\n",
            "Train Epoch: 3 [226560/288596 (78%)]\tLoss: 0.367444\n",
            "Train Epoch: 3 [227840/288596 (79%)]\tLoss: 0.469746\n",
            "Train Epoch: 3 [229120/288596 (79%)]\tLoss: 0.324368\n",
            "Train Epoch: 3 [230400/288596 (80%)]\tLoss: 0.272686\n",
            "Train Epoch: 3 [231680/288596 (80%)]\tLoss: 0.392434\n",
            "Train Epoch: 3 [232960/288596 (81%)]\tLoss: 0.446870\n",
            "Train Epoch: 3 [234240/288596 (81%)]\tLoss: 0.307421\n",
            "Train Epoch: 3 [235520/288596 (82%)]\tLoss: 0.432005\n",
            "Train Epoch: 3 [236800/288596 (82%)]\tLoss: 0.369781\n",
            "Train Epoch: 3 [238080/288596 (82%)]\tLoss: 0.328294\n",
            "Train Epoch: 3 [239360/288596 (83%)]\tLoss: 0.384818\n",
            "Train Epoch: 3 [240640/288596 (83%)]\tLoss: 0.309680\n",
            "Train Epoch: 3 [241920/288596 (84%)]\tLoss: 0.351817\n",
            "Train Epoch: 3 [243200/288596 (84%)]\tLoss: 0.354073\n",
            "Train Epoch: 3 [244480/288596 (85%)]\tLoss: 0.390811\n",
            "Train Epoch: 3 [245760/288596 (85%)]\tLoss: 0.365260\n",
            "Train Epoch: 3 [247040/288596 (86%)]\tLoss: 0.411404\n",
            "Train Epoch: 3 [248320/288596 (86%)]\tLoss: 0.339703\n",
            "Train Epoch: 3 [249600/288596 (86%)]\tLoss: 0.312321\n",
            "Train Epoch: 3 [250880/288596 (87%)]\tLoss: 0.315970\n",
            "Train Epoch: 3 [252160/288596 (87%)]\tLoss: 0.341079\n",
            "Train Epoch: 3 [253440/288596 (88%)]\tLoss: 0.475428\n",
            "Train Epoch: 3 [254720/288596 (88%)]\tLoss: 0.317298\n",
            "Train Epoch: 3 [256000/288596 (89%)]\tLoss: 0.299047\n",
            "Train Epoch: 3 [257280/288596 (89%)]\tLoss: 0.345044\n",
            "Train Epoch: 3 [258560/288596 (90%)]\tLoss: 0.373091\n",
            "Train Epoch: 3 [259840/288596 (90%)]\tLoss: 0.346575\n",
            "Train Epoch: 3 [261120/288596 (90%)]\tLoss: 0.444368\n",
            "Train Epoch: 3 [262400/288596 (91%)]\tLoss: 0.385069\n",
            "Train Epoch: 3 [263680/288596 (91%)]\tLoss: 0.412307\n",
            "Train Epoch: 3 [264960/288596 (92%)]\tLoss: 0.286064\n",
            "Train Epoch: 3 [266240/288596 (92%)]\tLoss: 0.329742\n",
            "Train Epoch: 3 [267520/288596 (93%)]\tLoss: 0.383608\n",
            "Train Epoch: 3 [268800/288596 (93%)]\tLoss: 0.389284\n",
            "Train Epoch: 3 [270080/288596 (94%)]\tLoss: 0.309476\n",
            "Train Epoch: 3 [271360/288596 (94%)]\tLoss: 0.417349\n",
            "Train Epoch: 3 [272640/288596 (94%)]\tLoss: 0.431901\n",
            "Train Epoch: 3 [273920/288596 (95%)]\tLoss: 0.319158\n",
            "Train Epoch: 3 [275200/288596 (95%)]\tLoss: 0.404782\n",
            "Train Epoch: 3 [276480/288596 (96%)]\tLoss: 0.290441\n",
            "Train Epoch: 3 [277760/288596 (96%)]\tLoss: 0.301914\n",
            "Train Epoch: 3 [279040/288596 (97%)]\tLoss: 0.301339\n",
            "Train Epoch: 3 [280320/288596 (97%)]\tLoss: 0.345255\n",
            "Train Epoch: 3 [281600/288596 (98%)]\tLoss: 0.300541\n",
            "Train Epoch: 3 [282880/288596 (98%)]\tLoss: 0.290651\n",
            "Train Epoch: 3 [284160/288596 (98%)]\tLoss: 0.318725\n",
            "Train Epoch: 3 [285440/288596 (99%)]\tLoss: 0.394255\n",
            "Train Epoch: 3 [286720/288596 (99%)]\tLoss: 0.470761\n",
            "Train Epoch: 3 [288000/288596 (100%)]\tLoss: 0.299389\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 107047/123684 (87%)\n",
            "\n",
            "Train Epoch: 4 [0/288596 (0%)]\tLoss: 0.344215\n",
            "Train Epoch: 4 [1280/288596 (0%)]\tLoss: 0.375010\n",
            "Train Epoch: 4 [2560/288596 (1%)]\tLoss: 0.461758\n",
            "Train Epoch: 4 [3840/288596 (1%)]\tLoss: 0.364603\n",
            "Train Epoch: 4 [5120/288596 (2%)]\tLoss: 0.404052\n",
            "Train Epoch: 4 [6400/288596 (2%)]\tLoss: 0.320996\n",
            "Train Epoch: 4 [7680/288596 (3%)]\tLoss: 0.388916\n",
            "Train Epoch: 4 [8960/288596 (3%)]\tLoss: 0.380862\n",
            "Train Epoch: 4 [10240/288596 (4%)]\tLoss: 0.345305\n",
            "Train Epoch: 4 [11520/288596 (4%)]\tLoss: 0.381076\n",
            "Train Epoch: 4 [12800/288596 (4%)]\tLoss: 0.296285\n",
            "Train Epoch: 4 [14080/288596 (5%)]\tLoss: 0.362854\n",
            "Train Epoch: 4 [15360/288596 (5%)]\tLoss: 0.291054\n",
            "Train Epoch: 4 [16640/288596 (6%)]\tLoss: 0.359783\n",
            "Train Epoch: 4 [17920/288596 (6%)]\tLoss: 0.463890\n",
            "Train Epoch: 4 [19200/288596 (7%)]\tLoss: 0.322518\n",
            "Train Epoch: 4 [20480/288596 (7%)]\tLoss: 0.287176\n",
            "Train Epoch: 4 [21760/288596 (8%)]\tLoss: 0.309612\n",
            "Train Epoch: 4 [23040/288596 (8%)]\tLoss: 0.241881\n",
            "Train Epoch: 4 [24320/288596 (8%)]\tLoss: 0.356385\n",
            "Train Epoch: 4 [25600/288596 (9%)]\tLoss: 0.340483\n",
            "Train Epoch: 4 [26880/288596 (9%)]\tLoss: 0.360857\n",
            "Train Epoch: 4 [28160/288596 (10%)]\tLoss: 0.395757\n",
            "Train Epoch: 4 [29440/288596 (10%)]\tLoss: 0.352632\n",
            "Train Epoch: 4 [30720/288596 (11%)]\tLoss: 0.480013\n",
            "Train Epoch: 4 [32000/288596 (11%)]\tLoss: 0.384872\n",
            "Train Epoch: 4 [33280/288596 (12%)]\tLoss: 0.312234\n",
            "Train Epoch: 4 [34560/288596 (12%)]\tLoss: 0.434689\n",
            "Train Epoch: 4 [35840/288596 (12%)]\tLoss: 0.300775\n",
            "Train Epoch: 4 [37120/288596 (13%)]\tLoss: 0.414368\n",
            "Train Epoch: 4 [38400/288596 (13%)]\tLoss: 0.248742\n",
            "Train Epoch: 4 [39680/288596 (14%)]\tLoss: 0.248137\n",
            "Train Epoch: 4 [40960/288596 (14%)]\tLoss: 0.429823\n",
            "Train Epoch: 4 [42240/288596 (15%)]\tLoss: 0.324312\n",
            "Train Epoch: 4 [43520/288596 (15%)]\tLoss: 0.377001\n",
            "Train Epoch: 4 [44800/288596 (16%)]\tLoss: 0.326236\n",
            "Train Epoch: 4 [46080/288596 (16%)]\tLoss: 0.446324\n",
            "Train Epoch: 4 [47360/288596 (16%)]\tLoss: 0.443491\n",
            "Train Epoch: 4 [48640/288596 (17%)]\tLoss: 0.273727\n",
            "Train Epoch: 4 [49920/288596 (17%)]\tLoss: 0.282870\n",
            "Train Epoch: 4 [51200/288596 (18%)]\tLoss: 0.318461\n",
            "Train Epoch: 4 [52480/288596 (18%)]\tLoss: 0.347913\n",
            "Train Epoch: 4 [53760/288596 (19%)]\tLoss: 0.387736\n",
            "Train Epoch: 4 [55040/288596 (19%)]\tLoss: 0.318752\n",
            "Train Epoch: 4 [56320/288596 (20%)]\tLoss: 0.343294\n",
            "Train Epoch: 4 [57600/288596 (20%)]\tLoss: 0.351348\n",
            "Train Epoch: 4 [58880/288596 (20%)]\tLoss: 0.464300\n",
            "Train Epoch: 4 [60160/288596 (21%)]\tLoss: 0.265435\n",
            "Train Epoch: 4 [61440/288596 (21%)]\tLoss: 0.424180\n",
            "Train Epoch: 4 [62720/288596 (22%)]\tLoss: 0.358105\n",
            "Train Epoch: 4 [64000/288596 (22%)]\tLoss: 0.298865\n",
            "Train Epoch: 4 [65280/288596 (23%)]\tLoss: 0.273445\n",
            "Train Epoch: 4 [66560/288596 (23%)]\tLoss: 0.300642\n",
            "Train Epoch: 4 [67840/288596 (24%)]\tLoss: 0.501155\n",
            "Train Epoch: 4 [69120/288596 (24%)]\tLoss: 0.377848\n",
            "Train Epoch: 4 [70400/288596 (24%)]\tLoss: 0.317190\n",
            "Train Epoch: 4 [71680/288596 (25%)]\tLoss: 0.326952\n",
            "Train Epoch: 4 [72960/288596 (25%)]\tLoss: 0.415963\n",
            "Train Epoch: 4 [74240/288596 (26%)]\tLoss: 0.384983\n",
            "Train Epoch: 4 [75520/288596 (26%)]\tLoss: 0.338443\n",
            "Train Epoch: 4 [76800/288596 (27%)]\tLoss: 0.321709\n",
            "Train Epoch: 4 [78080/288596 (27%)]\tLoss: 0.434938\n",
            "Train Epoch: 4 [79360/288596 (27%)]\tLoss: 0.351013\n",
            "Train Epoch: 4 [80640/288596 (28%)]\tLoss: 0.383924\n",
            "Train Epoch: 4 [81920/288596 (28%)]\tLoss: 0.332348\n",
            "Train Epoch: 4 [83200/288596 (29%)]\tLoss: 0.344720\n",
            "Train Epoch: 4 [84480/288596 (29%)]\tLoss: 0.417614\n",
            "Train Epoch: 4 [85760/288596 (30%)]\tLoss: 0.380953\n",
            "Train Epoch: 4 [87040/288596 (30%)]\tLoss: 0.380823\n",
            "Train Epoch: 4 [88320/288596 (31%)]\tLoss: 0.398588\n",
            "Train Epoch: 4 [89600/288596 (31%)]\tLoss: 0.369871\n",
            "Train Epoch: 4 [90880/288596 (31%)]\tLoss: 0.391313\n",
            "Train Epoch: 4 [92160/288596 (32%)]\tLoss: 0.294108\n",
            "Train Epoch: 4 [93440/288596 (32%)]\tLoss: 0.408524\n",
            "Train Epoch: 4 [94720/288596 (33%)]\tLoss: 0.246841\n",
            "Train Epoch: 4 [96000/288596 (33%)]\tLoss: 0.335299\n",
            "Train Epoch: 4 [97280/288596 (34%)]\tLoss: 0.353953\n",
            "Train Epoch: 4 [98560/288596 (34%)]\tLoss: 0.427395\n",
            "Train Epoch: 4 [99840/288596 (35%)]\tLoss: 0.321761\n",
            "Train Epoch: 4 [101120/288596 (35%)]\tLoss: 0.328555\n",
            "Train Epoch: 4 [102400/288596 (35%)]\tLoss: 0.427386\n",
            "Train Epoch: 4 [103680/288596 (36%)]\tLoss: 0.477192\n",
            "Train Epoch: 4 [104960/288596 (36%)]\tLoss: 0.368914\n",
            "Train Epoch: 4 [106240/288596 (37%)]\tLoss: 0.334140\n",
            "Train Epoch: 4 [107520/288596 (37%)]\tLoss: 0.352482\n",
            "Train Epoch: 4 [108800/288596 (38%)]\tLoss: 0.365817\n",
            "Train Epoch: 4 [110080/288596 (38%)]\tLoss: 0.387630\n",
            "Train Epoch: 4 [111360/288596 (39%)]\tLoss: 0.387641\n",
            "Train Epoch: 4 [112640/288596 (39%)]\tLoss: 0.384648\n",
            "Train Epoch: 4 [113920/288596 (39%)]\tLoss: 0.360441\n",
            "Train Epoch: 4 [115200/288596 (40%)]\tLoss: 0.287699\n",
            "Train Epoch: 4 [116480/288596 (40%)]\tLoss: 0.380951\n",
            "Train Epoch: 4 [117760/288596 (41%)]\tLoss: 0.474181\n",
            "Train Epoch: 4 [119040/288596 (41%)]\tLoss: 0.294695\n",
            "Train Epoch: 4 [120320/288596 (42%)]\tLoss: 0.311688\n",
            "Train Epoch: 4 [121600/288596 (42%)]\tLoss: 0.367543\n",
            "Train Epoch: 4 [122880/288596 (43%)]\tLoss: 0.359409\n",
            "Train Epoch: 4 [124160/288596 (43%)]\tLoss: 0.395940\n",
            "Train Epoch: 4 [125440/288596 (43%)]\tLoss: 0.313278\n",
            "Train Epoch: 4 [126720/288596 (44%)]\tLoss: 0.356520\n",
            "Train Epoch: 4 [128000/288596 (44%)]\tLoss: 0.351207\n",
            "Train Epoch: 4 [129280/288596 (45%)]\tLoss: 0.317164\n",
            "Train Epoch: 4 [130560/288596 (45%)]\tLoss: 0.376465\n",
            "Train Epoch: 4 [131840/288596 (46%)]\tLoss: 0.341818\n",
            "Train Epoch: 4 [133120/288596 (46%)]\tLoss: 0.263246\n",
            "Train Epoch: 4 [134400/288596 (47%)]\tLoss: 0.544847\n",
            "Train Epoch: 4 [135680/288596 (47%)]\tLoss: 0.386427\n",
            "Train Epoch: 4 [136960/288596 (47%)]\tLoss: 0.389997\n",
            "Train Epoch: 4 [138240/288596 (48%)]\tLoss: 0.358671\n",
            "Train Epoch: 4 [139520/288596 (48%)]\tLoss: 0.399708\n",
            "Train Epoch: 4 [140800/288596 (49%)]\tLoss: 0.350055\n",
            "Train Epoch: 4 [142080/288596 (49%)]\tLoss: 0.376650\n",
            "Train Epoch: 4 [143360/288596 (50%)]\tLoss: 0.373388\n",
            "Train Epoch: 4 [144640/288596 (50%)]\tLoss: 0.398097\n",
            "Train Epoch: 4 [145920/288596 (51%)]\tLoss: 0.307637\n",
            "Train Epoch: 4 [147200/288596 (51%)]\tLoss: 0.282612\n",
            "Train Epoch: 4 [148480/288596 (51%)]\tLoss: 0.326513\n",
            "Train Epoch: 4 [149760/288596 (52%)]\tLoss: 0.398041\n",
            "Train Epoch: 4 [151040/288596 (52%)]\tLoss: 0.345459\n",
            "Train Epoch: 4 [152320/288596 (53%)]\tLoss: 0.397161\n",
            "Train Epoch: 4 [153600/288596 (53%)]\tLoss: 0.346741\n",
            "Train Epoch: 4 [154880/288596 (54%)]\tLoss: 0.287370\n",
            "Train Epoch: 4 [156160/288596 (54%)]\tLoss: 0.360597\n",
            "Train Epoch: 4 [157440/288596 (55%)]\tLoss: 0.329470\n",
            "Train Epoch: 4 [158720/288596 (55%)]\tLoss: 0.271717\n",
            "Train Epoch: 4 [160000/288596 (55%)]\tLoss: 0.312570\n",
            "Train Epoch: 4 [161280/288596 (56%)]\tLoss: 0.363620\n",
            "Train Epoch: 4 [162560/288596 (56%)]\tLoss: 0.295935\n",
            "Train Epoch: 4 [163840/288596 (57%)]\tLoss: 0.443895\n",
            "Train Epoch: 4 [165120/288596 (57%)]\tLoss: 0.367786\n",
            "Train Epoch: 4 [166400/288596 (58%)]\tLoss: 0.310521\n",
            "Train Epoch: 4 [167680/288596 (58%)]\tLoss: 0.356395\n",
            "Train Epoch: 4 [168960/288596 (59%)]\tLoss: 0.445758\n",
            "Train Epoch: 4 [170240/288596 (59%)]\tLoss: 0.319449\n",
            "Train Epoch: 4 [171520/288596 (59%)]\tLoss: 0.410482\n",
            "Train Epoch: 4 [172800/288596 (60%)]\tLoss: 0.448747\n",
            "Train Epoch: 4 [174080/288596 (60%)]\tLoss: 0.351438\n",
            "Train Epoch: 4 [175360/288596 (61%)]\tLoss: 0.318224\n",
            "Train Epoch: 4 [176640/288596 (61%)]\tLoss: 0.327696\n",
            "Train Epoch: 4 [177920/288596 (62%)]\tLoss: 0.455954\n",
            "Train Epoch: 4 [179200/288596 (62%)]\tLoss: 0.363233\n",
            "Train Epoch: 4 [180480/288596 (63%)]\tLoss: 0.409623\n",
            "Train Epoch: 4 [181760/288596 (63%)]\tLoss: 0.404551\n",
            "Train Epoch: 4 [183040/288596 (63%)]\tLoss: 0.304366\n",
            "Train Epoch: 4 [184320/288596 (64%)]\tLoss: 0.383190\n",
            "Train Epoch: 4 [185600/288596 (64%)]\tLoss: 0.373658\n",
            "Train Epoch: 4 [186880/288596 (65%)]\tLoss: 0.420582\n",
            "Train Epoch: 4 [188160/288596 (65%)]\tLoss: 0.329947\n",
            "Train Epoch: 4 [189440/288596 (66%)]\tLoss: 0.334789\n",
            "Train Epoch: 4 [190720/288596 (66%)]\tLoss: 0.415490\n",
            "Train Epoch: 4 [192000/288596 (67%)]\tLoss: 0.365428\n",
            "Train Epoch: 4 [193280/288596 (67%)]\tLoss: 0.408248\n",
            "Train Epoch: 4 [194560/288596 (67%)]\tLoss: 0.336103\n",
            "Train Epoch: 4 [195840/288596 (68%)]\tLoss: 0.325987\n",
            "Train Epoch: 4 [197120/288596 (68%)]\tLoss: 0.355464\n",
            "Train Epoch: 4 [198400/288596 (69%)]\tLoss: 0.420503\n",
            "Train Epoch: 4 [199680/288596 (69%)]\tLoss: 0.382418\n",
            "Train Epoch: 4 [200960/288596 (70%)]\tLoss: 0.341460\n",
            "Train Epoch: 4 [202240/288596 (70%)]\tLoss: 0.420160\n",
            "Train Epoch: 4 [203520/288596 (71%)]\tLoss: 0.377623\n",
            "Train Epoch: 4 [204800/288596 (71%)]\tLoss: 0.318128\n",
            "Train Epoch: 4 [206080/288596 (71%)]\tLoss: 0.290853\n",
            "Train Epoch: 4 [207360/288596 (72%)]\tLoss: 0.313218\n",
            "Train Epoch: 4 [208640/288596 (72%)]\tLoss: 0.430602\n",
            "Train Epoch: 4 [209920/288596 (73%)]\tLoss: 0.405941\n",
            "Train Epoch: 4 [211200/288596 (73%)]\tLoss: 0.305465\n",
            "Train Epoch: 4 [212480/288596 (74%)]\tLoss: 0.324796\n",
            "Train Epoch: 4 [213760/288596 (74%)]\tLoss: 0.395509\n",
            "Train Epoch: 4 [215040/288596 (75%)]\tLoss: 0.436806\n",
            "Train Epoch: 4 [216320/288596 (75%)]\tLoss: 0.397942\n",
            "Train Epoch: 4 [217600/288596 (75%)]\tLoss: 0.386406\n",
            "Train Epoch: 4 [218880/288596 (76%)]\tLoss: 0.374553\n",
            "Train Epoch: 4 [220160/288596 (76%)]\tLoss: 0.362053\n",
            "Train Epoch: 4 [221440/288596 (77%)]\tLoss: 0.343916\n",
            "Train Epoch: 4 [222720/288596 (77%)]\tLoss: 0.344092\n",
            "Train Epoch: 4 [224000/288596 (78%)]\tLoss: 0.356478\n",
            "Train Epoch: 4 [225280/288596 (78%)]\tLoss: 0.384371\n",
            "Train Epoch: 4 [226560/288596 (78%)]\tLoss: 0.341760\n",
            "Train Epoch: 4 [227840/288596 (79%)]\tLoss: 0.288880\n",
            "Train Epoch: 4 [229120/288596 (79%)]\tLoss: 0.429679\n",
            "Train Epoch: 4 [230400/288596 (80%)]\tLoss: 0.347712\n",
            "Train Epoch: 4 [231680/288596 (80%)]\tLoss: 0.445310\n",
            "Train Epoch: 4 [232960/288596 (81%)]\tLoss: 0.336685\n",
            "Train Epoch: 4 [234240/288596 (81%)]\tLoss: 0.272570\n",
            "Train Epoch: 4 [235520/288596 (82%)]\tLoss: 0.284525\n",
            "Train Epoch: 4 [236800/288596 (82%)]\tLoss: 0.428056\n",
            "Train Epoch: 4 [238080/288596 (82%)]\tLoss: 0.356205\n",
            "Train Epoch: 4 [239360/288596 (83%)]\tLoss: 0.360149\n",
            "Train Epoch: 4 [240640/288596 (83%)]\tLoss: 0.273404\n",
            "Train Epoch: 4 [241920/288596 (84%)]\tLoss: 0.351661\n",
            "Train Epoch: 4 [243200/288596 (84%)]\tLoss: 0.332721\n",
            "Train Epoch: 4 [244480/288596 (85%)]\tLoss: 0.353760\n",
            "Train Epoch: 4 [245760/288596 (85%)]\tLoss: 0.419931\n",
            "Train Epoch: 4 [247040/288596 (86%)]\tLoss: 0.393818\n",
            "Train Epoch: 4 [248320/288596 (86%)]\tLoss: 0.314181\n",
            "Train Epoch: 4 [249600/288596 (86%)]\tLoss: 0.304938\n",
            "Train Epoch: 4 [250880/288596 (87%)]\tLoss: 0.387739\n",
            "Train Epoch: 4 [252160/288596 (87%)]\tLoss: 0.359207\n",
            "Train Epoch: 4 [253440/288596 (88%)]\tLoss: 0.328703\n",
            "Train Epoch: 4 [254720/288596 (88%)]\tLoss: 0.417758\n",
            "Train Epoch: 4 [256000/288596 (89%)]\tLoss: 0.295265\n",
            "Train Epoch: 4 [257280/288596 (89%)]\tLoss: 0.362618\n",
            "Train Epoch: 4 [258560/288596 (90%)]\tLoss: 0.398061\n",
            "Train Epoch: 4 [259840/288596 (90%)]\tLoss: 0.319448\n",
            "Train Epoch: 4 [261120/288596 (90%)]\tLoss: 0.368600\n",
            "Train Epoch: 4 [262400/288596 (91%)]\tLoss: 0.244411\n",
            "Train Epoch: 4 [263680/288596 (91%)]\tLoss: 0.380543\n",
            "Train Epoch: 4 [264960/288596 (92%)]\tLoss: 0.408053\n",
            "Train Epoch: 4 [266240/288596 (92%)]\tLoss: 0.346794\n",
            "Train Epoch: 4 [267520/288596 (93%)]\tLoss: 0.356288\n",
            "Train Epoch: 4 [268800/288596 (93%)]\tLoss: 0.314713\n",
            "Train Epoch: 4 [270080/288596 (94%)]\tLoss: 0.364600\n",
            "Train Epoch: 4 [271360/288596 (94%)]\tLoss: 0.281954\n",
            "Train Epoch: 4 [272640/288596 (94%)]\tLoss: 0.408575\n",
            "Train Epoch: 4 [273920/288596 (95%)]\tLoss: 0.314918\n",
            "Train Epoch: 4 [275200/288596 (95%)]\tLoss: 0.357435\n",
            "Train Epoch: 4 [276480/288596 (96%)]\tLoss: 0.333505\n",
            "Train Epoch: 4 [277760/288596 (96%)]\tLoss: 0.253623\n",
            "Train Epoch: 4 [279040/288596 (97%)]\tLoss: 0.388489\n",
            "Train Epoch: 4 [280320/288596 (97%)]\tLoss: 0.418776\n",
            "Train Epoch: 4 [281600/288596 (98%)]\tLoss: 0.335455\n",
            "Train Epoch: 4 [282880/288596 (98%)]\tLoss: 0.401304\n",
            "Train Epoch: 4 [284160/288596 (98%)]\tLoss: 0.281146\n",
            "Train Epoch: 4 [285440/288596 (99%)]\tLoss: 0.329393\n",
            "Train Epoch: 4 [286720/288596 (99%)]\tLoss: 0.366691\n",
            "Train Epoch: 4 [288000/288596 (100%)]\tLoss: 0.305315\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 107219/123684 (87%)\n",
            "\n",
            "Train Epoch: 5 [0/288596 (0%)]\tLoss: 0.312621\n",
            "Train Epoch: 5 [1280/288596 (0%)]\tLoss: 0.439261\n",
            "Train Epoch: 5 [2560/288596 (1%)]\tLoss: 0.318203\n",
            "Train Epoch: 5 [3840/288596 (1%)]\tLoss: 0.388801\n",
            "Train Epoch: 5 [5120/288596 (2%)]\tLoss: 0.360649\n",
            "Train Epoch: 5 [6400/288596 (2%)]\tLoss: 0.281825\n",
            "Train Epoch: 5 [7680/288596 (3%)]\tLoss: 0.322187\n",
            "Train Epoch: 5 [8960/288596 (3%)]\tLoss: 0.278867\n",
            "Train Epoch: 5 [10240/288596 (4%)]\tLoss: 0.328354\n",
            "Train Epoch: 5 [11520/288596 (4%)]\tLoss: 0.337272\n",
            "Train Epoch: 5 [12800/288596 (4%)]\tLoss: 0.342312\n",
            "Train Epoch: 5 [14080/288596 (5%)]\tLoss: 0.295326\n",
            "Train Epoch: 5 [15360/288596 (5%)]\tLoss: 0.325094\n",
            "Train Epoch: 5 [16640/288596 (6%)]\tLoss: 0.313154\n",
            "Train Epoch: 5 [17920/288596 (6%)]\tLoss: 0.299517\n",
            "Train Epoch: 5 [19200/288596 (7%)]\tLoss: 0.387399\n",
            "Train Epoch: 5 [20480/288596 (7%)]\tLoss: 0.396804\n",
            "Train Epoch: 5 [21760/288596 (8%)]\tLoss: 0.388770\n",
            "Train Epoch: 5 [23040/288596 (8%)]\tLoss: 0.457716\n",
            "Train Epoch: 5 [24320/288596 (8%)]\tLoss: 0.386116\n",
            "Train Epoch: 5 [25600/288596 (9%)]\tLoss: 0.405705\n",
            "Train Epoch: 5 [26880/288596 (9%)]\tLoss: 0.361148\n",
            "Train Epoch: 5 [28160/288596 (10%)]\tLoss: 0.345374\n",
            "Train Epoch: 5 [29440/288596 (10%)]\tLoss: 0.350286\n",
            "Train Epoch: 5 [30720/288596 (11%)]\tLoss: 0.338632\n",
            "Train Epoch: 5 [32000/288596 (11%)]\tLoss: 0.324296\n",
            "Train Epoch: 5 [33280/288596 (12%)]\tLoss: 0.419504\n",
            "Train Epoch: 5 [34560/288596 (12%)]\tLoss: 0.464952\n",
            "Train Epoch: 5 [35840/288596 (12%)]\tLoss: 0.333429\n",
            "Train Epoch: 5 [37120/288596 (13%)]\tLoss: 0.391917\n",
            "Train Epoch: 5 [38400/288596 (13%)]\tLoss: 0.357511\n",
            "Train Epoch: 5 [39680/288596 (14%)]\tLoss: 0.388361\n",
            "Train Epoch: 5 [40960/288596 (14%)]\tLoss: 0.312169\n",
            "Train Epoch: 5 [42240/288596 (15%)]\tLoss: 0.369394\n",
            "Train Epoch: 5 [43520/288596 (15%)]\tLoss: 0.364338\n",
            "Train Epoch: 5 [44800/288596 (16%)]\tLoss: 0.376720\n",
            "Train Epoch: 5 [46080/288596 (16%)]\tLoss: 0.376830\n",
            "Train Epoch: 5 [47360/288596 (16%)]\tLoss: 0.336783\n",
            "Train Epoch: 5 [48640/288596 (17%)]\tLoss: 0.365904\n",
            "Train Epoch: 5 [49920/288596 (17%)]\tLoss: 0.280880\n",
            "Train Epoch: 5 [51200/288596 (18%)]\tLoss: 0.294583\n",
            "Train Epoch: 5 [52480/288596 (18%)]\tLoss: 0.465466\n",
            "Train Epoch: 5 [53760/288596 (19%)]\tLoss: 0.408453\n",
            "Train Epoch: 5 [55040/288596 (19%)]\tLoss: 0.297714\n",
            "Train Epoch: 5 [56320/288596 (20%)]\tLoss: 0.391334\n",
            "Train Epoch: 5 [57600/288596 (20%)]\tLoss: 0.322752\n",
            "Train Epoch: 5 [58880/288596 (20%)]\tLoss: 0.326007\n",
            "Train Epoch: 5 [60160/288596 (21%)]\tLoss: 0.418429\n",
            "Train Epoch: 5 [61440/288596 (21%)]\tLoss: 0.288346\n",
            "Train Epoch: 5 [62720/288596 (22%)]\tLoss: 0.287473\n",
            "Train Epoch: 5 [64000/288596 (22%)]\tLoss: 0.295817\n",
            "Train Epoch: 5 [65280/288596 (23%)]\tLoss: 0.436876\n",
            "Train Epoch: 5 [66560/288596 (23%)]\tLoss: 0.378738\n",
            "Train Epoch: 5 [67840/288596 (24%)]\tLoss: 0.326359\n",
            "Train Epoch: 5 [69120/288596 (24%)]\tLoss: 0.256023\n",
            "Train Epoch: 5 [70400/288596 (24%)]\tLoss: 0.444419\n",
            "Train Epoch: 5 [71680/288596 (25%)]\tLoss: 0.468629\n",
            "Train Epoch: 5 [72960/288596 (25%)]\tLoss: 0.438279\n",
            "Train Epoch: 5 [74240/288596 (26%)]\tLoss: 0.293989\n",
            "Train Epoch: 5 [75520/288596 (26%)]\tLoss: 0.374678\n",
            "Train Epoch: 5 [76800/288596 (27%)]\tLoss: 0.435796\n",
            "Train Epoch: 5 [78080/288596 (27%)]\tLoss: 0.464610\n",
            "Train Epoch: 5 [79360/288596 (27%)]\tLoss: 0.267391\n",
            "Train Epoch: 5 [80640/288596 (28%)]\tLoss: 0.302003\n",
            "Train Epoch: 5 [81920/288596 (28%)]\tLoss: 0.303809\n",
            "Train Epoch: 5 [83200/288596 (29%)]\tLoss: 0.403394\n",
            "Train Epoch: 5 [84480/288596 (29%)]\tLoss: 0.453108\n",
            "Train Epoch: 5 [85760/288596 (30%)]\tLoss: 0.375572\n",
            "Train Epoch: 5 [87040/288596 (30%)]\tLoss: 0.353768\n",
            "Train Epoch: 5 [88320/288596 (31%)]\tLoss: 0.278760\n",
            "Train Epoch: 5 [89600/288596 (31%)]\tLoss: 0.300517\n",
            "Train Epoch: 5 [90880/288596 (31%)]\tLoss: 0.425462\n",
            "Train Epoch: 5 [92160/288596 (32%)]\tLoss: 0.299747\n",
            "Train Epoch: 5 [93440/288596 (32%)]\tLoss: 0.290737\n",
            "Train Epoch: 5 [94720/288596 (33%)]\tLoss: 0.378053\n",
            "Train Epoch: 5 [96000/288596 (33%)]\tLoss: 0.408706\n",
            "Train Epoch: 5 [97280/288596 (34%)]\tLoss: 0.380438\n",
            "Train Epoch: 5 [98560/288596 (34%)]\tLoss: 0.298482\n",
            "Train Epoch: 5 [99840/288596 (35%)]\tLoss: 0.363595\n",
            "Train Epoch: 5 [101120/288596 (35%)]\tLoss: 0.397616\n",
            "Train Epoch: 5 [102400/288596 (35%)]\tLoss: 0.420741\n",
            "Train Epoch: 5 [103680/288596 (36%)]\tLoss: 0.322887\n",
            "Train Epoch: 5 [104960/288596 (36%)]\tLoss: 0.383606\n",
            "Train Epoch: 5 [106240/288596 (37%)]\tLoss: 0.397506\n",
            "Train Epoch: 5 [107520/288596 (37%)]\tLoss: 0.497520\n",
            "Train Epoch: 5 [108800/288596 (38%)]\tLoss: 0.340829\n",
            "Train Epoch: 5 [110080/288596 (38%)]\tLoss: 0.363384\n",
            "Train Epoch: 5 [111360/288596 (39%)]\tLoss: 0.374092\n",
            "Train Epoch: 5 [112640/288596 (39%)]\tLoss: 0.340359\n",
            "Train Epoch: 5 [113920/288596 (39%)]\tLoss: 0.347227\n",
            "Train Epoch: 5 [115200/288596 (40%)]\tLoss: 0.220520\n",
            "Train Epoch: 5 [116480/288596 (40%)]\tLoss: 0.385773\n",
            "Train Epoch: 5 [117760/288596 (41%)]\tLoss: 0.354324\n",
            "Train Epoch: 5 [119040/288596 (41%)]\tLoss: 0.360497\n",
            "Train Epoch: 5 [120320/288596 (42%)]\tLoss: 0.287009\n",
            "Train Epoch: 5 [121600/288596 (42%)]\tLoss: 0.341845\n",
            "Train Epoch: 5 [122880/288596 (43%)]\tLoss: 0.376660\n",
            "Train Epoch: 5 [124160/288596 (43%)]\tLoss: 0.330041\n",
            "Train Epoch: 5 [125440/288596 (43%)]\tLoss: 0.329213\n",
            "Train Epoch: 5 [126720/288596 (44%)]\tLoss: 0.326949\n",
            "Train Epoch: 5 [128000/288596 (44%)]\tLoss: 0.338495\n",
            "Train Epoch: 5 [129280/288596 (45%)]\tLoss: 0.328012\n",
            "Train Epoch: 5 [130560/288596 (45%)]\tLoss: 0.355223\n",
            "Train Epoch: 5 [131840/288596 (46%)]\tLoss: 0.403890\n",
            "Train Epoch: 5 [133120/288596 (46%)]\tLoss: 0.250027\n",
            "Train Epoch: 5 [134400/288596 (47%)]\tLoss: 0.330575\n",
            "Train Epoch: 5 [135680/288596 (47%)]\tLoss: 0.379126\n",
            "Train Epoch: 5 [136960/288596 (47%)]\tLoss: 0.361132\n",
            "Train Epoch: 5 [138240/288596 (48%)]\tLoss: 0.334630\n",
            "Train Epoch: 5 [139520/288596 (48%)]\tLoss: 0.385710\n",
            "Train Epoch: 5 [140800/288596 (49%)]\tLoss: 0.422345\n",
            "Train Epoch: 5 [142080/288596 (49%)]\tLoss: 0.327579\n",
            "Train Epoch: 5 [143360/288596 (50%)]\tLoss: 0.302647\n",
            "Train Epoch: 5 [144640/288596 (50%)]\tLoss: 0.355038\n",
            "Train Epoch: 5 [145920/288596 (51%)]\tLoss: 0.373488\n",
            "Train Epoch: 5 [147200/288596 (51%)]\tLoss: 0.373507\n",
            "Train Epoch: 5 [148480/288596 (51%)]\tLoss: 0.414563\n",
            "Train Epoch: 5 [149760/288596 (52%)]\tLoss: 0.440647\n",
            "Train Epoch: 5 [151040/288596 (52%)]\tLoss: 0.417310\n",
            "Train Epoch: 5 [152320/288596 (53%)]\tLoss: 0.272320\n",
            "Train Epoch: 5 [153600/288596 (53%)]\tLoss: 0.220578\n",
            "Train Epoch: 5 [154880/288596 (54%)]\tLoss: 0.409852\n",
            "Train Epoch: 5 [156160/288596 (54%)]\tLoss: 0.381758\n",
            "Train Epoch: 5 [157440/288596 (55%)]\tLoss: 0.375270\n",
            "Train Epoch: 5 [158720/288596 (55%)]\tLoss: 0.399723\n",
            "Train Epoch: 5 [160000/288596 (55%)]\tLoss: 0.399975\n",
            "Train Epoch: 5 [161280/288596 (56%)]\tLoss: 0.304487\n",
            "Train Epoch: 5 [162560/288596 (56%)]\tLoss: 0.313406\n",
            "Train Epoch: 5 [163840/288596 (57%)]\tLoss: 0.363916\n",
            "Train Epoch: 5 [165120/288596 (57%)]\tLoss: 0.332878\n",
            "Train Epoch: 5 [166400/288596 (58%)]\tLoss: 0.494795\n",
            "Train Epoch: 5 [167680/288596 (58%)]\tLoss: 0.346086\n",
            "Train Epoch: 5 [168960/288596 (59%)]\tLoss: 0.432153\n",
            "Train Epoch: 5 [170240/288596 (59%)]\tLoss: 0.378230\n",
            "Train Epoch: 5 [171520/288596 (59%)]\tLoss: 0.345807\n",
            "Train Epoch: 5 [172800/288596 (60%)]\tLoss: 0.422180\n",
            "Train Epoch: 5 [174080/288596 (60%)]\tLoss: 0.327922\n",
            "Train Epoch: 5 [175360/288596 (61%)]\tLoss: 0.338715\n",
            "Train Epoch: 5 [176640/288596 (61%)]\tLoss: 0.431339\n",
            "Train Epoch: 5 [177920/288596 (62%)]\tLoss: 0.348335\n",
            "Train Epoch: 5 [179200/288596 (62%)]\tLoss: 0.370261\n",
            "Train Epoch: 5 [180480/288596 (63%)]\tLoss: 0.246509\n",
            "Train Epoch: 5 [181760/288596 (63%)]\tLoss: 0.331055\n",
            "Train Epoch: 5 [183040/288596 (63%)]\tLoss: 0.314307\n",
            "Train Epoch: 5 [184320/288596 (64%)]\tLoss: 0.345601\n",
            "Train Epoch: 5 [185600/288596 (64%)]\tLoss: 0.375419\n",
            "Train Epoch: 5 [186880/288596 (65%)]\tLoss: 0.359551\n",
            "Train Epoch: 5 [188160/288596 (65%)]\tLoss: 0.388214\n",
            "Train Epoch: 5 [189440/288596 (66%)]\tLoss: 0.359503\n",
            "Train Epoch: 5 [190720/288596 (66%)]\tLoss: 0.515801\n",
            "Train Epoch: 5 [192000/288596 (67%)]\tLoss: 0.376738\n",
            "Train Epoch: 5 [193280/288596 (67%)]\tLoss: 0.259147\n",
            "Train Epoch: 5 [194560/288596 (67%)]\tLoss: 0.310078\n",
            "Train Epoch: 5 [195840/288596 (68%)]\tLoss: 0.399579\n",
            "Train Epoch: 5 [197120/288596 (68%)]\tLoss: 0.408895\n",
            "Train Epoch: 5 [198400/288596 (69%)]\tLoss: 0.331607\n",
            "Train Epoch: 5 [199680/288596 (69%)]\tLoss: 0.476939\n",
            "Train Epoch: 5 [200960/288596 (70%)]\tLoss: 0.372607\n",
            "Train Epoch: 5 [202240/288596 (70%)]\tLoss: 0.387159\n",
            "Train Epoch: 5 [203520/288596 (71%)]\tLoss: 0.281858\n",
            "Train Epoch: 5 [204800/288596 (71%)]\tLoss: 0.395959\n",
            "Train Epoch: 5 [206080/288596 (71%)]\tLoss: 0.352790\n",
            "Train Epoch: 5 [207360/288596 (72%)]\tLoss: 0.326831\n",
            "Train Epoch: 5 [208640/288596 (72%)]\tLoss: 0.354550\n",
            "Train Epoch: 5 [209920/288596 (73%)]\tLoss: 0.328743\n",
            "Train Epoch: 5 [211200/288596 (73%)]\tLoss: 0.370721\n",
            "Train Epoch: 5 [212480/288596 (74%)]\tLoss: 0.392245\n",
            "Train Epoch: 5 [213760/288596 (74%)]\tLoss: 0.399608\n",
            "Train Epoch: 5 [215040/288596 (75%)]\tLoss: 0.357889\n",
            "Train Epoch: 5 [216320/288596 (75%)]\tLoss: 0.312538\n",
            "Train Epoch: 5 [217600/288596 (75%)]\tLoss: 0.426469\n",
            "Train Epoch: 5 [218880/288596 (76%)]\tLoss: 0.415499\n",
            "Train Epoch: 5 [220160/288596 (76%)]\tLoss: 0.356919\n",
            "Train Epoch: 5 [221440/288596 (77%)]\tLoss: 0.364664\n",
            "Train Epoch: 5 [222720/288596 (77%)]\tLoss: 0.335679\n",
            "Train Epoch: 5 [224000/288596 (78%)]\tLoss: 0.299676\n",
            "Train Epoch: 5 [225280/288596 (78%)]\tLoss: 0.438262\n",
            "Train Epoch: 5 [226560/288596 (78%)]\tLoss: 0.325309\n",
            "Train Epoch: 5 [227840/288596 (79%)]\tLoss: 0.252575\n",
            "Train Epoch: 5 [229120/288596 (79%)]\tLoss: 0.353014\n",
            "Train Epoch: 5 [230400/288596 (80%)]\tLoss: 0.358638\n",
            "Train Epoch: 5 [231680/288596 (80%)]\tLoss: 0.311515\n",
            "Train Epoch: 5 [232960/288596 (81%)]\tLoss: 0.431136\n",
            "Train Epoch: 5 [234240/288596 (81%)]\tLoss: 0.418616\n",
            "Train Epoch: 5 [235520/288596 (82%)]\tLoss: 0.299673\n",
            "Train Epoch: 5 [236800/288596 (82%)]\tLoss: 0.539260\n",
            "Train Epoch: 5 [238080/288596 (82%)]\tLoss: 0.311059\n",
            "Train Epoch: 5 [239360/288596 (83%)]\tLoss: 0.421615\n",
            "Train Epoch: 5 [240640/288596 (83%)]\tLoss: 0.417653\n",
            "Train Epoch: 5 [241920/288596 (84%)]\tLoss: 0.283606\n",
            "Train Epoch: 5 [243200/288596 (84%)]\tLoss: 0.428253\n",
            "Train Epoch: 5 [244480/288596 (85%)]\tLoss: 0.314330\n",
            "Train Epoch: 5 [245760/288596 (85%)]\tLoss: 0.299643\n",
            "Train Epoch: 5 [247040/288596 (86%)]\tLoss: 0.329812\n",
            "Train Epoch: 5 [248320/288596 (86%)]\tLoss: 0.435819\n",
            "Train Epoch: 5 [249600/288596 (86%)]\tLoss: 0.367144\n",
            "Train Epoch: 5 [250880/288596 (87%)]\tLoss: 0.301199\n",
            "Train Epoch: 5 [252160/288596 (87%)]\tLoss: 0.270921\n",
            "Train Epoch: 5 [253440/288596 (88%)]\tLoss: 0.317846\n",
            "Train Epoch: 5 [254720/288596 (88%)]\tLoss: 0.357220\n",
            "Train Epoch: 5 [256000/288596 (89%)]\tLoss: 0.370844\n",
            "Train Epoch: 5 [257280/288596 (89%)]\tLoss: 0.338097\n",
            "Train Epoch: 5 [258560/288596 (90%)]\tLoss: 0.263406\n",
            "Train Epoch: 5 [259840/288596 (90%)]\tLoss: 0.383009\n",
            "Train Epoch: 5 [261120/288596 (90%)]\tLoss: 0.437606\n",
            "Train Epoch: 5 [262400/288596 (91%)]\tLoss: 0.218361\n",
            "Train Epoch: 5 [263680/288596 (91%)]\tLoss: 0.361721\n",
            "Train Epoch: 5 [264960/288596 (92%)]\tLoss: 0.432915\n",
            "Train Epoch: 5 [266240/288596 (92%)]\tLoss: 0.301748\n",
            "Train Epoch: 5 [267520/288596 (93%)]\tLoss: 0.264057\n",
            "Train Epoch: 5 [268800/288596 (93%)]\tLoss: 0.422201\n",
            "Train Epoch: 5 [270080/288596 (94%)]\tLoss: 0.250781\n",
            "Train Epoch: 5 [271360/288596 (94%)]\tLoss: 0.293945\n",
            "Train Epoch: 5 [272640/288596 (94%)]\tLoss: 0.332082\n",
            "Train Epoch: 5 [273920/288596 (95%)]\tLoss: 0.393166\n",
            "Train Epoch: 5 [275200/288596 (95%)]\tLoss: 0.437244\n",
            "Train Epoch: 5 [276480/288596 (96%)]\tLoss: 0.360185\n",
            "Train Epoch: 5 [277760/288596 (96%)]\tLoss: 0.354908\n",
            "Train Epoch: 5 [279040/288596 (97%)]\tLoss: 0.411060\n",
            "Train Epoch: 5 [280320/288596 (97%)]\tLoss: 0.346538\n",
            "Train Epoch: 5 [281600/288596 (98%)]\tLoss: 0.303481\n",
            "Train Epoch: 5 [282880/288596 (98%)]\tLoss: 0.320891\n",
            "Train Epoch: 5 [284160/288596 (98%)]\tLoss: 0.431181\n",
            "Train Epoch: 5 [285440/288596 (99%)]\tLoss: 0.294856\n",
            "Train Epoch: 5 [286720/288596 (99%)]\tLoss: 0.381311\n",
            "Train Epoch: 5 [288000/288596 (100%)]\tLoss: 0.400378\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 107299/123684 (87%)\n",
            "\n",
            "Train Epoch: 6 [0/288596 (0%)]\tLoss: 0.358872\n",
            "Train Epoch: 6 [1280/288596 (0%)]\tLoss: 0.349380\n",
            "Train Epoch: 6 [2560/288596 (1%)]\tLoss: 0.333449\n",
            "Train Epoch: 6 [3840/288596 (1%)]\tLoss: 0.352903\n",
            "Train Epoch: 6 [5120/288596 (2%)]\tLoss: 0.261125\n",
            "Train Epoch: 6 [6400/288596 (2%)]\tLoss: 0.346565\n",
            "Train Epoch: 6 [7680/288596 (3%)]\tLoss: 0.390155\n",
            "Train Epoch: 6 [8960/288596 (3%)]\tLoss: 0.375818\n",
            "Train Epoch: 6 [10240/288596 (4%)]\tLoss: 0.345728\n",
            "Train Epoch: 6 [11520/288596 (4%)]\tLoss: 0.513457\n",
            "Train Epoch: 6 [12800/288596 (4%)]\tLoss: 0.303905\n",
            "Train Epoch: 6 [14080/288596 (5%)]\tLoss: 0.412322\n",
            "Train Epoch: 6 [15360/288596 (5%)]\tLoss: 0.318545\n",
            "Train Epoch: 6 [16640/288596 (6%)]\tLoss: 0.286941\n",
            "Train Epoch: 6 [17920/288596 (6%)]\tLoss: 0.303157\n",
            "Train Epoch: 6 [19200/288596 (7%)]\tLoss: 0.289562\n",
            "Train Epoch: 6 [20480/288596 (7%)]\tLoss: 0.394968\n",
            "Train Epoch: 6 [21760/288596 (8%)]\tLoss: 0.342446\n",
            "Train Epoch: 6 [23040/288596 (8%)]\tLoss: 0.307095\n",
            "Train Epoch: 6 [24320/288596 (8%)]\tLoss: 0.285135\n",
            "Train Epoch: 6 [25600/288596 (9%)]\tLoss: 0.291854\n",
            "Train Epoch: 6 [26880/288596 (9%)]\tLoss: 0.387663\n",
            "Train Epoch: 6 [28160/288596 (10%)]\tLoss: 0.359556\n",
            "Train Epoch: 6 [29440/288596 (10%)]\tLoss: 0.456106\n",
            "Train Epoch: 6 [30720/288596 (11%)]\tLoss: 0.297992\n",
            "Train Epoch: 6 [32000/288596 (11%)]\tLoss: 0.259677\n",
            "Train Epoch: 6 [33280/288596 (12%)]\tLoss: 0.343346\n",
            "Train Epoch: 6 [34560/288596 (12%)]\tLoss: 0.410112\n",
            "Train Epoch: 6 [35840/288596 (12%)]\tLoss: 0.445079\n",
            "Train Epoch: 6 [37120/288596 (13%)]\tLoss: 0.445739\n",
            "Train Epoch: 6 [38400/288596 (13%)]\tLoss: 0.413600\n",
            "Train Epoch: 6 [39680/288596 (14%)]\tLoss: 0.373465\n",
            "Train Epoch: 6 [40960/288596 (14%)]\tLoss: 0.268464\n",
            "Train Epoch: 6 [42240/288596 (15%)]\tLoss: 0.298824\n",
            "Train Epoch: 6 [43520/288596 (15%)]\tLoss: 0.471619\n",
            "Train Epoch: 6 [44800/288596 (16%)]\tLoss: 0.344772\n",
            "Train Epoch: 6 [46080/288596 (16%)]\tLoss: 0.350376\n",
            "Train Epoch: 6 [47360/288596 (16%)]\tLoss: 0.362767\n",
            "Train Epoch: 6 [48640/288596 (17%)]\tLoss: 0.344475\n",
            "Train Epoch: 6 [49920/288596 (17%)]\tLoss: 0.306652\n",
            "Train Epoch: 6 [51200/288596 (18%)]\tLoss: 0.346765\n",
            "Train Epoch: 6 [52480/288596 (18%)]\tLoss: 0.380139\n",
            "Train Epoch: 6 [53760/288596 (19%)]\tLoss: 0.317718\n",
            "Train Epoch: 6 [55040/288596 (19%)]\tLoss: 0.334228\n",
            "Train Epoch: 6 [56320/288596 (20%)]\tLoss: 0.278869\n",
            "Train Epoch: 6 [57600/288596 (20%)]\tLoss: 0.330082\n",
            "Train Epoch: 6 [58880/288596 (20%)]\tLoss: 0.302049\n",
            "Train Epoch: 6 [60160/288596 (21%)]\tLoss: 0.392535\n",
            "Train Epoch: 6 [61440/288596 (21%)]\tLoss: 0.342485\n",
            "Train Epoch: 6 [62720/288596 (22%)]\tLoss: 0.310750\n",
            "Train Epoch: 6 [64000/288596 (22%)]\tLoss: 0.385246\n",
            "Train Epoch: 6 [65280/288596 (23%)]\tLoss: 0.291875\n",
            "Train Epoch: 6 [66560/288596 (23%)]\tLoss: 0.365442\n",
            "Train Epoch: 6 [67840/288596 (24%)]\tLoss: 0.331558\n",
            "Train Epoch: 6 [69120/288596 (24%)]\tLoss: 0.315655\n",
            "Train Epoch: 6 [70400/288596 (24%)]\tLoss: 0.315635\n",
            "Train Epoch: 6 [71680/288596 (25%)]\tLoss: 0.289802\n",
            "Train Epoch: 6 [72960/288596 (25%)]\tLoss: 0.336216\n",
            "Train Epoch: 6 [74240/288596 (26%)]\tLoss: 0.355188\n",
            "Train Epoch: 6 [75520/288596 (26%)]\tLoss: 0.327727\n",
            "Train Epoch: 6 [76800/288596 (27%)]\tLoss: 0.285669\n",
            "Train Epoch: 6 [78080/288596 (27%)]\tLoss: 0.282752\n",
            "Train Epoch: 6 [79360/288596 (27%)]\tLoss: 0.327517\n",
            "Train Epoch: 6 [80640/288596 (28%)]\tLoss: 0.407743\n",
            "Train Epoch: 6 [81920/288596 (28%)]\tLoss: 0.263764\n",
            "Train Epoch: 6 [83200/288596 (29%)]\tLoss: 0.379716\n",
            "Train Epoch: 6 [84480/288596 (29%)]\tLoss: 0.308208\n",
            "Train Epoch: 6 [85760/288596 (30%)]\tLoss: 0.432234\n",
            "Train Epoch: 6 [87040/288596 (30%)]\tLoss: 0.305616\n",
            "Train Epoch: 6 [88320/288596 (31%)]\tLoss: 0.290361\n",
            "Train Epoch: 6 [89600/288596 (31%)]\tLoss: 0.268870\n",
            "Train Epoch: 6 [90880/288596 (31%)]\tLoss: 0.395297\n",
            "Train Epoch: 6 [92160/288596 (32%)]\tLoss: 0.362027\n",
            "Train Epoch: 6 [93440/288596 (32%)]\tLoss: 0.397787\n",
            "Train Epoch: 6 [94720/288596 (33%)]\tLoss: 0.351615\n",
            "Train Epoch: 6 [96000/288596 (33%)]\tLoss: 0.434249\n",
            "Train Epoch: 6 [97280/288596 (34%)]\tLoss: 0.408140\n",
            "Train Epoch: 6 [98560/288596 (34%)]\tLoss: 0.292807\n",
            "Train Epoch: 6 [99840/288596 (35%)]\tLoss: 0.323772\n",
            "Train Epoch: 6 [101120/288596 (35%)]\tLoss: 0.392453\n",
            "Train Epoch: 6 [102400/288596 (35%)]\tLoss: 0.326839\n",
            "Train Epoch: 6 [103680/288596 (36%)]\tLoss: 0.288657\n",
            "Train Epoch: 6 [104960/288596 (36%)]\tLoss: 0.357523\n",
            "Train Epoch: 6 [106240/288596 (37%)]\tLoss: 0.350206\n",
            "Train Epoch: 6 [107520/288596 (37%)]\tLoss: 0.339321\n",
            "Train Epoch: 6 [108800/288596 (38%)]\tLoss: 0.448645\n",
            "Train Epoch: 6 [110080/288596 (38%)]\tLoss: 0.363227\n",
            "Train Epoch: 6 [111360/288596 (39%)]\tLoss: 0.333561\n",
            "Train Epoch: 6 [112640/288596 (39%)]\tLoss: 0.377266\n",
            "Train Epoch: 6 [113920/288596 (39%)]\tLoss: 0.374275\n",
            "Train Epoch: 6 [115200/288596 (40%)]\tLoss: 0.333020\n",
            "Train Epoch: 6 [116480/288596 (40%)]\tLoss: 0.322700\n",
            "Train Epoch: 6 [117760/288596 (41%)]\tLoss: 0.359907\n",
            "Train Epoch: 6 [119040/288596 (41%)]\tLoss: 0.405557\n",
            "Train Epoch: 6 [120320/288596 (42%)]\tLoss: 0.255315\n",
            "Train Epoch: 6 [121600/288596 (42%)]\tLoss: 0.352364\n",
            "Train Epoch: 6 [122880/288596 (43%)]\tLoss: 0.326697\n",
            "Train Epoch: 6 [124160/288596 (43%)]\tLoss: 0.459461\n",
            "Train Epoch: 6 [125440/288596 (43%)]\tLoss: 0.325447\n",
            "Train Epoch: 6 [126720/288596 (44%)]\tLoss: 0.558219\n",
            "Train Epoch: 6 [128000/288596 (44%)]\tLoss: 0.298072\n",
            "Train Epoch: 6 [129280/288596 (45%)]\tLoss: 0.411692\n",
            "Train Epoch: 6 [130560/288596 (45%)]\tLoss: 0.367200\n",
            "Train Epoch: 6 [131840/288596 (46%)]\tLoss: 0.332372\n",
            "Train Epoch: 6 [133120/288596 (46%)]\tLoss: 0.311762\n",
            "Train Epoch: 6 [134400/288596 (47%)]\tLoss: 0.393876\n",
            "Train Epoch: 6 [135680/288596 (47%)]\tLoss: 0.385603\n",
            "Train Epoch: 6 [136960/288596 (47%)]\tLoss: 0.379308\n",
            "Train Epoch: 6 [138240/288596 (48%)]\tLoss: 0.301594\n",
            "Train Epoch: 6 [139520/288596 (48%)]\tLoss: 0.368954\n",
            "Train Epoch: 6 [140800/288596 (49%)]\tLoss: 0.353939\n",
            "Train Epoch: 6 [142080/288596 (49%)]\tLoss: 0.293142\n",
            "Train Epoch: 6 [143360/288596 (50%)]\tLoss: 0.362623\n",
            "Train Epoch: 6 [144640/288596 (50%)]\tLoss: 0.354067\n",
            "Train Epoch: 6 [145920/288596 (51%)]\tLoss: 0.415725\n",
            "Train Epoch: 6 [147200/288596 (51%)]\tLoss: 0.344960\n",
            "Train Epoch: 6 [148480/288596 (51%)]\tLoss: 0.391951\n",
            "Train Epoch: 6 [149760/288596 (52%)]\tLoss: 0.282803\n",
            "Train Epoch: 6 [151040/288596 (52%)]\tLoss: 0.389217\n",
            "Train Epoch: 6 [152320/288596 (53%)]\tLoss: 0.303679\n",
            "Train Epoch: 6 [153600/288596 (53%)]\tLoss: 0.341863\n",
            "Train Epoch: 6 [154880/288596 (54%)]\tLoss: 0.295815\n",
            "Train Epoch: 6 [156160/288596 (54%)]\tLoss: 0.396796\n",
            "Train Epoch: 6 [157440/288596 (55%)]\tLoss: 0.341436\n",
            "Train Epoch: 6 [158720/288596 (55%)]\tLoss: 0.436265\n",
            "Train Epoch: 6 [160000/288596 (55%)]\tLoss: 0.391835\n",
            "Train Epoch: 6 [161280/288596 (56%)]\tLoss: 0.320553\n",
            "Train Epoch: 6 [162560/288596 (56%)]\tLoss: 0.277836\n",
            "Train Epoch: 6 [163840/288596 (57%)]\tLoss: 0.369386\n",
            "Train Epoch: 6 [165120/288596 (57%)]\tLoss: 0.315323\n",
            "Train Epoch: 6 [166400/288596 (58%)]\tLoss: 0.367857\n",
            "Train Epoch: 6 [167680/288596 (58%)]\tLoss: 0.344886\n",
            "Train Epoch: 6 [168960/288596 (59%)]\tLoss: 0.276278\n",
            "Train Epoch: 6 [170240/288596 (59%)]\tLoss: 0.427012\n",
            "Train Epoch: 6 [171520/288596 (59%)]\tLoss: 0.299818\n",
            "Train Epoch: 6 [172800/288596 (60%)]\tLoss: 0.401764\n",
            "Train Epoch: 6 [174080/288596 (60%)]\tLoss: 0.240103\n",
            "Train Epoch: 6 [175360/288596 (61%)]\tLoss: 0.239143\n",
            "Train Epoch: 6 [176640/288596 (61%)]\tLoss: 0.381726\n",
            "Train Epoch: 6 [177920/288596 (62%)]\tLoss: 0.307621\n",
            "Train Epoch: 6 [179200/288596 (62%)]\tLoss: 0.381968\n",
            "Train Epoch: 6 [180480/288596 (63%)]\tLoss: 0.411025\n",
            "Train Epoch: 6 [181760/288596 (63%)]\tLoss: 0.302032\n",
            "Train Epoch: 6 [183040/288596 (63%)]\tLoss: 0.256779\n",
            "Train Epoch: 6 [184320/288596 (64%)]\tLoss: 0.362567\n",
            "Train Epoch: 6 [185600/288596 (64%)]\tLoss: 0.439226\n",
            "Train Epoch: 6 [186880/288596 (65%)]\tLoss: 0.294047\n",
            "Train Epoch: 6 [188160/288596 (65%)]\tLoss: 0.345809\n",
            "Train Epoch: 6 [189440/288596 (66%)]\tLoss: 0.290851\n",
            "Train Epoch: 6 [190720/288596 (66%)]\tLoss: 0.400130\n",
            "Train Epoch: 6 [192000/288596 (67%)]\tLoss: 0.327616\n",
            "Train Epoch: 6 [193280/288596 (67%)]\tLoss: 0.358235\n",
            "Train Epoch: 6 [194560/288596 (67%)]\tLoss: 0.338376\n",
            "Train Epoch: 6 [195840/288596 (68%)]\tLoss: 0.297966\n",
            "Train Epoch: 6 [197120/288596 (68%)]\tLoss: 0.363446\n",
            "Train Epoch: 6 [198400/288596 (69%)]\tLoss: 0.245686\n",
            "Train Epoch: 6 [199680/288596 (69%)]\tLoss: 0.381688\n",
            "Train Epoch: 6 [200960/288596 (70%)]\tLoss: 0.366766\n",
            "Train Epoch: 6 [202240/288596 (70%)]\tLoss: 0.344130\n",
            "Train Epoch: 6 [203520/288596 (71%)]\tLoss: 0.356992\n",
            "Train Epoch: 6 [204800/288596 (71%)]\tLoss: 0.328841\n",
            "Train Epoch: 6 [206080/288596 (71%)]\tLoss: 0.402235\n",
            "Train Epoch: 6 [207360/288596 (72%)]\tLoss: 0.322025\n",
            "Train Epoch: 6 [208640/288596 (72%)]\tLoss: 0.303804\n",
            "Train Epoch: 6 [209920/288596 (73%)]\tLoss: 0.357348\n",
            "Train Epoch: 6 [211200/288596 (73%)]\tLoss: 0.326854\n",
            "Train Epoch: 6 [212480/288596 (74%)]\tLoss: 0.376161\n",
            "Train Epoch: 6 [213760/288596 (74%)]\tLoss: 0.351704\n",
            "Train Epoch: 6 [215040/288596 (75%)]\tLoss: 0.327401\n",
            "Train Epoch: 6 [216320/288596 (75%)]\tLoss: 0.410363\n",
            "Train Epoch: 6 [217600/288596 (75%)]\tLoss: 0.277129\n",
            "Train Epoch: 6 [218880/288596 (76%)]\tLoss: 0.306183\n",
            "Train Epoch: 6 [220160/288596 (76%)]\tLoss: 0.296954\n",
            "Train Epoch: 6 [221440/288596 (77%)]\tLoss: 0.326178\n",
            "Train Epoch: 6 [222720/288596 (77%)]\tLoss: 0.399693\n",
            "Train Epoch: 6 [224000/288596 (78%)]\tLoss: 0.273936\n",
            "Train Epoch: 6 [225280/288596 (78%)]\tLoss: 0.436288\n",
            "Train Epoch: 6 [226560/288596 (78%)]\tLoss: 0.487316\n",
            "Train Epoch: 6 [227840/288596 (79%)]\tLoss: 0.359406\n",
            "Train Epoch: 6 [229120/288596 (79%)]\tLoss: 0.285218\n",
            "Train Epoch: 6 [230400/288596 (80%)]\tLoss: 0.219193\n",
            "Train Epoch: 6 [231680/288596 (80%)]\tLoss: 0.443770\n",
            "Train Epoch: 6 [232960/288596 (81%)]\tLoss: 0.358045\n",
            "Train Epoch: 6 [234240/288596 (81%)]\tLoss: 0.314328\n",
            "Train Epoch: 6 [235520/288596 (82%)]\tLoss: 0.351833\n",
            "Train Epoch: 6 [236800/288596 (82%)]\tLoss: 0.364179\n",
            "Train Epoch: 6 [238080/288596 (82%)]\tLoss: 0.478357\n",
            "Train Epoch: 6 [239360/288596 (83%)]\tLoss: 0.422955\n",
            "Train Epoch: 6 [240640/288596 (83%)]\tLoss: 0.395293\n",
            "Train Epoch: 6 [241920/288596 (84%)]\tLoss: 0.283521\n",
            "Train Epoch: 6 [243200/288596 (84%)]\tLoss: 0.401880\n",
            "Train Epoch: 6 [244480/288596 (85%)]\tLoss: 0.357783\n",
            "Train Epoch: 6 [245760/288596 (85%)]\tLoss: 0.261409\n",
            "Train Epoch: 6 [247040/288596 (86%)]\tLoss: 0.274338\n",
            "Train Epoch: 6 [248320/288596 (86%)]\tLoss: 0.510334\n",
            "Train Epoch: 6 [249600/288596 (86%)]\tLoss: 0.304505\n",
            "Train Epoch: 6 [250880/288596 (87%)]\tLoss: 0.396619\n",
            "Train Epoch: 6 [252160/288596 (87%)]\tLoss: 0.452651\n",
            "Train Epoch: 6 [253440/288596 (88%)]\tLoss: 0.376265\n",
            "Train Epoch: 6 [254720/288596 (88%)]\tLoss: 0.411617\n",
            "Train Epoch: 6 [256000/288596 (89%)]\tLoss: 0.384987\n",
            "Train Epoch: 6 [257280/288596 (89%)]\tLoss: 0.327525\n",
            "Train Epoch: 6 [258560/288596 (90%)]\tLoss: 0.327470\n",
            "Train Epoch: 6 [259840/288596 (90%)]\tLoss: 0.317543\n",
            "Train Epoch: 6 [261120/288596 (90%)]\tLoss: 0.323626\n",
            "Train Epoch: 6 [262400/288596 (91%)]\tLoss: 0.337240\n",
            "Train Epoch: 6 [263680/288596 (91%)]\tLoss: 0.450958\n",
            "Train Epoch: 6 [264960/288596 (92%)]\tLoss: 0.258641\n",
            "Train Epoch: 6 [266240/288596 (92%)]\tLoss: 0.321708\n",
            "Train Epoch: 6 [267520/288596 (93%)]\tLoss: 0.268943\n",
            "Train Epoch: 6 [268800/288596 (93%)]\tLoss: 0.453811\n",
            "Train Epoch: 6 [270080/288596 (94%)]\tLoss: 0.439554\n",
            "Train Epoch: 6 [271360/288596 (94%)]\tLoss: 0.416257\n",
            "Train Epoch: 6 [272640/288596 (94%)]\tLoss: 0.285868\n",
            "Train Epoch: 6 [273920/288596 (95%)]\tLoss: 0.553964\n",
            "Train Epoch: 6 [275200/288596 (95%)]\tLoss: 0.283589\n",
            "Train Epoch: 6 [276480/288596 (96%)]\tLoss: 0.365897\n",
            "Train Epoch: 6 [277760/288596 (96%)]\tLoss: 0.317059\n",
            "Train Epoch: 6 [279040/288596 (97%)]\tLoss: 0.414774\n",
            "Train Epoch: 6 [280320/288596 (97%)]\tLoss: 0.417307\n",
            "Train Epoch: 6 [281600/288596 (98%)]\tLoss: 0.441499\n",
            "Train Epoch: 6 [282880/288596 (98%)]\tLoss: 0.341973\n",
            "Train Epoch: 6 [284160/288596 (98%)]\tLoss: 0.418395\n",
            "Train Epoch: 6 [285440/288596 (99%)]\tLoss: 0.281679\n",
            "Train Epoch: 6 [286720/288596 (99%)]\tLoss: 0.348302\n",
            "Train Epoch: 6 [288000/288596 (100%)]\tLoss: 0.322883\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 107362/123684 (87%)\n",
            "\n",
            "Train Epoch: 7 [0/288596 (0%)]\tLoss: 0.360305\n",
            "Train Epoch: 7 [1280/288596 (0%)]\tLoss: 0.355385\n",
            "Train Epoch: 7 [2560/288596 (1%)]\tLoss: 0.386434\n",
            "Train Epoch: 7 [3840/288596 (1%)]\tLoss: 0.473703\n",
            "Train Epoch: 7 [5120/288596 (2%)]\tLoss: 0.303665\n",
            "Train Epoch: 7 [6400/288596 (2%)]\tLoss: 0.332177\n",
            "Train Epoch: 7 [7680/288596 (3%)]\tLoss: 0.368779\n",
            "Train Epoch: 7 [8960/288596 (3%)]\tLoss: 0.311299\n",
            "Train Epoch: 7 [10240/288596 (4%)]\tLoss: 0.301583\n",
            "Train Epoch: 7 [11520/288596 (4%)]\tLoss: 0.369471\n",
            "Train Epoch: 7 [12800/288596 (4%)]\tLoss: 0.480961\n",
            "Train Epoch: 7 [14080/288596 (5%)]\tLoss: 0.272020\n",
            "Train Epoch: 7 [15360/288596 (5%)]\tLoss: 0.321893\n",
            "Train Epoch: 7 [16640/288596 (6%)]\tLoss: 0.335241\n",
            "Train Epoch: 7 [17920/288596 (6%)]\tLoss: 0.375701\n",
            "Train Epoch: 7 [19200/288596 (7%)]\tLoss: 0.311602\n",
            "Train Epoch: 7 [20480/288596 (7%)]\tLoss: 0.287117\n",
            "Train Epoch: 7 [21760/288596 (8%)]\tLoss: 0.364528\n",
            "Train Epoch: 7 [23040/288596 (8%)]\tLoss: 0.315716\n",
            "Train Epoch: 7 [24320/288596 (8%)]\tLoss: 0.400312\n",
            "Train Epoch: 7 [25600/288596 (9%)]\tLoss: 0.397751\n",
            "Train Epoch: 7 [26880/288596 (9%)]\tLoss: 0.382784\n",
            "Train Epoch: 7 [28160/288596 (10%)]\tLoss: 0.352459\n",
            "Train Epoch: 7 [29440/288596 (10%)]\tLoss: 0.353398\n",
            "Train Epoch: 7 [30720/288596 (11%)]\tLoss: 0.270937\n",
            "Train Epoch: 7 [32000/288596 (11%)]\tLoss: 0.432440\n",
            "Train Epoch: 7 [33280/288596 (12%)]\tLoss: 0.295888\n",
            "Train Epoch: 7 [34560/288596 (12%)]\tLoss: 0.261361\n",
            "Train Epoch: 7 [35840/288596 (12%)]\tLoss: 0.336372\n",
            "Train Epoch: 7 [37120/288596 (13%)]\tLoss: 0.276562\n",
            "Train Epoch: 7 [38400/288596 (13%)]\tLoss: 0.279334\n",
            "Train Epoch: 7 [39680/288596 (14%)]\tLoss: 0.280420\n",
            "Train Epoch: 7 [40960/288596 (14%)]\tLoss: 0.376742\n",
            "Train Epoch: 7 [42240/288596 (15%)]\tLoss: 0.415323\n",
            "Train Epoch: 7 [43520/288596 (15%)]\tLoss: 0.283835\n",
            "Train Epoch: 7 [44800/288596 (16%)]\tLoss: 0.352953\n",
            "Train Epoch: 7 [46080/288596 (16%)]\tLoss: 0.286046\n",
            "Train Epoch: 7 [47360/288596 (16%)]\tLoss: 0.397025\n",
            "Train Epoch: 7 [48640/288596 (17%)]\tLoss: 0.338418\n",
            "Train Epoch: 7 [49920/288596 (17%)]\tLoss: 0.337856\n",
            "Train Epoch: 7 [51200/288596 (18%)]\tLoss: 0.447624\n",
            "Train Epoch: 7 [52480/288596 (18%)]\tLoss: 0.378790\n",
            "Train Epoch: 7 [53760/288596 (19%)]\tLoss: 0.392143\n",
            "Train Epoch: 7 [55040/288596 (19%)]\tLoss: 0.438020\n",
            "Train Epoch: 7 [56320/288596 (20%)]\tLoss: 0.390040\n",
            "Train Epoch: 7 [57600/288596 (20%)]\tLoss: 0.320945\n",
            "Train Epoch: 7 [58880/288596 (20%)]\tLoss: 0.302871\n",
            "Train Epoch: 7 [60160/288596 (21%)]\tLoss: 0.369031\n",
            "Train Epoch: 7 [61440/288596 (21%)]\tLoss: 0.342560\n",
            "Train Epoch: 7 [62720/288596 (22%)]\tLoss: 0.295136\n",
            "Train Epoch: 7 [64000/288596 (22%)]\tLoss: 0.239783\n",
            "Train Epoch: 7 [65280/288596 (23%)]\tLoss: 0.336460\n",
            "Train Epoch: 7 [66560/288596 (23%)]\tLoss: 0.393807\n",
            "Train Epoch: 7 [67840/288596 (24%)]\tLoss: 0.349242\n",
            "Train Epoch: 7 [69120/288596 (24%)]\tLoss: 0.332276\n",
            "Train Epoch: 7 [70400/288596 (24%)]\tLoss: 0.296063\n",
            "Train Epoch: 7 [71680/288596 (25%)]\tLoss: 0.386421\n",
            "Train Epoch: 7 [72960/288596 (25%)]\tLoss: 0.286077\n",
            "Train Epoch: 7 [74240/288596 (26%)]\tLoss: 0.354642\n",
            "Train Epoch: 7 [75520/288596 (26%)]\tLoss: 0.357433\n",
            "Train Epoch: 7 [76800/288596 (27%)]\tLoss: 0.335784\n",
            "Train Epoch: 7 [78080/288596 (27%)]\tLoss: 0.391519\n",
            "Train Epoch: 7 [79360/288596 (27%)]\tLoss: 0.331905\n",
            "Train Epoch: 7 [80640/288596 (28%)]\tLoss: 0.366494\n",
            "Train Epoch: 7 [81920/288596 (28%)]\tLoss: 0.289460\n",
            "Train Epoch: 7 [83200/288596 (29%)]\tLoss: 0.277915\n",
            "Train Epoch: 7 [84480/288596 (29%)]\tLoss: 0.447397\n",
            "Train Epoch: 7 [85760/288596 (30%)]\tLoss: 0.398149\n",
            "Train Epoch: 7 [87040/288596 (30%)]\tLoss: 0.325671\n",
            "Train Epoch: 7 [88320/288596 (31%)]\tLoss: 0.280490\n",
            "Train Epoch: 7 [89600/288596 (31%)]\tLoss: 0.303941\n",
            "Train Epoch: 7 [90880/288596 (31%)]\tLoss: 0.335874\n",
            "Train Epoch: 7 [92160/288596 (32%)]\tLoss: 0.467759\n",
            "Train Epoch: 7 [93440/288596 (32%)]\tLoss: 0.347687\n",
            "Train Epoch: 7 [94720/288596 (33%)]\tLoss: 0.318306\n",
            "Train Epoch: 7 [96000/288596 (33%)]\tLoss: 0.269448\n",
            "Train Epoch: 7 [97280/288596 (34%)]\tLoss: 0.321982\n",
            "Train Epoch: 7 [98560/288596 (34%)]\tLoss: 0.310367\n",
            "Train Epoch: 7 [99840/288596 (35%)]\tLoss: 0.386642\n",
            "Train Epoch: 7 [101120/288596 (35%)]\tLoss: 0.262579\n",
            "Train Epoch: 7 [102400/288596 (35%)]\tLoss: 0.433032\n",
            "Train Epoch: 7 [103680/288596 (36%)]\tLoss: 0.370952\n",
            "Train Epoch: 7 [104960/288596 (36%)]\tLoss: 0.431911\n",
            "Train Epoch: 7 [106240/288596 (37%)]\tLoss: 0.351148\n",
            "Train Epoch: 7 [107520/288596 (37%)]\tLoss: 0.299757\n",
            "Train Epoch: 7 [108800/288596 (38%)]\tLoss: 0.398067\n",
            "Train Epoch: 7 [110080/288596 (38%)]\tLoss: 0.455375\n",
            "Train Epoch: 7 [111360/288596 (39%)]\tLoss: 0.296821\n",
            "Train Epoch: 7 [112640/288596 (39%)]\tLoss: 0.477328\n",
            "Train Epoch: 7 [113920/288596 (39%)]\tLoss: 0.318913\n",
            "Train Epoch: 7 [115200/288596 (40%)]\tLoss: 0.367649\n",
            "Train Epoch: 7 [116480/288596 (40%)]\tLoss: 0.331809\n",
            "Train Epoch: 7 [117760/288596 (41%)]\tLoss: 0.389406\n",
            "Train Epoch: 7 [119040/288596 (41%)]\tLoss: 0.319588\n",
            "Train Epoch: 7 [120320/288596 (42%)]\tLoss: 0.340208\n",
            "Train Epoch: 7 [121600/288596 (42%)]\tLoss: 0.272370\n",
            "Train Epoch: 7 [122880/288596 (43%)]\tLoss: 0.339342\n",
            "Train Epoch: 7 [124160/288596 (43%)]\tLoss: 0.279206\n",
            "Train Epoch: 7 [125440/288596 (43%)]\tLoss: 0.320747\n",
            "Train Epoch: 7 [126720/288596 (44%)]\tLoss: 0.370500\n",
            "Train Epoch: 7 [128000/288596 (44%)]\tLoss: 0.388201\n",
            "Train Epoch: 7 [129280/288596 (45%)]\tLoss: 0.356572\n",
            "Train Epoch: 7 [130560/288596 (45%)]\tLoss: 0.370642\n",
            "Train Epoch: 7 [131840/288596 (46%)]\tLoss: 0.353540\n",
            "Train Epoch: 7 [133120/288596 (46%)]\tLoss: 0.331062\n",
            "Train Epoch: 7 [134400/288596 (47%)]\tLoss: 0.304735\n",
            "Train Epoch: 7 [135680/288596 (47%)]\tLoss: 0.272795\n",
            "Train Epoch: 7 [136960/288596 (47%)]\tLoss: 0.317630\n",
            "Train Epoch: 7 [138240/288596 (48%)]\tLoss: 0.335533\n",
            "Train Epoch: 7 [139520/288596 (48%)]\tLoss: 0.358384\n",
            "Train Epoch: 7 [140800/288596 (49%)]\tLoss: 0.329875\n",
            "Train Epoch: 7 [142080/288596 (49%)]\tLoss: 0.312530\n",
            "Train Epoch: 7 [143360/288596 (50%)]\tLoss: 0.339560\n",
            "Train Epoch: 7 [144640/288596 (50%)]\tLoss: 0.347604\n",
            "Train Epoch: 7 [145920/288596 (51%)]\tLoss: 0.374834\n",
            "Train Epoch: 7 [147200/288596 (51%)]\tLoss: 0.294592\n",
            "Train Epoch: 7 [148480/288596 (51%)]\tLoss: 0.271207\n",
            "Train Epoch: 7 [149760/288596 (52%)]\tLoss: 0.431096\n",
            "Train Epoch: 7 [151040/288596 (52%)]\tLoss: 0.346859\n",
            "Train Epoch: 7 [152320/288596 (53%)]\tLoss: 0.295698\n",
            "Train Epoch: 7 [153600/288596 (53%)]\tLoss: 0.382901\n",
            "Train Epoch: 7 [154880/288596 (54%)]\tLoss: 0.345953\n",
            "Train Epoch: 7 [156160/288596 (54%)]\tLoss: 0.323490\n",
            "Train Epoch: 7 [157440/288596 (55%)]\tLoss: 0.352970\n",
            "Train Epoch: 7 [158720/288596 (55%)]\tLoss: 0.427040\n",
            "Train Epoch: 7 [160000/288596 (55%)]\tLoss: 0.305473\n",
            "Train Epoch: 7 [161280/288596 (56%)]\tLoss: 0.379892\n",
            "Train Epoch: 7 [162560/288596 (56%)]\tLoss: 0.485940\n",
            "Train Epoch: 7 [163840/288596 (57%)]\tLoss: 0.290530\n",
            "Train Epoch: 7 [165120/288596 (57%)]\tLoss: 0.309534\n",
            "Train Epoch: 7 [166400/288596 (58%)]\tLoss: 0.395597\n",
            "Train Epoch: 7 [167680/288596 (58%)]\tLoss: 0.367770\n",
            "Train Epoch: 7 [168960/288596 (59%)]\tLoss: 0.314875\n",
            "Train Epoch: 7 [170240/288596 (59%)]\tLoss: 0.411865\n",
            "Train Epoch: 7 [171520/288596 (59%)]\tLoss: 0.464991\n",
            "Train Epoch: 7 [172800/288596 (60%)]\tLoss: 0.420119\n",
            "Train Epoch: 7 [174080/288596 (60%)]\tLoss: 0.357005\n",
            "Train Epoch: 7 [175360/288596 (61%)]\tLoss: 0.354331\n",
            "Train Epoch: 7 [176640/288596 (61%)]\tLoss: 0.350917\n",
            "Train Epoch: 7 [177920/288596 (62%)]\tLoss: 0.442776\n",
            "Train Epoch: 7 [179200/288596 (62%)]\tLoss: 0.343533\n",
            "Train Epoch: 7 [180480/288596 (63%)]\tLoss: 0.412299\n",
            "Train Epoch: 7 [181760/288596 (63%)]\tLoss: 0.378958\n",
            "Train Epoch: 7 [183040/288596 (63%)]\tLoss: 0.364481\n",
            "Train Epoch: 7 [184320/288596 (64%)]\tLoss: 0.378670\n",
            "Train Epoch: 7 [185600/288596 (64%)]\tLoss: 0.350472\n",
            "Train Epoch: 7 [186880/288596 (65%)]\tLoss: 0.267634\n",
            "Train Epoch: 7 [188160/288596 (65%)]\tLoss: 0.347497\n",
            "Train Epoch: 7 [189440/288596 (66%)]\tLoss: 0.380851\n",
            "Train Epoch: 7 [190720/288596 (66%)]\tLoss: 0.337478\n",
            "Train Epoch: 7 [192000/288596 (67%)]\tLoss: 0.213299\n",
            "Train Epoch: 7 [193280/288596 (67%)]\tLoss: 0.306768\n",
            "Train Epoch: 7 [194560/288596 (67%)]\tLoss: 0.385941\n",
            "Train Epoch: 7 [195840/288596 (68%)]\tLoss: 0.308052\n",
            "Train Epoch: 7 [197120/288596 (68%)]\tLoss: 0.312640\n",
            "Train Epoch: 7 [198400/288596 (69%)]\tLoss: 0.396220\n",
            "Train Epoch: 7 [199680/288596 (69%)]\tLoss: 0.344031\n",
            "Train Epoch: 7 [200960/288596 (70%)]\tLoss: 0.386566\n",
            "Train Epoch: 7 [202240/288596 (70%)]\tLoss: 0.430464\n",
            "Train Epoch: 7 [203520/288596 (71%)]\tLoss: 0.365887\n",
            "Train Epoch: 7 [204800/288596 (71%)]\tLoss: 0.337443\n",
            "Train Epoch: 7 [206080/288596 (71%)]\tLoss: 0.256628\n",
            "Train Epoch: 7 [207360/288596 (72%)]\tLoss: 0.316857\n",
            "Train Epoch: 7 [208640/288596 (72%)]\tLoss: 0.339919\n",
            "Train Epoch: 7 [209920/288596 (73%)]\tLoss: 0.461038\n",
            "Train Epoch: 7 [211200/288596 (73%)]\tLoss: 0.362807\n",
            "Train Epoch: 7 [212480/288596 (74%)]\tLoss: 0.432475\n",
            "Train Epoch: 7 [213760/288596 (74%)]\tLoss: 0.278375\n",
            "Train Epoch: 7 [215040/288596 (75%)]\tLoss: 0.332208\n",
            "Train Epoch: 7 [216320/288596 (75%)]\tLoss: 0.349052\n",
            "Train Epoch: 7 [217600/288596 (75%)]\tLoss: 0.466756\n",
            "Train Epoch: 7 [218880/288596 (76%)]\tLoss: 0.285168\n",
            "Train Epoch: 7 [220160/288596 (76%)]\tLoss: 0.418404\n",
            "Train Epoch: 7 [221440/288596 (77%)]\tLoss: 0.496747\n",
            "Train Epoch: 7 [222720/288596 (77%)]\tLoss: 0.348669\n",
            "Train Epoch: 7 [224000/288596 (78%)]\tLoss: 0.370707\n",
            "Train Epoch: 7 [225280/288596 (78%)]\tLoss: 0.396128\n",
            "Train Epoch: 7 [226560/288596 (78%)]\tLoss: 0.359958\n",
            "Train Epoch: 7 [227840/288596 (79%)]\tLoss: 0.414513\n",
            "Train Epoch: 7 [229120/288596 (79%)]\tLoss: 0.307901\n",
            "Train Epoch: 7 [230400/288596 (80%)]\tLoss: 0.370447\n",
            "Train Epoch: 7 [231680/288596 (80%)]\tLoss: 0.323481\n",
            "Train Epoch: 7 [232960/288596 (81%)]\tLoss: 0.416116\n",
            "Train Epoch: 7 [234240/288596 (81%)]\tLoss: 0.351771\n",
            "Train Epoch: 7 [235520/288596 (82%)]\tLoss: 0.385496\n",
            "Train Epoch: 7 [236800/288596 (82%)]\tLoss: 0.367801\n",
            "Train Epoch: 7 [238080/288596 (82%)]\tLoss: 0.349857\n",
            "Train Epoch: 7 [239360/288596 (83%)]\tLoss: 0.327073\n",
            "Train Epoch: 7 [240640/288596 (83%)]\tLoss: 0.314265\n",
            "Train Epoch: 7 [241920/288596 (84%)]\tLoss: 0.313432\n",
            "Train Epoch: 7 [243200/288596 (84%)]\tLoss: 0.335310\n",
            "Train Epoch: 7 [244480/288596 (85%)]\tLoss: 0.379896\n",
            "Train Epoch: 7 [245760/288596 (85%)]\tLoss: 0.331345\n",
            "Train Epoch: 7 [247040/288596 (86%)]\tLoss: 0.416508\n",
            "Train Epoch: 7 [248320/288596 (86%)]\tLoss: 0.355636\n",
            "Train Epoch: 7 [249600/288596 (86%)]\tLoss: 0.299216\n",
            "Train Epoch: 7 [250880/288596 (87%)]\tLoss: 0.257843\n",
            "Train Epoch: 7 [252160/288596 (87%)]\tLoss: 0.435071\n",
            "Train Epoch: 7 [253440/288596 (88%)]\tLoss: 0.272925\n",
            "Train Epoch: 7 [254720/288596 (88%)]\tLoss: 0.336418\n",
            "Train Epoch: 7 [256000/288596 (89%)]\tLoss: 0.379878\n",
            "Train Epoch: 7 [257280/288596 (89%)]\tLoss: 0.320631\n",
            "Train Epoch: 7 [258560/288596 (90%)]\tLoss: 0.394619\n",
            "Train Epoch: 7 [259840/288596 (90%)]\tLoss: 0.336891\n",
            "Train Epoch: 7 [261120/288596 (90%)]\tLoss: 0.356237\n",
            "Train Epoch: 7 [262400/288596 (91%)]\tLoss: 0.340616\n",
            "Train Epoch: 7 [263680/288596 (91%)]\tLoss: 0.314648\n",
            "Train Epoch: 7 [264960/288596 (92%)]\tLoss: 0.358316\n",
            "Train Epoch: 7 [266240/288596 (92%)]\tLoss: 0.311260\n",
            "Train Epoch: 7 [267520/288596 (93%)]\tLoss: 0.375897\n",
            "Train Epoch: 7 [268800/288596 (93%)]\tLoss: 0.371690\n",
            "Train Epoch: 7 [270080/288596 (94%)]\tLoss: 0.334607\n",
            "Train Epoch: 7 [271360/288596 (94%)]\tLoss: 0.387351\n",
            "Train Epoch: 7 [272640/288596 (94%)]\tLoss: 0.388785\n",
            "Train Epoch: 7 [273920/288596 (95%)]\tLoss: 0.388786\n",
            "Train Epoch: 7 [275200/288596 (95%)]\tLoss: 0.354396\n",
            "Train Epoch: 7 [276480/288596 (96%)]\tLoss: 0.306332\n",
            "Train Epoch: 7 [277760/288596 (96%)]\tLoss: 0.373318\n",
            "Train Epoch: 7 [279040/288596 (97%)]\tLoss: 0.401531\n",
            "Train Epoch: 7 [280320/288596 (97%)]\tLoss: 0.375186\n",
            "Train Epoch: 7 [281600/288596 (98%)]\tLoss: 0.289325\n",
            "Train Epoch: 7 [282880/288596 (98%)]\tLoss: 0.345529\n",
            "Train Epoch: 7 [284160/288596 (98%)]\tLoss: 0.383398\n",
            "Train Epoch: 7 [285440/288596 (99%)]\tLoss: 0.394534\n",
            "Train Epoch: 7 [286720/288596 (99%)]\tLoss: 0.324896\n",
            "Train Epoch: 7 [288000/288596 (100%)]\tLoss: 0.323727\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 107423/123684 (87%)\n",
            "\n",
            "Train Epoch: 8 [0/288596 (0%)]\tLoss: 0.312808\n",
            "Train Epoch: 8 [1280/288596 (0%)]\tLoss: 0.304357\n",
            "Train Epoch: 8 [2560/288596 (1%)]\tLoss: 0.384817\n",
            "Train Epoch: 8 [3840/288596 (1%)]\tLoss: 0.415125\n",
            "Train Epoch: 8 [5120/288596 (2%)]\tLoss: 0.420436\n",
            "Train Epoch: 8 [6400/288596 (2%)]\tLoss: 0.382229\n",
            "Train Epoch: 8 [7680/288596 (3%)]\tLoss: 0.245422\n",
            "Train Epoch: 8 [8960/288596 (3%)]\tLoss: 0.335185\n",
            "Train Epoch: 8 [10240/288596 (4%)]\tLoss: 0.329387\n",
            "Train Epoch: 8 [11520/288596 (4%)]\tLoss: 0.248715\n",
            "Train Epoch: 8 [12800/288596 (4%)]\tLoss: 0.388058\n",
            "Train Epoch: 8 [14080/288596 (5%)]\tLoss: 0.450328\n",
            "Train Epoch: 8 [15360/288596 (5%)]\tLoss: 0.339356\n",
            "Train Epoch: 8 [16640/288596 (6%)]\tLoss: 0.396487\n",
            "Train Epoch: 8 [17920/288596 (6%)]\tLoss: 0.312069\n",
            "Train Epoch: 8 [19200/288596 (7%)]\tLoss: 0.292467\n",
            "Train Epoch: 8 [20480/288596 (7%)]\tLoss: 0.308491\n",
            "Train Epoch: 8 [21760/288596 (8%)]\tLoss: 0.443734\n",
            "Train Epoch: 8 [23040/288596 (8%)]\tLoss: 0.285503\n",
            "Train Epoch: 8 [24320/288596 (8%)]\tLoss: 0.311140\n",
            "Train Epoch: 8 [25600/288596 (9%)]\tLoss: 0.328360\n",
            "Train Epoch: 8 [26880/288596 (9%)]\tLoss: 0.353248\n",
            "Train Epoch: 8 [28160/288596 (10%)]\tLoss: 0.351415\n",
            "Train Epoch: 8 [29440/288596 (10%)]\tLoss: 0.344686\n",
            "Train Epoch: 8 [30720/288596 (11%)]\tLoss: 0.392444\n",
            "Train Epoch: 8 [32000/288596 (11%)]\tLoss: 0.316418\n",
            "Train Epoch: 8 [33280/288596 (12%)]\tLoss: 0.327990\n",
            "Train Epoch: 8 [34560/288596 (12%)]\tLoss: 0.390175\n",
            "Train Epoch: 8 [35840/288596 (12%)]\tLoss: 0.322796\n",
            "Train Epoch: 8 [37120/288596 (13%)]\tLoss: 0.343484\n",
            "Train Epoch: 8 [38400/288596 (13%)]\tLoss: 0.402169\n",
            "Train Epoch: 8 [39680/288596 (14%)]\tLoss: 0.274942\n",
            "Train Epoch: 8 [40960/288596 (14%)]\tLoss: 0.269539\n",
            "Train Epoch: 8 [42240/288596 (15%)]\tLoss: 0.300808\n",
            "Train Epoch: 8 [43520/288596 (15%)]\tLoss: 0.301604\n",
            "Train Epoch: 8 [44800/288596 (16%)]\tLoss: 0.452253\n",
            "Train Epoch: 8 [46080/288596 (16%)]\tLoss: 0.343574\n",
            "Train Epoch: 8 [47360/288596 (16%)]\tLoss: 0.392929\n",
            "Train Epoch: 8 [48640/288596 (17%)]\tLoss: 0.324834\n",
            "Train Epoch: 8 [49920/288596 (17%)]\tLoss: 0.372552\n",
            "Train Epoch: 8 [51200/288596 (18%)]\tLoss: 0.308429\n",
            "Train Epoch: 8 [52480/288596 (18%)]\tLoss: 0.291212\n",
            "Train Epoch: 8 [53760/288596 (19%)]\tLoss: 0.374816\n",
            "Train Epoch: 8 [55040/288596 (19%)]\tLoss: 0.346897\n",
            "Train Epoch: 8 [56320/288596 (20%)]\tLoss: 0.364097\n",
            "Train Epoch: 8 [57600/288596 (20%)]\tLoss: 0.372829\n",
            "Train Epoch: 8 [58880/288596 (20%)]\tLoss: 0.353661\n",
            "Train Epoch: 8 [60160/288596 (21%)]\tLoss: 0.299751\n",
            "Train Epoch: 8 [61440/288596 (21%)]\tLoss: 0.364906\n",
            "Train Epoch: 8 [62720/288596 (22%)]\tLoss: 0.304866\n",
            "Train Epoch: 8 [64000/288596 (22%)]\tLoss: 0.371164\n",
            "Train Epoch: 8 [65280/288596 (23%)]\tLoss: 0.504083\n",
            "Train Epoch: 8 [66560/288596 (23%)]\tLoss: 0.324307\n",
            "Train Epoch: 8 [67840/288596 (24%)]\tLoss: 0.272685\n",
            "Train Epoch: 8 [69120/288596 (24%)]\tLoss: 0.303648\n",
            "Train Epoch: 8 [70400/288596 (24%)]\tLoss: 0.304854\n",
            "Train Epoch: 8 [71680/288596 (25%)]\tLoss: 0.387142\n",
            "Train Epoch: 8 [72960/288596 (25%)]\tLoss: 0.353153\n",
            "Train Epoch: 8 [74240/288596 (26%)]\tLoss: 0.234391\n",
            "Train Epoch: 8 [75520/288596 (26%)]\tLoss: 0.362558\n",
            "Train Epoch: 8 [76800/288596 (27%)]\tLoss: 0.306898\n",
            "Train Epoch: 8 [78080/288596 (27%)]\tLoss: 0.349524\n",
            "Train Epoch: 8 [79360/288596 (27%)]\tLoss: 0.472518\n",
            "Train Epoch: 8 [80640/288596 (28%)]\tLoss: 0.232982\n",
            "Train Epoch: 8 [81920/288596 (28%)]\tLoss: 0.364864\n",
            "Train Epoch: 8 [83200/288596 (29%)]\tLoss: 0.460402\n",
            "Train Epoch: 8 [84480/288596 (29%)]\tLoss: 0.313412\n",
            "Train Epoch: 8 [85760/288596 (30%)]\tLoss: 0.410076\n",
            "Train Epoch: 8 [87040/288596 (30%)]\tLoss: 0.348689\n",
            "Train Epoch: 8 [88320/288596 (31%)]\tLoss: 0.366235\n",
            "Train Epoch: 8 [89600/288596 (31%)]\tLoss: 0.365627\n",
            "Train Epoch: 8 [90880/288596 (31%)]\tLoss: 0.338731\n",
            "Train Epoch: 8 [92160/288596 (32%)]\tLoss: 0.442562\n",
            "Train Epoch: 8 [93440/288596 (32%)]\tLoss: 0.372951\n",
            "Train Epoch: 8 [94720/288596 (33%)]\tLoss: 0.382134\n",
            "Train Epoch: 8 [96000/288596 (33%)]\tLoss: 0.373630\n",
            "Train Epoch: 8 [97280/288596 (34%)]\tLoss: 0.345775\n",
            "Train Epoch: 8 [98560/288596 (34%)]\tLoss: 0.295085\n",
            "Train Epoch: 8 [99840/288596 (35%)]\tLoss: 0.351546\n",
            "Train Epoch: 8 [101120/288596 (35%)]\tLoss: 0.339606\n",
            "Train Epoch: 8 [102400/288596 (35%)]\tLoss: 0.348502\n",
            "Train Epoch: 8 [103680/288596 (36%)]\tLoss: 0.394230\n",
            "Train Epoch: 8 [104960/288596 (36%)]\tLoss: 0.359416\n",
            "Train Epoch: 8 [106240/288596 (37%)]\tLoss: 0.267377\n",
            "Train Epoch: 8 [107520/288596 (37%)]\tLoss: 0.269094\n",
            "Train Epoch: 8 [108800/288596 (38%)]\tLoss: 0.340985\n",
            "Train Epoch: 8 [110080/288596 (38%)]\tLoss: 0.458453\n",
            "Train Epoch: 8 [111360/288596 (39%)]\tLoss: 0.336123\n",
            "Train Epoch: 8 [112640/288596 (39%)]\tLoss: 0.388576\n",
            "Train Epoch: 8 [113920/288596 (39%)]\tLoss: 0.310714\n",
            "Train Epoch: 8 [115200/288596 (40%)]\tLoss: 0.346946\n",
            "Train Epoch: 8 [116480/288596 (40%)]\tLoss: 0.449422\n",
            "Train Epoch: 8 [117760/288596 (41%)]\tLoss: 0.338484\n",
            "Train Epoch: 8 [119040/288596 (41%)]\tLoss: 0.368280\n",
            "Train Epoch: 8 [120320/288596 (42%)]\tLoss: 0.293126\n",
            "Train Epoch: 8 [121600/288596 (42%)]\tLoss: 0.391009\n",
            "Train Epoch: 8 [122880/288596 (43%)]\tLoss: 0.346340\n",
            "Train Epoch: 8 [124160/288596 (43%)]\tLoss: 0.335976\n",
            "Train Epoch: 8 [125440/288596 (43%)]\tLoss: 0.372012\n",
            "Train Epoch: 8 [126720/288596 (44%)]\tLoss: 0.329917\n",
            "Train Epoch: 8 [128000/288596 (44%)]\tLoss: 0.330926\n",
            "Train Epoch: 8 [129280/288596 (45%)]\tLoss: 0.345742\n",
            "Train Epoch: 8 [130560/288596 (45%)]\tLoss: 0.400879\n",
            "Train Epoch: 8 [131840/288596 (46%)]\tLoss: 0.406632\n",
            "Train Epoch: 8 [133120/288596 (46%)]\tLoss: 0.380688\n",
            "Train Epoch: 8 [134400/288596 (47%)]\tLoss: 0.333566\n",
            "Train Epoch: 8 [135680/288596 (47%)]\tLoss: 0.358167\n",
            "Train Epoch: 8 [136960/288596 (47%)]\tLoss: 0.288151\n",
            "Train Epoch: 8 [138240/288596 (48%)]\tLoss: 0.448149\n",
            "Train Epoch: 8 [139520/288596 (48%)]\tLoss: 0.355293\n",
            "Train Epoch: 8 [140800/288596 (49%)]\tLoss: 0.331983\n",
            "Train Epoch: 8 [142080/288596 (49%)]\tLoss: 0.415300\n",
            "Train Epoch: 8 [143360/288596 (50%)]\tLoss: 0.371351\n",
            "Train Epoch: 8 [144640/288596 (50%)]\tLoss: 0.282369\n",
            "Train Epoch: 8 [145920/288596 (51%)]\tLoss: 0.347848\n",
            "Train Epoch: 8 [147200/288596 (51%)]\tLoss: 0.334126\n",
            "Train Epoch: 8 [148480/288596 (51%)]\tLoss: 0.478077\n",
            "Train Epoch: 8 [149760/288596 (52%)]\tLoss: 0.387012\n",
            "Train Epoch: 8 [151040/288596 (52%)]\tLoss: 0.354722\n",
            "Train Epoch: 8 [152320/288596 (53%)]\tLoss: 0.323799\n",
            "Train Epoch: 8 [153600/288596 (53%)]\tLoss: 0.414266\n",
            "Train Epoch: 8 [154880/288596 (54%)]\tLoss: 0.335321\n",
            "Train Epoch: 8 [156160/288596 (54%)]\tLoss: 0.330588\n",
            "Train Epoch: 8 [157440/288596 (55%)]\tLoss: 0.365614\n",
            "Train Epoch: 8 [158720/288596 (55%)]\tLoss: 0.290433\n",
            "Train Epoch: 8 [160000/288596 (55%)]\tLoss: 0.387853\n",
            "Train Epoch: 8 [161280/288596 (56%)]\tLoss: 0.360787\n",
            "Train Epoch: 8 [162560/288596 (56%)]\tLoss: 0.363359\n",
            "Train Epoch: 8 [163840/288596 (57%)]\tLoss: 0.285481\n",
            "Train Epoch: 8 [165120/288596 (57%)]\tLoss: 0.270356\n",
            "Train Epoch: 8 [166400/288596 (58%)]\tLoss: 0.382439\n",
            "Train Epoch: 8 [167680/288596 (58%)]\tLoss: 0.269123\n",
            "Train Epoch: 8 [168960/288596 (59%)]\tLoss: 0.352487\n",
            "Train Epoch: 8 [170240/288596 (59%)]\tLoss: 0.336898\n",
            "Train Epoch: 8 [171520/288596 (59%)]\tLoss: 0.343199\n",
            "Train Epoch: 8 [172800/288596 (60%)]\tLoss: 0.290542\n",
            "Train Epoch: 8 [174080/288596 (60%)]\tLoss: 0.312336\n",
            "Train Epoch: 8 [175360/288596 (61%)]\tLoss: 0.362272\n",
            "Train Epoch: 8 [176640/288596 (61%)]\tLoss: 0.326262\n",
            "Train Epoch: 8 [177920/288596 (62%)]\tLoss: 0.396120\n",
            "Train Epoch: 8 [179200/288596 (62%)]\tLoss: 0.369414\n",
            "Train Epoch: 8 [180480/288596 (63%)]\tLoss: 0.326280\n",
            "Train Epoch: 8 [181760/288596 (63%)]\tLoss: 0.344255\n",
            "Train Epoch: 8 [183040/288596 (63%)]\tLoss: 0.280883\n",
            "Train Epoch: 8 [184320/288596 (64%)]\tLoss: 0.336458\n",
            "Train Epoch: 8 [185600/288596 (64%)]\tLoss: 0.288622\n",
            "Train Epoch: 8 [186880/288596 (65%)]\tLoss: 0.403479\n",
            "Train Epoch: 8 [188160/288596 (65%)]\tLoss: 0.389453\n",
            "Train Epoch: 8 [189440/288596 (66%)]\tLoss: 0.300375\n",
            "Train Epoch: 8 [190720/288596 (66%)]\tLoss: 0.241954\n",
            "Train Epoch: 8 [192000/288596 (67%)]\tLoss: 0.416292\n",
            "Train Epoch: 8 [193280/288596 (67%)]\tLoss: 0.324143\n",
            "Train Epoch: 8 [194560/288596 (67%)]\tLoss: 0.401670\n",
            "Train Epoch: 8 [195840/288596 (68%)]\tLoss: 0.283319\n",
            "Train Epoch: 8 [197120/288596 (68%)]\tLoss: 0.274179\n",
            "Train Epoch: 8 [198400/288596 (69%)]\tLoss: 0.302010\n",
            "Train Epoch: 8 [199680/288596 (69%)]\tLoss: 0.286195\n",
            "Train Epoch: 8 [200960/288596 (70%)]\tLoss: 0.340628\n",
            "Train Epoch: 8 [202240/288596 (70%)]\tLoss: 0.440822\n",
            "Train Epoch: 8 [203520/288596 (71%)]\tLoss: 0.433918\n",
            "Train Epoch: 8 [204800/288596 (71%)]\tLoss: 0.397907\n",
            "Train Epoch: 8 [206080/288596 (71%)]\tLoss: 0.356961\n",
            "Train Epoch: 8 [207360/288596 (72%)]\tLoss: 0.234937\n",
            "Train Epoch: 8 [208640/288596 (72%)]\tLoss: 0.351177\n",
            "Train Epoch: 8 [209920/288596 (73%)]\tLoss: 0.437390\n",
            "Train Epoch: 8 [211200/288596 (73%)]\tLoss: 0.386809\n",
            "Train Epoch: 8 [212480/288596 (74%)]\tLoss: 0.267911\n",
            "Train Epoch: 8 [213760/288596 (74%)]\tLoss: 0.354056\n",
            "Train Epoch: 8 [215040/288596 (75%)]\tLoss: 0.239975\n",
            "Train Epoch: 8 [216320/288596 (75%)]\tLoss: 0.372224\n",
            "Train Epoch: 8 [217600/288596 (75%)]\tLoss: 0.310663\n",
            "Train Epoch: 8 [218880/288596 (76%)]\tLoss: 0.357832\n",
            "Train Epoch: 8 [220160/288596 (76%)]\tLoss: 0.419892\n",
            "Train Epoch: 8 [221440/288596 (77%)]\tLoss: 0.314909\n",
            "Train Epoch: 8 [222720/288596 (77%)]\tLoss: 0.329681\n",
            "Train Epoch: 8 [224000/288596 (78%)]\tLoss: 0.377896\n",
            "Train Epoch: 8 [225280/288596 (78%)]\tLoss: 0.455656\n",
            "Train Epoch: 8 [226560/288596 (78%)]\tLoss: 0.452593\n",
            "Train Epoch: 8 [227840/288596 (79%)]\tLoss: 0.302007\n",
            "Train Epoch: 8 [229120/288596 (79%)]\tLoss: 0.442345\n",
            "Train Epoch: 8 [230400/288596 (80%)]\tLoss: 0.372797\n",
            "Train Epoch: 8 [231680/288596 (80%)]\tLoss: 0.435432\n",
            "Train Epoch: 8 [232960/288596 (81%)]\tLoss: 0.301403\n",
            "Train Epoch: 8 [234240/288596 (81%)]\tLoss: 0.258139\n",
            "Train Epoch: 8 [235520/288596 (82%)]\tLoss: 0.292851\n",
            "Train Epoch: 8 [236800/288596 (82%)]\tLoss: 0.308153\n",
            "Train Epoch: 8 [238080/288596 (82%)]\tLoss: 0.307278\n",
            "Train Epoch: 8 [239360/288596 (83%)]\tLoss: 0.372609\n",
            "Train Epoch: 8 [240640/288596 (83%)]\tLoss: 0.429052\n",
            "Train Epoch: 8 [241920/288596 (84%)]\tLoss: 0.295240\n",
            "Train Epoch: 8 [243200/288596 (84%)]\tLoss: 0.362053\n",
            "Train Epoch: 8 [244480/288596 (85%)]\tLoss: 0.199611\n",
            "Train Epoch: 8 [245760/288596 (85%)]\tLoss: 0.343130\n",
            "Train Epoch: 8 [247040/288596 (86%)]\tLoss: 0.475144\n",
            "Train Epoch: 8 [248320/288596 (86%)]\tLoss: 0.313110\n",
            "Train Epoch: 8 [249600/288596 (86%)]\tLoss: 0.469750\n",
            "Train Epoch: 8 [250880/288596 (87%)]\tLoss: 0.342856\n",
            "Train Epoch: 8 [252160/288596 (87%)]\tLoss: 0.401733\n",
            "Train Epoch: 8 [253440/288596 (88%)]\tLoss: 0.423617\n",
            "Train Epoch: 8 [254720/288596 (88%)]\tLoss: 0.333472\n",
            "Train Epoch: 8 [256000/288596 (89%)]\tLoss: 0.280723\n",
            "Train Epoch: 8 [257280/288596 (89%)]\tLoss: 0.304314\n",
            "Train Epoch: 8 [258560/288596 (90%)]\tLoss: 0.414156\n",
            "Train Epoch: 8 [259840/288596 (90%)]\tLoss: 0.309857\n",
            "Train Epoch: 8 [261120/288596 (90%)]\tLoss: 0.343367\n",
            "Train Epoch: 8 [262400/288596 (91%)]\tLoss: 0.357672\n",
            "Train Epoch: 8 [263680/288596 (91%)]\tLoss: 0.371987\n",
            "Train Epoch: 8 [264960/288596 (92%)]\tLoss: 0.395792\n",
            "Train Epoch: 8 [266240/288596 (92%)]\tLoss: 0.375157\n",
            "Train Epoch: 8 [267520/288596 (93%)]\tLoss: 0.319531\n",
            "Train Epoch: 8 [268800/288596 (93%)]\tLoss: 0.487823\n",
            "Train Epoch: 8 [270080/288596 (94%)]\tLoss: 0.437063\n",
            "Train Epoch: 8 [271360/288596 (94%)]\tLoss: 0.299002\n",
            "Train Epoch: 8 [272640/288596 (94%)]\tLoss: 0.336468\n",
            "Train Epoch: 8 [273920/288596 (95%)]\tLoss: 0.380474\n",
            "Train Epoch: 8 [275200/288596 (95%)]\tLoss: 0.312825\n",
            "Train Epoch: 8 [276480/288596 (96%)]\tLoss: 0.361679\n",
            "Train Epoch: 8 [277760/288596 (96%)]\tLoss: 0.388450\n",
            "Train Epoch: 8 [279040/288596 (97%)]\tLoss: 0.332636\n",
            "Train Epoch: 8 [280320/288596 (97%)]\tLoss: 0.375818\n",
            "Train Epoch: 8 [281600/288596 (98%)]\tLoss: 0.505672\n",
            "Train Epoch: 8 [282880/288596 (98%)]\tLoss: 0.395345\n",
            "Train Epoch: 8 [284160/288596 (98%)]\tLoss: 0.386220\n",
            "Train Epoch: 8 [285440/288596 (99%)]\tLoss: 0.273571\n",
            "Train Epoch: 8 [286720/288596 (99%)]\tLoss: 0.324224\n",
            "Train Epoch: 8 [288000/288596 (100%)]\tLoss: 0.401139\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 107456/123684 (87%)\n",
            "\n",
            "Train Epoch: 9 [0/288596 (0%)]\tLoss: 0.376212\n",
            "Train Epoch: 9 [1280/288596 (0%)]\tLoss: 0.280943\n",
            "Train Epoch: 9 [2560/288596 (1%)]\tLoss: 0.376885\n",
            "Train Epoch: 9 [3840/288596 (1%)]\tLoss: 0.426909\n",
            "Train Epoch: 9 [5120/288596 (2%)]\tLoss: 0.288981\n",
            "Train Epoch: 9 [6400/288596 (2%)]\tLoss: 0.355227\n",
            "Train Epoch: 9 [7680/288596 (3%)]\tLoss: 0.326909\n",
            "Train Epoch: 9 [8960/288596 (3%)]\tLoss: 0.397984\n",
            "Train Epoch: 9 [10240/288596 (4%)]\tLoss: 0.390742\n",
            "Train Epoch: 9 [11520/288596 (4%)]\tLoss: 0.308186\n",
            "Train Epoch: 9 [12800/288596 (4%)]\tLoss: 0.321384\n",
            "Train Epoch: 9 [14080/288596 (5%)]\tLoss: 0.406622\n",
            "Train Epoch: 9 [15360/288596 (5%)]\tLoss: 0.401536\n",
            "Train Epoch: 9 [16640/288596 (6%)]\tLoss: 0.355444\n",
            "Train Epoch: 9 [17920/288596 (6%)]\tLoss: 0.344682\n",
            "Train Epoch: 9 [19200/288596 (7%)]\tLoss: 0.297904\n",
            "Train Epoch: 9 [20480/288596 (7%)]\tLoss: 0.450525\n",
            "Train Epoch: 9 [21760/288596 (8%)]\tLoss: 0.290693\n",
            "Train Epoch: 9 [23040/288596 (8%)]\tLoss: 0.364410\n",
            "Train Epoch: 9 [24320/288596 (8%)]\tLoss: 0.391667\n",
            "Train Epoch: 9 [25600/288596 (9%)]\tLoss: 0.382745\n",
            "Train Epoch: 9 [26880/288596 (9%)]\tLoss: 0.452791\n",
            "Train Epoch: 9 [28160/288596 (10%)]\tLoss: 0.444828\n",
            "Train Epoch: 9 [29440/288596 (10%)]\tLoss: 0.270165\n",
            "Train Epoch: 9 [30720/288596 (11%)]\tLoss: 0.298594\n",
            "Train Epoch: 9 [32000/288596 (11%)]\tLoss: 0.407155\n",
            "Train Epoch: 9 [33280/288596 (12%)]\tLoss: 0.272604\n",
            "Train Epoch: 9 [34560/288596 (12%)]\tLoss: 0.308401\n",
            "Train Epoch: 9 [35840/288596 (12%)]\tLoss: 0.312781\n",
            "Train Epoch: 9 [37120/288596 (13%)]\tLoss: 0.386450\n",
            "Train Epoch: 9 [38400/288596 (13%)]\tLoss: 0.357108\n",
            "Train Epoch: 9 [39680/288596 (14%)]\tLoss: 0.447168\n",
            "Train Epoch: 9 [40960/288596 (14%)]\tLoss: 0.332791\n",
            "Train Epoch: 9 [42240/288596 (15%)]\tLoss: 0.253875\n",
            "Train Epoch: 9 [43520/288596 (15%)]\tLoss: 0.506901\n",
            "Train Epoch: 9 [44800/288596 (16%)]\tLoss: 0.277879\n",
            "Train Epoch: 9 [46080/288596 (16%)]\tLoss: 0.299825\n",
            "Train Epoch: 9 [47360/288596 (16%)]\tLoss: 0.349216\n",
            "Train Epoch: 9 [48640/288596 (17%)]\tLoss: 0.263285\n",
            "Train Epoch: 9 [49920/288596 (17%)]\tLoss: 0.325428\n",
            "Train Epoch: 9 [51200/288596 (18%)]\tLoss: 0.295511\n",
            "Train Epoch: 9 [52480/288596 (18%)]\tLoss: 0.349492\n",
            "Train Epoch: 9 [53760/288596 (19%)]\tLoss: 0.392326\n",
            "Train Epoch: 9 [55040/288596 (19%)]\tLoss: 0.318856\n",
            "Train Epoch: 9 [56320/288596 (20%)]\tLoss: 0.380354\n",
            "Train Epoch: 9 [57600/288596 (20%)]\tLoss: 0.274013\n",
            "Train Epoch: 9 [58880/288596 (20%)]\tLoss: 0.341029\n",
            "Train Epoch: 9 [60160/288596 (21%)]\tLoss: 0.509935\n",
            "Train Epoch: 9 [61440/288596 (21%)]\tLoss: 0.305653\n",
            "Train Epoch: 9 [62720/288596 (22%)]\tLoss: 0.383900\n",
            "Train Epoch: 9 [64000/288596 (22%)]\tLoss: 0.365109\n",
            "Train Epoch: 9 [65280/288596 (23%)]\tLoss: 0.284970\n",
            "Train Epoch: 9 [66560/288596 (23%)]\tLoss: 0.329848\n",
            "Train Epoch: 9 [67840/288596 (24%)]\tLoss: 0.278852\n",
            "Train Epoch: 9 [69120/288596 (24%)]\tLoss: 0.319846\n",
            "Train Epoch: 9 [70400/288596 (24%)]\tLoss: 0.415052\n",
            "Train Epoch: 9 [71680/288596 (25%)]\tLoss: 0.357370\n",
            "Train Epoch: 9 [72960/288596 (25%)]\tLoss: 0.406102\n",
            "Train Epoch: 9 [74240/288596 (26%)]\tLoss: 0.389883\n",
            "Train Epoch: 9 [75520/288596 (26%)]\tLoss: 0.285660\n",
            "Train Epoch: 9 [76800/288596 (27%)]\tLoss: 0.300765\n",
            "Train Epoch: 9 [78080/288596 (27%)]\tLoss: 0.361708\n",
            "Train Epoch: 9 [79360/288596 (27%)]\tLoss: 0.327297\n",
            "Train Epoch: 9 [80640/288596 (28%)]\tLoss: 0.363785\n",
            "Train Epoch: 9 [81920/288596 (28%)]\tLoss: 0.405983\n",
            "Train Epoch: 9 [83200/288596 (29%)]\tLoss: 0.262774\n",
            "Train Epoch: 9 [84480/288596 (29%)]\tLoss: 0.317601\n",
            "Train Epoch: 9 [85760/288596 (30%)]\tLoss: 0.380353\n",
            "Train Epoch: 9 [87040/288596 (30%)]\tLoss: 0.424888\n",
            "Train Epoch: 9 [88320/288596 (31%)]\tLoss: 0.339532\n",
            "Train Epoch: 9 [89600/288596 (31%)]\tLoss: 0.337430\n",
            "Train Epoch: 9 [90880/288596 (31%)]\tLoss: 0.376096\n",
            "Train Epoch: 9 [92160/288596 (32%)]\tLoss: 0.376981\n",
            "Train Epoch: 9 [93440/288596 (32%)]\tLoss: 0.349424\n",
            "Train Epoch: 9 [94720/288596 (33%)]\tLoss: 0.353260\n",
            "Train Epoch: 9 [96000/288596 (33%)]\tLoss: 0.291661\n",
            "Train Epoch: 9 [97280/288596 (34%)]\tLoss: 0.388926\n",
            "Train Epoch: 9 [98560/288596 (34%)]\tLoss: 0.428901\n",
            "Train Epoch: 9 [99840/288596 (35%)]\tLoss: 0.374417\n",
            "Train Epoch: 9 [101120/288596 (35%)]\tLoss: 0.282443\n",
            "Train Epoch: 9 [102400/288596 (35%)]\tLoss: 0.367786\n",
            "Train Epoch: 9 [103680/288596 (36%)]\tLoss: 0.413511\n",
            "Train Epoch: 9 [104960/288596 (36%)]\tLoss: 0.361000\n",
            "Train Epoch: 9 [106240/288596 (37%)]\tLoss: 0.298838\n",
            "Train Epoch: 9 [107520/288596 (37%)]\tLoss: 0.375985\n",
            "Train Epoch: 9 [108800/288596 (38%)]\tLoss: 0.310764\n",
            "Train Epoch: 9 [110080/288596 (38%)]\tLoss: 0.283738\n",
            "Train Epoch: 9 [111360/288596 (39%)]\tLoss: 0.337357\n",
            "Train Epoch: 9 [112640/288596 (39%)]\tLoss: 0.415033\n",
            "Train Epoch: 9 [113920/288596 (39%)]\tLoss: 0.359474\n",
            "Train Epoch: 9 [115200/288596 (40%)]\tLoss: 0.489984\n",
            "Train Epoch: 9 [116480/288596 (40%)]\tLoss: 0.271417\n",
            "Train Epoch: 9 [117760/288596 (41%)]\tLoss: 0.366153\n",
            "Train Epoch: 9 [119040/288596 (41%)]\tLoss: 0.405472\n",
            "Train Epoch: 9 [120320/288596 (42%)]\tLoss: 0.292766\n",
            "Train Epoch: 9 [121600/288596 (42%)]\tLoss: 0.357072\n",
            "Train Epoch: 9 [122880/288596 (43%)]\tLoss: 0.301123\n",
            "Train Epoch: 9 [124160/288596 (43%)]\tLoss: 0.315467\n",
            "Train Epoch: 9 [125440/288596 (43%)]\tLoss: 0.471333\n",
            "Train Epoch: 9 [126720/288596 (44%)]\tLoss: 0.303535\n",
            "Train Epoch: 9 [128000/288596 (44%)]\tLoss: 0.386246\n",
            "Train Epoch: 9 [129280/288596 (45%)]\tLoss: 0.399921\n",
            "Train Epoch: 9 [130560/288596 (45%)]\tLoss: 0.319202\n",
            "Train Epoch: 9 [131840/288596 (46%)]\tLoss: 0.267377\n",
            "Train Epoch: 9 [133120/288596 (46%)]\tLoss: 0.257096\n",
            "Train Epoch: 9 [134400/288596 (47%)]\tLoss: 0.379299\n",
            "Train Epoch: 9 [135680/288596 (47%)]\tLoss: 0.347514\n",
            "Train Epoch: 9 [136960/288596 (47%)]\tLoss: 0.365929\n",
            "Train Epoch: 9 [138240/288596 (48%)]\tLoss: 0.367330\n",
            "Train Epoch: 9 [139520/288596 (48%)]\tLoss: 0.434743\n",
            "Train Epoch: 9 [140800/288596 (49%)]\tLoss: 0.435609\n",
            "Train Epoch: 9 [142080/288596 (49%)]\tLoss: 0.438242\n",
            "Train Epoch: 9 [143360/288596 (50%)]\tLoss: 0.356704\n",
            "Train Epoch: 9 [144640/288596 (50%)]\tLoss: 0.317218\n",
            "Train Epoch: 9 [145920/288596 (51%)]\tLoss: 0.379350\n",
            "Train Epoch: 9 [147200/288596 (51%)]\tLoss: 0.307324\n",
            "Train Epoch: 9 [148480/288596 (51%)]\tLoss: 0.488934\n",
            "Train Epoch: 9 [149760/288596 (52%)]\tLoss: 0.311190\n",
            "Train Epoch: 9 [151040/288596 (52%)]\tLoss: 0.298616\n",
            "Train Epoch: 9 [152320/288596 (53%)]\tLoss: 0.376102\n",
            "Train Epoch: 9 [153600/288596 (53%)]\tLoss: 0.403814\n",
            "Train Epoch: 9 [154880/288596 (54%)]\tLoss: 0.307664\n",
            "Train Epoch: 9 [156160/288596 (54%)]\tLoss: 0.273304\n",
            "Train Epoch: 9 [157440/288596 (55%)]\tLoss: 0.340071\n",
            "Train Epoch: 9 [158720/288596 (55%)]\tLoss: 0.354130\n",
            "Train Epoch: 9 [160000/288596 (55%)]\tLoss: 0.337649\n",
            "Train Epoch: 9 [161280/288596 (56%)]\tLoss: 0.264097\n",
            "Train Epoch: 9 [162560/288596 (56%)]\tLoss: 0.465030\n",
            "Train Epoch: 9 [163840/288596 (57%)]\tLoss: 0.330953\n",
            "Train Epoch: 9 [165120/288596 (57%)]\tLoss: 0.298092\n",
            "Train Epoch: 9 [166400/288596 (58%)]\tLoss: 0.318905\n",
            "Train Epoch: 9 [167680/288596 (58%)]\tLoss: 0.263710\n",
            "Train Epoch: 9 [168960/288596 (59%)]\tLoss: 0.391314\n",
            "Train Epoch: 9 [170240/288596 (59%)]\tLoss: 0.504768\n",
            "Train Epoch: 9 [171520/288596 (59%)]\tLoss: 0.294826\n",
            "Train Epoch: 9 [172800/288596 (60%)]\tLoss: 0.345422\n",
            "Train Epoch: 9 [174080/288596 (60%)]\tLoss: 0.387833\n",
            "Train Epoch: 9 [175360/288596 (61%)]\tLoss: 0.264139\n",
            "Train Epoch: 9 [176640/288596 (61%)]\tLoss: 0.376180\n",
            "Train Epoch: 9 [177920/288596 (62%)]\tLoss: 0.224047\n",
            "Train Epoch: 9 [179200/288596 (62%)]\tLoss: 0.527663\n",
            "Train Epoch: 9 [180480/288596 (63%)]\tLoss: 0.500971\n",
            "Train Epoch: 9 [181760/288596 (63%)]\tLoss: 0.267070\n",
            "Train Epoch: 9 [183040/288596 (63%)]\tLoss: 0.334150\n",
            "Train Epoch: 9 [184320/288596 (64%)]\tLoss: 0.379286\n",
            "Train Epoch: 9 [185600/288596 (64%)]\tLoss: 0.284245\n",
            "Train Epoch: 9 [186880/288596 (65%)]\tLoss: 0.367998\n",
            "Train Epoch: 9 [188160/288596 (65%)]\tLoss: 0.266349\n",
            "Train Epoch: 9 [189440/288596 (66%)]\tLoss: 0.287042\n",
            "Train Epoch: 9 [190720/288596 (66%)]\tLoss: 0.390568\n",
            "Train Epoch: 9 [192000/288596 (67%)]\tLoss: 0.398987\n",
            "Train Epoch: 9 [193280/288596 (67%)]\tLoss: 0.343853\n",
            "Train Epoch: 9 [194560/288596 (67%)]\tLoss: 0.426772\n",
            "Train Epoch: 9 [195840/288596 (68%)]\tLoss: 0.446764\n",
            "Train Epoch: 9 [197120/288596 (68%)]\tLoss: 0.385295\n",
            "Train Epoch: 9 [198400/288596 (69%)]\tLoss: 0.410603\n",
            "Train Epoch: 9 [199680/288596 (69%)]\tLoss: 0.326432\n",
            "Train Epoch: 9 [200960/288596 (70%)]\tLoss: 0.355002\n",
            "Train Epoch: 9 [202240/288596 (70%)]\tLoss: 0.324413\n",
            "Train Epoch: 9 [203520/288596 (71%)]\tLoss: 0.341810\n",
            "Train Epoch: 9 [204800/288596 (71%)]\tLoss: 0.376612\n",
            "Train Epoch: 9 [206080/288596 (71%)]\tLoss: 0.344750\n",
            "Train Epoch: 9 [207360/288596 (72%)]\tLoss: 0.317967\n",
            "Train Epoch: 9 [208640/288596 (72%)]\tLoss: 0.264835\n",
            "Train Epoch: 9 [209920/288596 (73%)]\tLoss: 0.369062\n",
            "Train Epoch: 9 [211200/288596 (73%)]\tLoss: 0.383678\n",
            "Train Epoch: 9 [212480/288596 (74%)]\tLoss: 0.350589\n",
            "Train Epoch: 9 [213760/288596 (74%)]\tLoss: 0.317102\n",
            "Train Epoch: 9 [215040/288596 (75%)]\tLoss: 0.278874\n",
            "Train Epoch: 9 [216320/288596 (75%)]\tLoss: 0.317531\n",
            "Train Epoch: 9 [217600/288596 (75%)]\tLoss: 0.430325\n",
            "Train Epoch: 9 [218880/288596 (76%)]\tLoss: 0.412201\n",
            "Train Epoch: 9 [220160/288596 (76%)]\tLoss: 0.372581\n",
            "Train Epoch: 9 [221440/288596 (77%)]\tLoss: 0.346655\n",
            "Train Epoch: 9 [222720/288596 (77%)]\tLoss: 0.363999\n",
            "Train Epoch: 9 [224000/288596 (78%)]\tLoss: 0.346799\n",
            "Train Epoch: 9 [225280/288596 (78%)]\tLoss: 0.269688\n",
            "Train Epoch: 9 [226560/288596 (78%)]\tLoss: 0.358148\n",
            "Train Epoch: 9 [227840/288596 (79%)]\tLoss: 0.282134\n",
            "Train Epoch: 9 [229120/288596 (79%)]\tLoss: 0.361620\n",
            "Train Epoch: 9 [230400/288596 (80%)]\tLoss: 0.484492\n",
            "Train Epoch: 9 [231680/288596 (80%)]\tLoss: 0.418087\n",
            "Train Epoch: 9 [232960/288596 (81%)]\tLoss: 0.468723\n",
            "Train Epoch: 9 [234240/288596 (81%)]\tLoss: 0.366653\n",
            "Train Epoch: 9 [235520/288596 (82%)]\tLoss: 0.351015\n",
            "Train Epoch: 9 [236800/288596 (82%)]\tLoss: 0.290748\n",
            "Train Epoch: 9 [238080/288596 (82%)]\tLoss: 0.423860\n",
            "Train Epoch: 9 [239360/288596 (83%)]\tLoss: 0.330822\n",
            "Train Epoch: 9 [240640/288596 (83%)]\tLoss: 0.282490\n",
            "Train Epoch: 9 [241920/288596 (84%)]\tLoss: 0.299861\n",
            "Train Epoch: 9 [243200/288596 (84%)]\tLoss: 0.358743\n",
            "Train Epoch: 9 [244480/288596 (85%)]\tLoss: 0.287811\n",
            "Train Epoch: 9 [245760/288596 (85%)]\tLoss: 0.349975\n",
            "Train Epoch: 9 [247040/288596 (86%)]\tLoss: 0.261697\n",
            "Train Epoch: 9 [248320/288596 (86%)]\tLoss: 0.373614\n",
            "Train Epoch: 9 [249600/288596 (86%)]\tLoss: 0.301128\n",
            "Train Epoch: 9 [250880/288596 (87%)]\tLoss: 0.406176\n",
            "Train Epoch: 9 [252160/288596 (87%)]\tLoss: 0.405095\n",
            "Train Epoch: 9 [253440/288596 (88%)]\tLoss: 0.302585\n",
            "Train Epoch: 9 [254720/288596 (88%)]\tLoss: 0.251681\n",
            "Train Epoch: 9 [256000/288596 (89%)]\tLoss: 0.271671\n",
            "Train Epoch: 9 [257280/288596 (89%)]\tLoss: 0.391364\n",
            "Train Epoch: 9 [258560/288596 (90%)]\tLoss: 0.389882\n",
            "Train Epoch: 9 [259840/288596 (90%)]\tLoss: 0.371287\n",
            "Train Epoch: 9 [261120/288596 (90%)]\tLoss: 0.314198\n",
            "Train Epoch: 9 [262400/288596 (91%)]\tLoss: 0.347338\n",
            "Train Epoch: 9 [263680/288596 (91%)]\tLoss: 0.242951\n",
            "Train Epoch: 9 [264960/288596 (92%)]\tLoss: 0.333463\n",
            "Train Epoch: 9 [266240/288596 (92%)]\tLoss: 0.373077\n",
            "Train Epoch: 9 [267520/288596 (93%)]\tLoss: 0.406819\n",
            "Train Epoch: 9 [268800/288596 (93%)]\tLoss: 0.428260\n",
            "Train Epoch: 9 [270080/288596 (94%)]\tLoss: 0.452670\n",
            "Train Epoch: 9 [271360/288596 (94%)]\tLoss: 0.261069\n",
            "Train Epoch: 9 [272640/288596 (94%)]\tLoss: 0.393212\n",
            "Train Epoch: 9 [273920/288596 (95%)]\tLoss: 0.307241\n",
            "Train Epoch: 9 [275200/288596 (95%)]\tLoss: 0.439829\n",
            "Train Epoch: 9 [276480/288596 (96%)]\tLoss: 0.291805\n",
            "Train Epoch: 9 [277760/288596 (96%)]\tLoss: 0.293352\n",
            "Train Epoch: 9 [279040/288596 (97%)]\tLoss: 0.385191\n",
            "Train Epoch: 9 [280320/288596 (97%)]\tLoss: 0.441537\n",
            "Train Epoch: 9 [281600/288596 (98%)]\tLoss: 0.346276\n",
            "Train Epoch: 9 [282880/288596 (98%)]\tLoss: 0.369454\n",
            "Train Epoch: 9 [284160/288596 (98%)]\tLoss: 0.346029\n",
            "Train Epoch: 9 [285440/288596 (99%)]\tLoss: 0.322455\n",
            "Train Epoch: 9 [286720/288596 (99%)]\tLoss: 0.280660\n",
            "Train Epoch: 9 [288000/288596 (100%)]\tLoss: 0.373072\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 107459/123684 (87%)\n",
            "\n",
            "Train Epoch: 10 [0/288596 (0%)]\tLoss: 0.408377\n",
            "Train Epoch: 10 [1280/288596 (0%)]\tLoss: 0.426392\n",
            "Train Epoch: 10 [2560/288596 (1%)]\tLoss: 0.404383\n",
            "Train Epoch: 10 [3840/288596 (1%)]\tLoss: 0.269482\n",
            "Train Epoch: 10 [5120/288596 (2%)]\tLoss: 0.285314\n",
            "Train Epoch: 10 [6400/288596 (2%)]\tLoss: 0.338631\n",
            "Train Epoch: 10 [7680/288596 (3%)]\tLoss: 0.351831\n",
            "Train Epoch: 10 [8960/288596 (3%)]\tLoss: 0.289820\n",
            "Train Epoch: 10 [10240/288596 (4%)]\tLoss: 0.402260\n",
            "Train Epoch: 10 [11520/288596 (4%)]\tLoss: 0.356968\n",
            "Train Epoch: 10 [12800/288596 (4%)]\tLoss: 0.382871\n",
            "Train Epoch: 10 [14080/288596 (5%)]\tLoss: 0.262703\n",
            "Train Epoch: 10 [15360/288596 (5%)]\tLoss: 0.385504\n",
            "Train Epoch: 10 [16640/288596 (6%)]\tLoss: 0.295744\n",
            "Train Epoch: 10 [17920/288596 (6%)]\tLoss: 0.314276\n",
            "Train Epoch: 10 [19200/288596 (7%)]\tLoss: 0.403191\n",
            "Train Epoch: 10 [20480/288596 (7%)]\tLoss: 0.372126\n",
            "Train Epoch: 10 [21760/288596 (8%)]\tLoss: 0.350777\n",
            "Train Epoch: 10 [23040/288596 (8%)]\tLoss: 0.393043\n",
            "Train Epoch: 10 [24320/288596 (8%)]\tLoss: 0.369383\n",
            "Train Epoch: 10 [25600/288596 (9%)]\tLoss: 0.340473\n",
            "Train Epoch: 10 [26880/288596 (9%)]\tLoss: 0.467391\n",
            "Train Epoch: 10 [28160/288596 (10%)]\tLoss: 0.406068\n",
            "Train Epoch: 10 [29440/288596 (10%)]\tLoss: 0.392111\n",
            "Train Epoch: 10 [30720/288596 (11%)]\tLoss: 0.316165\n",
            "Train Epoch: 10 [32000/288596 (11%)]\tLoss: 0.312139\n",
            "Train Epoch: 10 [33280/288596 (12%)]\tLoss: 0.338058\n",
            "Train Epoch: 10 [34560/288596 (12%)]\tLoss: 0.377396\n",
            "Train Epoch: 10 [35840/288596 (12%)]\tLoss: 0.356228\n",
            "Train Epoch: 10 [37120/288596 (13%)]\tLoss: 0.377088\n",
            "Train Epoch: 10 [38400/288596 (13%)]\tLoss: 0.406729\n",
            "Train Epoch: 10 [39680/288596 (14%)]\tLoss: 0.363130\n",
            "Train Epoch: 10 [40960/288596 (14%)]\tLoss: 0.371206\n",
            "Train Epoch: 10 [42240/288596 (15%)]\tLoss: 0.361970\n",
            "Train Epoch: 10 [43520/288596 (15%)]\tLoss: 0.360605\n",
            "Train Epoch: 10 [44800/288596 (16%)]\tLoss: 0.354771\n",
            "Train Epoch: 10 [46080/288596 (16%)]\tLoss: 0.386816\n",
            "Train Epoch: 10 [47360/288596 (16%)]\tLoss: 0.323532\n",
            "Train Epoch: 10 [48640/288596 (17%)]\tLoss: 0.343973\n",
            "Train Epoch: 10 [49920/288596 (17%)]\tLoss: 0.299230\n",
            "Train Epoch: 10 [51200/288596 (18%)]\tLoss: 0.421459\n",
            "Train Epoch: 10 [52480/288596 (18%)]\tLoss: 0.345464\n",
            "Train Epoch: 10 [53760/288596 (19%)]\tLoss: 0.372883\n",
            "Train Epoch: 10 [55040/288596 (19%)]\tLoss: 0.362066\n",
            "Train Epoch: 10 [56320/288596 (20%)]\tLoss: 0.316501\n",
            "Train Epoch: 10 [57600/288596 (20%)]\tLoss: 0.383215\n",
            "Train Epoch: 10 [58880/288596 (20%)]\tLoss: 0.321664\n",
            "Train Epoch: 10 [60160/288596 (21%)]\tLoss: 0.488906\n",
            "Train Epoch: 10 [61440/288596 (21%)]\tLoss: 0.333656\n",
            "Train Epoch: 10 [62720/288596 (22%)]\tLoss: 0.393063\n",
            "Train Epoch: 10 [64000/288596 (22%)]\tLoss: 0.289960\n",
            "Train Epoch: 10 [65280/288596 (23%)]\tLoss: 0.345176\n",
            "Train Epoch: 10 [66560/288596 (23%)]\tLoss: 0.228537\n",
            "Train Epoch: 10 [67840/288596 (24%)]\tLoss: 0.384442\n",
            "Train Epoch: 10 [69120/288596 (24%)]\tLoss: 0.308342\n",
            "Train Epoch: 10 [70400/288596 (24%)]\tLoss: 0.333241\n",
            "Train Epoch: 10 [71680/288596 (25%)]\tLoss: 0.284849\n",
            "Train Epoch: 10 [72960/288596 (25%)]\tLoss: 0.282469\n",
            "Train Epoch: 10 [74240/288596 (26%)]\tLoss: 0.349845\n",
            "Train Epoch: 10 [75520/288596 (26%)]\tLoss: 0.356051\n",
            "Train Epoch: 10 [76800/288596 (27%)]\tLoss: 0.332166\n",
            "Train Epoch: 10 [78080/288596 (27%)]\tLoss: 0.395145\n",
            "Train Epoch: 10 [79360/288596 (27%)]\tLoss: 0.338066\n",
            "Train Epoch: 10 [80640/288596 (28%)]\tLoss: 0.549683\n",
            "Train Epoch: 10 [81920/288596 (28%)]\tLoss: 0.286347\n",
            "Train Epoch: 10 [83200/288596 (29%)]\tLoss: 0.416209\n",
            "Train Epoch: 10 [84480/288596 (29%)]\tLoss: 0.328667\n",
            "Train Epoch: 10 [85760/288596 (30%)]\tLoss: 0.394425\n",
            "Train Epoch: 10 [87040/288596 (30%)]\tLoss: 0.460760\n",
            "Train Epoch: 10 [88320/288596 (31%)]\tLoss: 0.364621\n",
            "Train Epoch: 10 [89600/288596 (31%)]\tLoss: 0.252308\n",
            "Train Epoch: 10 [90880/288596 (31%)]\tLoss: 0.340992\n",
            "Train Epoch: 10 [92160/288596 (32%)]\tLoss: 0.338545\n",
            "Train Epoch: 10 [93440/288596 (32%)]\tLoss: 0.351480\n",
            "Train Epoch: 10 [94720/288596 (33%)]\tLoss: 0.379707\n",
            "Train Epoch: 10 [96000/288596 (33%)]\tLoss: 0.373254\n",
            "Train Epoch: 10 [97280/288596 (34%)]\tLoss: 0.405435\n",
            "Train Epoch: 10 [98560/288596 (34%)]\tLoss: 0.388998\n",
            "Train Epoch: 10 [99840/288596 (35%)]\tLoss: 0.412125\n",
            "Train Epoch: 10 [101120/288596 (35%)]\tLoss: 0.334910\n",
            "Train Epoch: 10 [102400/288596 (35%)]\tLoss: 0.314289\n",
            "Train Epoch: 10 [103680/288596 (36%)]\tLoss: 0.344556\n",
            "Train Epoch: 10 [104960/288596 (36%)]\tLoss: 0.327063\n",
            "Train Epoch: 10 [106240/288596 (37%)]\tLoss: 0.308768\n",
            "Train Epoch: 10 [107520/288596 (37%)]\tLoss: 0.280851\n",
            "Train Epoch: 10 [108800/288596 (38%)]\tLoss: 0.301045\n",
            "Train Epoch: 10 [110080/288596 (38%)]\tLoss: 0.249675\n",
            "Train Epoch: 10 [111360/288596 (39%)]\tLoss: 0.324688\n",
            "Train Epoch: 10 [112640/288596 (39%)]\tLoss: 0.347751\n",
            "Train Epoch: 10 [113920/288596 (39%)]\tLoss: 0.391630\n",
            "Train Epoch: 10 [115200/288596 (40%)]\tLoss: 0.361673\n",
            "Train Epoch: 10 [116480/288596 (40%)]\tLoss: 0.337314\n",
            "Train Epoch: 10 [117760/288596 (41%)]\tLoss: 0.321291\n",
            "Train Epoch: 10 [119040/288596 (41%)]\tLoss: 0.425594\n",
            "Train Epoch: 10 [120320/288596 (42%)]\tLoss: 0.296870\n",
            "Train Epoch: 10 [121600/288596 (42%)]\tLoss: 0.362105\n",
            "Train Epoch: 10 [122880/288596 (43%)]\tLoss: 0.307935\n",
            "Train Epoch: 10 [124160/288596 (43%)]\tLoss: 0.321084\n",
            "Train Epoch: 10 [125440/288596 (43%)]\tLoss: 0.329863\n",
            "Train Epoch: 10 [126720/288596 (44%)]\tLoss: 0.369854\n",
            "Train Epoch: 10 [128000/288596 (44%)]\tLoss: 0.460489\n",
            "Train Epoch: 10 [129280/288596 (45%)]\tLoss: 0.375097\n",
            "Train Epoch: 10 [130560/288596 (45%)]\tLoss: 0.287481\n",
            "Train Epoch: 10 [131840/288596 (46%)]\tLoss: 0.438436\n",
            "Train Epoch: 10 [133120/288596 (46%)]\tLoss: 0.350975\n",
            "Train Epoch: 10 [134400/288596 (47%)]\tLoss: 0.413108\n",
            "Train Epoch: 10 [135680/288596 (47%)]\tLoss: 0.360209\n",
            "Train Epoch: 10 [136960/288596 (47%)]\tLoss: 0.372923\n",
            "Train Epoch: 10 [138240/288596 (48%)]\tLoss: 0.359538\n",
            "Train Epoch: 10 [139520/288596 (48%)]\tLoss: 0.336918\n",
            "Train Epoch: 10 [140800/288596 (49%)]\tLoss: 0.399391\n",
            "Train Epoch: 10 [142080/288596 (49%)]\tLoss: 0.319426\n",
            "Train Epoch: 10 [143360/288596 (50%)]\tLoss: 0.324607\n",
            "Train Epoch: 10 [144640/288596 (50%)]\tLoss: 0.312860\n",
            "Train Epoch: 10 [145920/288596 (51%)]\tLoss: 0.345669\n",
            "Train Epoch: 10 [147200/288596 (51%)]\tLoss: 0.372684\n",
            "Train Epoch: 10 [148480/288596 (51%)]\tLoss: 0.296011\n",
            "Train Epoch: 10 [149760/288596 (52%)]\tLoss: 0.376798\n",
            "Train Epoch: 10 [151040/288596 (52%)]\tLoss: 0.393895\n",
            "Train Epoch: 10 [152320/288596 (53%)]\tLoss: 0.232482\n",
            "Train Epoch: 10 [153600/288596 (53%)]\tLoss: 0.391455\n",
            "Train Epoch: 10 [154880/288596 (54%)]\tLoss: 0.424430\n",
            "Train Epoch: 10 [156160/288596 (54%)]\tLoss: 0.236615\n",
            "Train Epoch: 10 [157440/288596 (55%)]\tLoss: 0.454386\n",
            "Train Epoch: 10 [158720/288596 (55%)]\tLoss: 0.354826\n",
            "Train Epoch: 10 [160000/288596 (55%)]\tLoss: 0.413790\n",
            "Train Epoch: 10 [161280/288596 (56%)]\tLoss: 0.392637\n",
            "Train Epoch: 10 [162560/288596 (56%)]\tLoss: 0.301514\n",
            "Train Epoch: 10 [163840/288596 (57%)]\tLoss: 0.370736\n",
            "Train Epoch: 10 [165120/288596 (57%)]\tLoss: 0.352556\n",
            "Train Epoch: 10 [166400/288596 (58%)]\tLoss: 0.368489\n",
            "Train Epoch: 10 [167680/288596 (58%)]\tLoss: 0.325762\n",
            "Train Epoch: 10 [168960/288596 (59%)]\tLoss: 0.302269\n",
            "Train Epoch: 10 [170240/288596 (59%)]\tLoss: 0.311538\n",
            "Train Epoch: 10 [171520/288596 (59%)]\tLoss: 0.292913\n",
            "Train Epoch: 10 [172800/288596 (60%)]\tLoss: 0.413469\n",
            "Train Epoch: 10 [174080/288596 (60%)]\tLoss: 0.434328\n",
            "Train Epoch: 10 [175360/288596 (61%)]\tLoss: 0.475112\n",
            "Train Epoch: 10 [176640/288596 (61%)]\tLoss: 0.289316\n",
            "Train Epoch: 10 [177920/288596 (62%)]\tLoss: 0.380952\n",
            "Train Epoch: 10 [179200/288596 (62%)]\tLoss: 0.323489\n",
            "Train Epoch: 10 [180480/288596 (63%)]\tLoss: 0.420030\n",
            "Train Epoch: 10 [181760/288596 (63%)]\tLoss: 0.396092\n",
            "Train Epoch: 10 [183040/288596 (63%)]\tLoss: 0.325252\n",
            "Train Epoch: 10 [184320/288596 (64%)]\tLoss: 0.430111\n",
            "Train Epoch: 10 [185600/288596 (64%)]\tLoss: 0.350380\n",
            "Train Epoch: 10 [186880/288596 (65%)]\tLoss: 0.309279\n",
            "Train Epoch: 10 [188160/288596 (65%)]\tLoss: 0.384406\n",
            "Train Epoch: 10 [189440/288596 (66%)]\tLoss: 0.376628\n",
            "Train Epoch: 10 [190720/288596 (66%)]\tLoss: 0.366648\n",
            "Train Epoch: 10 [192000/288596 (67%)]\tLoss: 0.255636\n",
            "Train Epoch: 10 [193280/288596 (67%)]\tLoss: 0.429483\n",
            "Train Epoch: 10 [194560/288596 (67%)]\tLoss: 0.352648\n",
            "Train Epoch: 10 [195840/288596 (68%)]\tLoss: 0.290033\n",
            "Train Epoch: 10 [197120/288596 (68%)]\tLoss: 0.291353\n",
            "Train Epoch: 10 [198400/288596 (69%)]\tLoss: 0.359126\n",
            "Train Epoch: 10 [199680/288596 (69%)]\tLoss: 0.317401\n",
            "Train Epoch: 10 [200960/288596 (70%)]\tLoss: 0.355851\n",
            "Train Epoch: 10 [202240/288596 (70%)]\tLoss: 0.253357\n",
            "Train Epoch: 10 [203520/288596 (71%)]\tLoss: 0.343995\n",
            "Train Epoch: 10 [204800/288596 (71%)]\tLoss: 0.284960\n",
            "Train Epoch: 10 [206080/288596 (71%)]\tLoss: 0.376951\n",
            "Train Epoch: 10 [207360/288596 (72%)]\tLoss: 0.384479\n",
            "Train Epoch: 10 [208640/288596 (72%)]\tLoss: 0.274413\n",
            "Train Epoch: 10 [209920/288596 (73%)]\tLoss: 0.417560\n",
            "Train Epoch: 10 [211200/288596 (73%)]\tLoss: 0.366500\n",
            "Train Epoch: 10 [212480/288596 (74%)]\tLoss: 0.288098\n",
            "Train Epoch: 10 [213760/288596 (74%)]\tLoss: 0.473837\n",
            "Train Epoch: 10 [215040/288596 (75%)]\tLoss: 0.435270\n",
            "Train Epoch: 10 [216320/288596 (75%)]\tLoss: 0.292553\n",
            "Train Epoch: 10 [217600/288596 (75%)]\tLoss: 0.272238\n",
            "Train Epoch: 10 [218880/288596 (76%)]\tLoss: 0.340806\n",
            "Train Epoch: 10 [220160/288596 (76%)]\tLoss: 0.279049\n",
            "Train Epoch: 10 [221440/288596 (77%)]\tLoss: 0.389364\n",
            "Train Epoch: 10 [222720/288596 (77%)]\tLoss: 0.296034\n",
            "Train Epoch: 10 [224000/288596 (78%)]\tLoss: 0.421148\n",
            "Train Epoch: 10 [225280/288596 (78%)]\tLoss: 0.292071\n",
            "Train Epoch: 10 [226560/288596 (78%)]\tLoss: 0.289158\n",
            "Train Epoch: 10 [227840/288596 (79%)]\tLoss: 0.399491\n",
            "Train Epoch: 10 [229120/288596 (79%)]\tLoss: 0.333706\n",
            "Train Epoch: 10 [230400/288596 (80%)]\tLoss: 0.343875\n",
            "Train Epoch: 10 [231680/288596 (80%)]\tLoss: 0.351528\n",
            "Train Epoch: 10 [232960/288596 (81%)]\tLoss: 0.305164\n",
            "Train Epoch: 10 [234240/288596 (81%)]\tLoss: 0.284554\n",
            "Train Epoch: 10 [235520/288596 (82%)]\tLoss: 0.373627\n",
            "Train Epoch: 10 [236800/288596 (82%)]\tLoss: 0.360057\n",
            "Train Epoch: 10 [238080/288596 (82%)]\tLoss: 0.406156\n",
            "Train Epoch: 10 [239360/288596 (83%)]\tLoss: 0.309214\n",
            "Train Epoch: 10 [240640/288596 (83%)]\tLoss: 0.319368\n",
            "Train Epoch: 10 [241920/288596 (84%)]\tLoss: 0.307871\n",
            "Train Epoch: 10 [243200/288596 (84%)]\tLoss: 0.386647\n",
            "Train Epoch: 10 [244480/288596 (85%)]\tLoss: 0.349292\n",
            "Train Epoch: 10 [245760/288596 (85%)]\tLoss: 0.399115\n",
            "Train Epoch: 10 [247040/288596 (86%)]\tLoss: 0.346998\n",
            "Train Epoch: 10 [248320/288596 (86%)]\tLoss: 0.355393\n",
            "Train Epoch: 10 [249600/288596 (86%)]\tLoss: 0.383963\n",
            "Train Epoch: 10 [250880/288596 (87%)]\tLoss: 0.368051\n",
            "Train Epoch: 10 [252160/288596 (87%)]\tLoss: 0.411950\n",
            "Train Epoch: 10 [253440/288596 (88%)]\tLoss: 0.326527\n",
            "Train Epoch: 10 [254720/288596 (88%)]\tLoss: 0.332926\n",
            "Train Epoch: 10 [256000/288596 (89%)]\tLoss: 0.219407\n",
            "Train Epoch: 10 [257280/288596 (89%)]\tLoss: 0.259928\n",
            "Train Epoch: 10 [258560/288596 (90%)]\tLoss: 0.385682\n",
            "Train Epoch: 10 [259840/288596 (90%)]\tLoss: 0.322255\n",
            "Train Epoch: 10 [261120/288596 (90%)]\tLoss: 0.406898\n",
            "Train Epoch: 10 [262400/288596 (91%)]\tLoss: 0.344241\n",
            "Train Epoch: 10 [263680/288596 (91%)]\tLoss: 0.377504\n",
            "Train Epoch: 10 [264960/288596 (92%)]\tLoss: 0.384359\n",
            "Train Epoch: 10 [266240/288596 (92%)]\tLoss: 0.319845\n",
            "Train Epoch: 10 [267520/288596 (93%)]\tLoss: 0.281508\n",
            "Train Epoch: 10 [268800/288596 (93%)]\tLoss: 0.293105\n",
            "Train Epoch: 10 [270080/288596 (94%)]\tLoss: 0.437656\n",
            "Train Epoch: 10 [271360/288596 (94%)]\tLoss: 0.327363\n",
            "Train Epoch: 10 [272640/288596 (94%)]\tLoss: 0.373469\n",
            "Train Epoch: 10 [273920/288596 (95%)]\tLoss: 0.333769\n",
            "Train Epoch: 10 [275200/288596 (95%)]\tLoss: 0.256648\n",
            "Train Epoch: 10 [276480/288596 (96%)]\tLoss: 0.368015\n",
            "Train Epoch: 10 [277760/288596 (96%)]\tLoss: 0.377854\n",
            "Train Epoch: 10 [279040/288596 (97%)]\tLoss: 0.456591\n",
            "Train Epoch: 10 [280320/288596 (97%)]\tLoss: 0.388635\n",
            "Train Epoch: 10 [281600/288596 (98%)]\tLoss: 0.352889\n",
            "Train Epoch: 10 [282880/288596 (98%)]\tLoss: 0.299412\n",
            "Train Epoch: 10 [284160/288596 (98%)]\tLoss: 0.376447\n",
            "Train Epoch: 10 [285440/288596 (99%)]\tLoss: 0.314532\n",
            "Train Epoch: 10 [286720/288596 (99%)]\tLoss: 0.404290\n",
            "Train Epoch: 10 [288000/288596 (100%)]\tLoss: 0.399410\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 107461/123684 (87%)\n",
            "\n",
            "107461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Testing"
      ],
      "metadata": {
        "id": "84TdbM5Y0WiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_labels = []\n",
        "\n",
        "for data, target in test_loader:\n",
        "  target.cpu()\n",
        "  target = target.numpy()\n",
        "  y_test_labels.append(target)\n",
        "\n",
        "y_test_pred = test(model, device, test_loader)[1]\n",
        "y_test_pred = [label for sublist in y_test_pred for label in sublist]\n",
        "y_test_pred = torch.tensor(y_test_pred, device = device)\n",
        "y_test_pred = y_test_pred.cpu().numpy()\n",
        "y_test_labels = [label for sublist in y_test_labels for label in sublist]\n",
        "\n",
        "print(classification_report(y_test_labels, y_test_pred))"
      ],
      "metadata": {
        "id": "gjUK8W6bmLp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70cce8c9-54c4-4cfd-c985-a6c5849346e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 153423/176692 (87%)\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.88      0.87     88346\n",
            "           1       0.88      0.86      0.87     88346\n",
            "\n",
            "    accuracy                           0.87    176692\n",
            "   macro avg       0.87      0.87      0.87    176692\n",
            "weighted avg       0.87      0.87      0.87    176692\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Model: SVM with TFIDFVectorizer"
      ],
      "metadata": {
        "id": "qCjyFRSirmQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_review_us_tfidf_vec = tfidf_vec.transform(df_X_test)"
      ],
      "metadata": {
        "id": "uuCQh7ylr66p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh6Bu3Kbrp06"
      },
      "outputs": [],
      "source": [
        "y_pred__svc__tfidf_vec__test_us = svc_clf__tfidf_us.predict(X_test_review_us_tfidf_vec)\n",
        "y_pred_proba__svc__tfidf_vec__test_us = svc_clf__tfidf_us.predict_proba(X_test_review_us_tfidf_vec)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40b446a-7c44-4f6c-eb65-095bd5c3dbcc",
        "scrolled": true,
        "id": "bINQsgFsrp1H"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.8845630590210438\n",
            "Precision Score: 0.8889197290995892\n",
            "Recall Score: 0.8896111111111111\n",
            "F1 Score: 0.8892652857222191\n",
            "ROC AUC Score: 0.9392515696030834\n",
            "Confusion Matrix:\n",
            " [[14546  2001]\n",
            " [ 1987 16013]]\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "get_scores(df_y_test, y_pred__svc__tfidf_vec__test_us, y_pred_proba__svc__tfidf_vec__test_us)"
      ]
    }
  ]
}